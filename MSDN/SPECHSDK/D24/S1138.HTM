<HTML><HEAD><script async src="https://www.googletagmanager.com/gtag/js?id=UA-83731338-2"></script><script>  window.dataLayer = window.dataLayer || [];  function gtag(){dataLayer.push(arguments);}  gtag('js', new Date());  gtag('config', 'UA-83731338-2');</script><title>Low-Level Text-to-Speech</TITLE><META HTTP-EQUIV="CONTENT-TYPE" CONTENT="text/html; charset= iso-8859-1"><style>@import url(/msdn_ie4.css);</style>
<link disabled rel="stylesheet" href="/msdn_ie3.css"></HEAD><BODY BGCOLOR="#FFFFFF">
<FONT FACE="verdana,atial,helvetica" SIZE="2"><FORM NAME="x"><OBJECT CLASSID="clsid:9c2ac687-ceef-11cf-96d9-00a0c903b016" NAME="iv"></OBJECT></FORM>
<H3>Low-Level Text-to-Speech </H3><P>When an application uses the low-level text-to-speech interfaces, it is talking directly to the engine. As with speech recognition, this provides the application with a lot more control, but requires more work of it. Because the low-level API is more complex, this document won't go into detail about how objects are used. However, an architectural overview follows.</P>
<P>The low-level API consists of many more objects than the high-level API (voice text). Here's how the process works: </P>
<P>The application determines where the text-to-speech audio should be sent and creates an audio-destination object through which the engine sends the data. Microsoft supplies an audio-destination object that sends its audio to the multimedia wave-out device, but the application may use customized audio destinations, such as an audio destination that writes to a .WAV file.</P>
<P>The application, through a text-to-speech enumerator object (not shown here, but provided by Microsoft), locates a text-to-speech engine and voice that it wants to use. It then creates an instance of the engine object and passes it the audio-destination object.</P>
<P>The engine object has a dialog with the audio-destination object to find a common data format for the digital audio. Once an acceptable format is established, the engine creates an Audio-Destination Notification Sink that it passes to the audio-destination object. From then on, the audio-destination object submits status information to the engine through the notification sink.</P>
<P>The application can then register a Main Notification Sink that receives buffer-independent notifications, such as whether the synthesized voice is speaking and mouth positions for animation.</P>
<P>When it is ready, the application passes one or more text buffers down to the engine. These will be queued up and then spoken (to the audio destination) by the engine.</P>
<P>To find out what words are currently being spoken, the application can create a Buffer Notification Sink for every buffer object. When the engine speaks a word, reaches a bookmark, or some other event occurs, it calls functions in the Buffer Notification Sinks. The notification sink is released when the buffer is finished being read. </P>
<P>For more information look in the "Low-Level Text-to-Speech Section."</P></FONT></BODY></HTML>
