<HTML><HEAD><script async src="https://www.googletagmanager.com/gtag/js?id=UA-83731338-2"></script><script>  window.dataLayer = window.dataLayer || [];  function gtag(){dataLayer.push(arguments);}  gtag('js', new Date());  gtag('config', 'UA-83731338-2');</script><title>Grammar-Specific Notifications</TITLE><META HTTP-EQUIV="CONTENT-TYPE" CONTENT="text/html; charset= iso-8859-1"><style>@import url(/msdn_ie4.css);</style>
<link disabled rel="stylesheet" href="/msdn_ie3.css"></HEAD><BODY BGCOLOR="#FFFFFF">
<FONT FACE="verdana,atial,helvetica" SIZE="2"><FORM NAME="x"><OBJECT CLASSID="clsid:9c2ac687-ceef-11cf-96d9-00a0c903b016" NAME="iv"></OBJECT></FORM>
<H3>Grammar-Specific Notifications</H3><P>When the user speaks a command or phrase, the engine notifies the application by calling the <B>ISRNotifySink::UtteranceBegin </B>member function. When the user stops speaking, the engine calls the <B>UtteranceEnd </B>member function. The application should not use these notifications to influence recognition.</P>
<P>Unless the sound turns out to be just noise, some time after the engine calls <B>UtteranceBegin</B>, the <B>PhraseStart</B> member functions of all <B>ISRGramNotifySink</B> interfaces on active grammar objects are called, indicating that the engine has begun recognition processing on the audio. The application is notified of the time (in bytes) when the audio began. An application can convert this into hours, minutes, and seconds by calling the <B>ISRCentral::ToFileTime </B>member function.</P>
<P>If an engine supports hypotheses, it periodically calls the <B>ISRGramNotifySink::PhraseHypothesis</B> member function after the initial call to the <B>PhraseStart </B>member function. Many applications ignore calls to <B>PhraseHypothesis</B> because they have no use for the information, but some applications display the hypothesis to the user or perform preprocessing on the hypothesis. </P>
<P>When an engine has concluded its processing of the audio, it calls the <B>PhraseFinish </B>member function. The <B>PhraseHypothesis</B> and <B>PhraseFinish</B> member functions pass the following information to the application:</P>
<P> <FONT FACE="Symbol">·</FONT>    The start and end time of the phrase. The application can use this information to isolate the audio or synchronize it with other events in the system.</P>
<P> <FONT FACE="Symbol">·</FONT>    Two flags. One flag indicates whether the engine is confident enough about the recognition that the application should act on it, or the recognition results are unclear and the application should ask for clarification (much as a person would say "Could you repeat that, please"). The other flag indicates whether the best recognition result was obtained from this grammar or from another active grammar. An application should act upon a phrase recognition only if it was obtained from its own grammar.</P>
<P> <FONT FACE="Symbol">·</FONT>    Address of a <B>SRPHRASE</B> structure. If the engine can recognize words from the speech, it passes the address of an <B>SRPHRASE</B> structure that contains a sequential list of <B>SRWORD</B> structures, each indicating the text and word identifier of the recognized word. An application can display these words, parse them, or do whatever it needs to do with the information. (If the grammar is context-free, the words must be in the grammar's set of words.)</P>
<P> <FONT FACE="Symbol">·</FONT>    Address of a results object. Some engines support a speech-recognition results object that allows an application to get more information about what was recognized, such as accurate timing and alternative recognition possibilities. If a results object exists, the engine passes a non-NULL address of an <B>IUnknown</B> interface for the object. For more information about the speech-recognition results object, see "Speech-Recognition Results Objects" later in this section.</P>
<P>If the application needs to use the speech-recognition results object, it calls the <B>IUnknown::QueryInterface</B> member function to get an interface on the object (for example, <B>ISRResBasic</B>). In this case, it is the application's responsibility to release the results object. If the application doesn't use the results object, the engine calls the <B>IUnknown::Release</B> member function on the results object after the application returns from the <B>PhraseFinish</B> notification. The engine calls the <B>Release</B> member function so that an application that ignores the <B>PhraseFinish</B> notification does not inadvertently leave results objects in memory.</P>
<P></P>
<P>While training or double-checking its recognitions, an engine may change the result of a recognition. If so, the engine calls the <B>ISRGramNotifySink::ReEvaluate</B> member function to notify the application that the results object has changed and should be updated. Applications that use speech recognition for command and control typically ignore this notification, but those using it for data or text entry may use the information to improve performance.</P>
<P>The notification object is used by the grammar object until the latter frees itself, at which point it calls the <B>ISRGramNotifySink::Release</B> member function to release the notification sink. This may not occur immediately after the grammar object is released by the application because of asynchronous nature of speech recognition — that is, the engine may need to finish processing a certain amount of speech after the grammar object is released.</P></FONT></BODY></HTML>
