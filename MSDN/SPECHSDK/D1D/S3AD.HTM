<HTML><HEAD><script async src="https://www.googletagmanager.com/gtag/js?id=UA-83731338-2"></script><script>  window.dataLayer = window.dataLayer || [];  function gtag(){dataLayer.push(arguments);}  gtag('js', new Date());  gtag('config', 'UA-83731338-2');</script><title>Information Chunks in Dictation Grammars</TITLE><META HTTP-EQUIV="CONTENT-TYPE" CONTENT="text/html; charset= iso-8859-1"><style>@import url(/msdn_ie4.css);</style>
<link disabled rel="stylesheet" href="/msdn_ie3.css"></HEAD><BODY BGCOLOR="#FFFFFF">
<FONT FACE="verdana,atial,helvetica" SIZE="2"><FORM NAME="x"><OBJECT CLASSID="clsid:9c2ac687-ceef-11cf-96d9-00a0c903b016" NAME="iv"></OBJECT></FORM>
<H3>Information Chunks in Dictation Grammars</H3><P>The dictation grammar supports the following kinds of information chunks. </P>
<P> <FONT FACE="Symbol">·</FONT>    SRCKD_COMMON</P>
<P> <FONT FACE="Symbol">·</FONT>    SRCKD_GROUP</P>
<P> <FONT FACE="Symbol">·</FONT>    SRCKD_SAMPLE</P>
<P> <FONT FACE="Symbol">·</FONT>    SRCKD_TOPIC</P>
<P> <FONT FACE="Symbol">·</FONT>    SRCKD_NGRAM</P>
<P></P>
<P>All chunks are optional, and the grammar can contain more than one of each.</P>
<H4><A NAME="sec0"></A>Common Words (SRCKD_COMMON)</H4><P>This chunk contains a list of common words, stored as a sequential list of null-terminated word strings. Common words will probably be used in the dictation, but they may not be expected by the language model. For example: </P>
<P>"Automobile\0Franchise\0Repair\0Muffler\0Drive Shaft"</P>
<P></P>
<H4><A NAME="sec1"></A>Word Groups (SRCKD_GROUP)</H4><P>This chunk contains the same information as SRCKD_COMMON. Word groups are related in some way but are not automatically associated by the language model. For example, a dictation system for the air travel industry would supply a list of cities with major airports, such as:</P>
<P>"New York City\0Boston\0Los Angeles\0Chicago\0Seattle"</P>
<P></P>
<H4><A NAME="sec2"></A>Sample Text (SRCKD_SAMPLE)</H4><P>This chunk contains sample text, stored as a single null-terminated string. The sample text might be from a document written by the speaker or a document similar to the one that the speaker will dictate. It allows the engine to identify a topic, common words, and authoring style. The text can be any length, although most sample text will be only a few pages long (not enough to form the basis for a language model).</P>
<H4><A NAME="sec3"></A>Topic Keyword (SRCKD_TOPIC)</H4><P>This chunk contains a null-terminated string of a topic keyword. Topic keywords categorize the dictation into topics, such as "legal" and medical."</P>
<P>A grammar might have several topics ranging from very general to very specific, so that if the engine doesn't recognize a specific topic it may recognize a more general one.</P>
<H4><A NAME="sec4"></A>N-Gram Probability File (SRCKD_NGRAM)</H4><P>(New for 3.0)</P>
<P>This chunk contains N-gram information so an engine will know the probabilities of words given a context. An application might wish to scan documents similar to what the user will be dictating and produce an N-gram chunk. The N-gram chunk has the following format:</P>
<P>typedef struct {</P>
<P>    QWORD    dwTotalCounts</P>
<P>    DWORD    dwNumWordsClasses;</P>
<P>    DWORD    dwWordClassNameOffset;</P>
<P>    DWORD    dwNumContextGroups;</P>
<P>    DWORD    dwContextGroupOffset;</P>
<P>    DWORD    dwNumClasses;</P>
<P>    DWORD    dwClassOffset;</P>
<P>    BYTE    bBitsPerWord</P>
<P>    BYTE    abFiller[3];</P>
<P>    DWORD    adwProbability[256];</P>
<P>    } NGRAMHDR, * PNGRAMHDR;</P>
<P>dwTotalCounts - Total number of words "scanned" to produce the N-gram data chunk. This affects how much the N-gram overrides the existing language model. At -1, the default language model provided by the dictation engine is completely overridden. For lower values, the engine will do some interpolation between its built in language model and the application model. The amount of interpolation is left up to the engine.</P>
<P>dwNumWordsClasses - Number of words/classes used by the N-gram.</P>
<P>dwWordClassNameOffset - Offset (in bytes) from the beginning of the chunk where the word/class name data begins. The words/class name data is stored as list of NULL-terminated strings. Words/classes are automatically assigned word ID, sequentially starting at 1. It's possible to have an empty string for a class. If a word/class ID does not appear in the class definitions then it is a word ID. To minimize memory, applications may want to sort classes and words so the most frequent classes and words appear at the top of the list.</P>
<P>dwNumContextGroups - Number of context groups stored in the N-gram chunk.</P>
<P>dwContextGroupOffset - Offset (in bytes) from the beginning of the chunk where the context groups begin. Context groups will be stored sequentially, one after the other. An application can jump to the next context group by skipping forward NGRAMCGHDR-&gt;dwSize from the beginning of the current context group.</P>
<P>dwNumClasses - Number of classes stored in the N-gram chunk.</P>
<P>dwClassOffset - Offset (in bytes) from the beginning of then chunk where the class definitions begin. . Class definitions will be stored sequentially, one after the other. An application can jump to the next class definition by skipping forward NGRAMCGHDR-&gt;dwSize from the beginning of the current class definition.</P>
<P>bBitsPerWord - Number of bits to represent a word ID. This can be 16 or 32.</P>
<P>adwProbability - This is an array of 256 DWORDs, which map a probability token (stored in a byte) into a 4-byte linear probability value. 0xfffffff = 1.0, 0x80000000 = 0.5, etc. Thus, if a probability for a word in a context is token 52, then the real probability is adwProbability[52].</P>
<P></P>
<P>An N-gram chunk will have thousands (perhaps millions) or context groups. Context groups must be written in sorted order by adwWordClassID[0..3]. (Example: The context (25,54,0,0) comes before (25, 55, 0, 0), and also comes before (25, 54, 10, 0).) Each context group has a header:</P>
<P>typedef struct {</P>
<P>    DWORD    dwSize;</P>
<P>    QWORD    dwTotalCounts</P>
<P>    DWORD    adwWordClassID[4];</P>
<P>    DWORD    dwNumSequential;</P>
<P>    DWORD    dwNumRandomAccess;</P>
<P>    BYTE    bBackOffProbToken;</P>
<P>    BYTE    abFiller[3];</P>
<P>    // WORD or DWORD awRandomAccessWordsClasses[dwNumRandomAccess];</P>
<P>    // BYTE    abProbToken[dwNumSequential+dwNumRandomAccess];</P>
<P>    } NGRAMCGHDR, *PNGRAMCGHDR;</P>
<P>dwSize - Number of bytes in this context group, including the header, rounded up to the nearest DWORD (4 bytes). The dwSize field can be used to skip to the next context group.</P>
<P>dwTotalCounts - Total number of words "scanned" to produce context. This affects how much the N-gram overrides the existing language model. At -1, the default language model (for this context) provided by the dictation engine is completely overridden. For lower values, the engine will do some interpolation between its built in language model and the application model. The amount of interpolation is left up to the engine.</P>
<P>adwWordClassID - Array of word ID's that define the context. Unused context information is set to 0. Thus, to represent a context of "new York" for a tri-gram table, adwWordClassID[0] would be the ID of "new", and adwWordClassID[1] would be the ID of "York", while all other values would be 0. This array must be unique within the context groups in the grammar; There cannot be two "new York" contexts. Applications should avoid using a class ID in the context since its effect is undetermined.</P>
<P>dwNumSequential - Number of sequential word/class probability tokens. (See below for details.)</P>
<P>dwNumRandomAccess - Number of random access word/class probability tokens. (See below for details.)</P>
<P>bBackOffProbToken - Probability token that indicates the probability that the context does not contain the "correct" word sequence and that the (N-1)-gram model should be used. Thus, with the context "new York", this is the probability that the "new" context should be used instead. The probability token can be converted to a real probability by indexing it into adwProbability[].</P>
<P>Immediately after each NGRAMCGHDR comes the following data:</P>
<P>awRandomAccessWordsClasses[dwNumRandomAccess] - Array of WORD or DWORD structures (depending upon bBitsPerWord) with dwNumRandomAccess elements. These are the word IDs used to reference the probability tokens. Word/class IDs must be in ascending order so that an engine can access the data quickly.</P>
<P>abProbToken[dwNumSequential+dwNumRandomAccess] - Array of (dwNumSequential + dwNumRandomAccess) bytes representing probability tokens. The first dwNumSequential bytes correspond to the probabilities for word ID's 1 through dwNumSequential. The remaining dwNumRandomAccess bytes correspond to the word ID defined by awRandomAccessWords. (The probabilities, including the back-off, must sum to 1.0) The probability (prob) of word/class N is:</P>
<P>if (N &lt;= dwNumSequential)</P>
<P>    prob = adwProbability[abProbToken[N-1]];</P>
<P>else {</P>
<P>    for (I = 0; I &lt; dwNumRandomAccess; I++)</P>
<P>        if (awRandomAccessWordsClasses[I] == N)</P>
<P>            break;</P>
<P>    if (I &lt; dwNumRandomAccess)</P>
<P>        prob = adwProbability[abProbToken[I+dwNumSequential]];</P>
<P>    else</P>
<P>        prob = abProbToken[bBackOffProbToken] *</P>
<P>            (N-1)-gram context probability</P>
<P>}</P>
<P></P>
<P>An N-gram chunk might have class definitions that group like words and serve to compress the N-gram's size. Class definitions must be written in sorted order by dwClassID. (Example: The class definition for class ID 10 comes before the definition for class ID 28.) Each class definition has a header:</P>
<P>typedef struct {</P>
<P>    DWORD    dwSize;</P>
<P>    QWORD    qwTotalCounts;</P>
<P>    DWORD    dwClassID;</P>
<P>    DWORD    dwNumSequential;</P>
<P>    DWORD    dwNumRandomAccess;</P>
<P>    // WORD or DWORD awRandomAccessWordsClasses[dwNumRandomAccess];</P>
<P>    // BYTE    abProbToken[dwNumSequential+dwNumRandomAccess];</P>
<P>} NGRAMCLASSHDR;</P>
<P>dwSize - Number of bytes in this class definition, including the header, rounded up to the nearest DWORD (4 bytes). The dwSize field can be used to skip to the next class definition.</P>
<P>dwTotalCounts - Total number of words "scanned" to produce class definition. This affects how much the N-gram overrides the existing language model.</P>
<P>dwClassID - ID of the class being defined. A class can only be defined once in an N-gram.</P>
<P>dwNumSequential - Number of sequential word probability tokens. (See below for details.)</P>
<P>dwNumRandomAccess - Number of random access word probability tokens. (See below for details.)</P>
<P>Immediately after each NGRAMCLASSHDR comes the following data:</P>
<P>awRandomAccessWordsClasses[dwNumRandomAccess] - Array of WORD or DWORD structures (depending upon bBitsPerWord) with dwNumRandomAccess elements. These are the word IDs used to reference the probability tokens. Word/class IDs must be in ascending order so that an engine can access the data quickly.</P>
<P>abProbToken[dwNumSequential+dwNumRandomAccess] - Array of (dwNumSequential + dwNumRandomAccess) bytes representing probability tokens. The first dwNumSequential bytes correspond to the probabilities for word ID's 1 through dwNumSequential. The remaining dwNumRandomAccess bytes correspond to the word ID defined by awRandomAccessWords. (The probabilities must sum to 1.0) The probability (prob) of word/class N is:</P>
<P>if (N &lt;= dwNumSequential)</P>
<P>    prob = adwProbability[abProbToken[N-1]];</P>
<P>else {</P>
<P>    for (I = 0; I &lt; dwNumRandomAccess; I++)</P>
<P>        if (awRandomAccessWordsClasses[I] == N)</P>
<P>            break;</P>
<P>    if (I &lt; dwNumRandomAccess)</P>
<P>        prob = adwProbability[abProbToken[I+dwNumSequential]];</P>
<P>    else</P>
<P>        prob = 0;</P>
<P>}</P>
<P></P></FONT></BODY></HTML>
