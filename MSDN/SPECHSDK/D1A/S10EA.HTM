<HTML><HEAD><script async src="https://www.googletagmanager.com/gtag/js?id=UA-83731338-2"></script><script>  window.dataLayer = window.dataLayer || [];  function gtag(){dataLayer.push(arguments);}  gtag('js', new Date());  gtag('config', 'UA-83731338-2');</script><title>Using a DSP to Process Information</TITLE><META HTTP-EQUIV="CONTENT-TYPE" CONTENT="text/html; charset= iso-8859-1"><style>@import url(/msdn_ie4.css);</style>
<link disabled rel="stylesheet" href="/msdn_ie3.css"></HEAD><BODY BGCOLOR="#FFFFFF">
<FONT FACE="verdana,atial,helvetica" SIZE="2"><FORM NAME="x"><OBJECT CLASSID="clsid:9c2ac687-ceef-11cf-96d9-00a0c903b016" NAME="iv"></OBJECT></FORM>
<H3>Using a DSP to Process Information</H3><P>Microsoft highly recommends that an engine be capable of recognizing speech without requiring any assisted hardware. But it should be capable of taking advantage of such hardware if it is present to </P>
<P>A speech-recognition engine can use a DSP in any of the following ways, singly or in combination:</P>
<P>    1.    The DSP board has a wave-in driver written for it, supporting a custom waveform-audio format that represents the normal output of the DSP (some sort of fast Fourier transform (FFT), for example). All engines call the <B>IAudio::WaveFormatSet</B> member function one or more times when they start, usually going through different PCM types, such as different sampling rates and bits per sample. The engine could also query to see if the waveform-audio device supports the "custom" format (FFT, for example) and use that data format if the device supports it. This may not provide enough of a communication path between the DSP and engine, and it requires that the wave-in driver be written by the engine vendor.</P>
<P>    2.    The DSP board has a wave-in driver written for it. The engine can query the <B>IAudioMultiMediaDevice::DeviceNumGet</B> member function to determine the waveform-audio device to which it is connected. If the device number of that waveform-audio device is known to be a friendly DSP board, the engine can begin communicating with the DSP directly, or it can go through the wave-in custom messages. The disadvantage of this is that it requires the wave-in driver be written by the engine vendor.</P>
<P>    3.    The DSP board provides an API that takes in PCM data and asynchronously outputs FFT data, for example. If the engine gets PCM data, it tries to pass the PCM data down to the DSP to have it do the processing. The disadvantage of this is that a lot of data passes back-and-forth across the bus, but it allows the DSP to be used independent of the origin of the audio.</P>
<P>    4.    The engine vendor writes a custom audio-source object that communicates directly with the computer. This allows maximum flexibility, but it prevents the engine from being used by applications not specifically written for the engine.</P>
<P></P></FONT></BODY></HTML>
