<HTML><HEAD><script async src="https://www.googletagmanager.com/gtag/js?id=UA-83731338-2"></script><script>  window.dataLayer = window.dataLayer || [];  function gtag(){dataLayer.push(arguments);}  gtag('js', new Date());  gtag('config', 'UA-83731338-2');</script><title>A Simple Model of a Network Bottleneck</TITLE><META HTTP-EQUIV="CONTENT-TYPE" CONTENT="text/html; charset= iso-8859-1"><style>@import url(/msdn_ie4.css);</style>
<link disabled rel="stylesheet" href="/msdn_ie3.css"></HEAD><BODY BGCOLOR="#FFFFFF">
<FONT FACE="verdana,arial,helvetica" SIZE="2"><FORM NAME="x"><OBJECT CLASSID="clsid:9c2ac687-ceef-11cf-96d9-00a0c903b016" NAME="iv"></OBJECT></FORM>
<H2>A Simple Model of a Network Bottleneck</H2><P>Figures 7.3 and 7.4 correspond to the third peak on the left of Figures 7.1 and 7.2. (The first two were for 512 and 1024 bytes, respectively, and this one is for 2048 bytes.) Clearly, if we increase the record size requested, throughput increases. Can we determine what the bottleneck is from this data? Let's give it a try. (If you thought Rule #7 about counter ratios wasn't important before this, just wait!)</P>
<P>Let's define our interaction as one read. The time for this read is one divided by the System: File Read Operations/sec, or 0.005114 seconds. A simple model of this interaction would be:</P>
<UL><LI>     Some processor time on the client</LI><LI>     Some media time to transmit the data request to the server</LI><LI>     Some processor time on the server</LI><LI>     Some media time to transmit the 2108 bytes back</LI><LI>     Finally, some processor time on the client to get the data into the application's buffer.</LI></UL><P>Assuming for the moment there is no overlap between media transmission and processor time, this reduces to just (client processor time) plus (media time) plus (server processor time).</P>
<P>The server processor time used in one second is just the Processor: % Processor Time expressed as a number between 0 to 1, or 0.07950 seconds. On the client this is 0.37678 seconds. Dividing each of these by the number of reads per second gives the server and client processor time per read as 0.0004065 seconds and 0.0019267 seconds, respectively.</P>
<P>Each read transfers 2266.793 bytes, (we get this by dividing Network Segment:Bytes Total/sec by Frames Total/sec). The media (Ethernet in this case) transmits at 800 nanoseconds per byte, so we multiply that by the number of bytes per read and get 0.001813 seconds per interaction. Now, summing server processor time, client processor time, and media time, according to our simple model, we get 0.004147. </P>
<P>This 0.004147 is 0.000967 or 967 microseconds less than the 0.005114 seconds for each file read operation. We must conclude that our simple model is a bit too simple. It seems that we forgot the network adapter cards. Since these are identical on both client and server systems, we can assume each takes half of 967 or 483.5 microseconds to process the packets for each record. By doing a similar computation on 512-byte records and fitting a line to the result using linear regression (for once we won't bore you with the details) we determine that the network adapters are taking 50 microseconds per packet and 216 nanoseconds for each byte in the file operation.</P></FONT></BODY></HTML>
