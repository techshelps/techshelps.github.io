<HTML><HEAD><script async src="https://www.googletagmanager.com/gtag/js?id=UA-83731338-2"></script><script>  window.dataLayer = window.dataLayer || [];  function gtag(){dataLayer.push(arguments);}  gtag('js', new Date());  gtag('config', 'UA-83731338-2');</script><title>Looking at Redundant Arrays of Inexpensive Disks</TITLE><META HTTP-EQUIV="CONTENT-TYPE" CONTENT="text/html; charset= iso-8859-1"><style>@import url(/msdn_ie4.css);</style>
<link disabled rel="stylesheet" href="/msdn_ie3.css"></HEAD><BODY BGCOLOR="#FFFFFF">
<FONT FACE="verdana,arial,helvetica" SIZE="2"><FORM NAME="x"><OBJECT CLASSID="clsid:9c2ac687-ceef-11cf-96d9-00a0c903b016" NAME="iv"></OBJECT></FORM>
<H2>Looking at Redundant Arrays of Inexpensive Disks</H2><P>What we touch on here is how Performance Monitor sees various redundant arrays of inexpensive disks (RAID) and fault-tolerant disk configurations. We'll also mention some issues relating to their relative performance as observed on one computer, but it would be an error to extrapolate these results to some other system. You need to perform these experiments on your own configurations and under your own real or anticipated workloads to make judgments about optimal disk configuration. </P>
<P>In our example, we have (as physical unit 0) a hardware RAID array of 4 spindles and 800 MB capacity. We partitioned this into drive C (300 MB), and drives F and G, which are 250 MB each. We also have three other disk units with about 340 MB capacity each to play with. We created a 200 MB file on a single partitioned drive D, and another one on a mirrored partition on the other two disks, drive E. After we finished the experiments on drives D and E, we rearranged those three spindles as a single striped partition for drive D (no parity) and created a 200 MB file on that.</P>
<P>We had two disk controllers, one for the hardware RAID array, and another for all three of the other disk units.</P>
<P>All the 200-MB file creation times were 420 seconds, except for the striped partition on drive D which created itself in 314 seconds. In the next two figures we show the difference in behavior of drive D as a single drive and as a striped volume.</P>
<P><img src="XOPD23.gif"></P>
<P>Figure 4.23    File creation on a single spindle</P>
<P><img src="XOPD24.gif"></P>
<P>Figure 4.24    File creation on a three-spindle striped volume without parity</P>
<P>Notice the Avg. Disk sec/Write is 0.081 for the single unit and 0.061 for the striped set. This results in higher Disk Bytes/sec. Striping reduces seeking and therefore improves performance. </P>
<P>In Figure 4.24, drive D is striped across units 1, 2, and 3. Let's look at the performance of the Physical Disks. </P>
<P><img src="XOPD25.gif"></P>
<P>Figure 4.25    Physical disk statistics for a striped volume</P>
<P>Whoops. What happened to units 2 and 3? Well, DISKPERF.SYS cannot see which physical volume the write operation executes on. This is because DISKPERF.SYS is located above the fault-tolerant disk driver FTDISK.SYS in the driver stack, as shown in Figure 4.1. The decision as to which spindle will get the data is made by FTDISK.SYS and therefore is invisible to DISKPERF.SYS. The only way to get visibility would be to add another measurement driver below FTDISK.SYS on the stack. But this would increase the overhead, and we elected not to do it. The additional information is not important enough to warrant the overhead. </P>
<P>Mirrors, stripes, and hardware RAID devices all share this Performance Monitor characteristic:  Performance Monitor summarizes all Physical Disk statistics under the first unit assigned to the disk array.</P>
<P>The next experiment was to read 100 unbuffered (with no file system cache), normally distributed records of 8192 bytes from the file on each drive type. </P>
<P><img src="XOPD26.gif"></P>
<P>Figure 4.26    Reading from three-spindle striped volume</P>
<P><img src="XOPD27.gif"></P>
<P>Figure 4.27    Reading from four-spindle hardware RAID</P>
<P>The hardware RAID is slower at this, for some reason. Perhaps its physical drives are slower. The next two figures show our old test of rereading records of various sizes to determine maximum disk throughput. The first figure shows the striped volume, the next one shows the hardware RAID.</P>
<P><img src="XOPD29.gif"></P>
<P>Figure 4.28    Disk throughput test for a three-spindle striped volume</P>
<P><img src="XOPD30.gif"></P>
<P>Figure 4.29    Disk throughput test for a four-spindle hardware RAID</P>
<P>Well, now isn't that interesting! The RAID device is quite impressive at higher transfer sizes, and increases monotonically in performance as the transfer size does. The striped volume is not so pretty. It has spots where the performance degrades due to missed revolutions. (Because we are rereading the same record over and over, only one spindle participates in this test.) But there is a serendipitous node at the 8192 transfer size, which just happened to be the size in our test case. </P>
<P>Which of these two technologies would you rather spend your money on? You need to understand the transfer size characteristics of your traffic to be sure. For 4096- or 8192-byte transfers, the striped volume wins; for transfers larger than 5 pages, the hardware RAID is the clear winner. Now don't get us in trouble by trying to use these results directly in your shop. There are a lot of variables. With a different controller, drive, or processor you get different results.</P>
<P>Another way to alter the outcome is to try writing instead of reading. When we substitute writing for reading in the above test, we get 0.016 seconds per record for the striped volume, 0.028 for the single spindle, 0.030 for the hardware RAID, and 0.041 for the mirror. Writing is slower on the mirror because both spindles must be written. If we had another controller for one half of the mirrored pair we would have possibly seen an improvement, not to mention better fault tolerance.</P>
<P>For detailed information on configuring your hardware for RAID access, refer to the RAID.DOC file on the floppy disk included with this book.</P>
<P></P></FONT></BODY></HTML>
