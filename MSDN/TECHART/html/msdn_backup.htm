<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2 Final//EN">
<HTML dir=ltr>
<HEAD>
<META HTTP-EQUIV="Content-Type" Content="text/html; charset=Windows-1252">
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-83731338-2"></script><script>  window.dataLayer = window.dataLayer || [];  function gtag(){dataLayer.push(arguments);}  gtag('js', new Date());  gtag('config', 'UA-83731338-2');</script><title>Backup and Recovery Guidelines for Microsoft SQL Server</title>
                <style>@import url(msdn_ie4.css);</style>
	<link disabled rel="stylesheet" href="msdn_ie3.css">
</HEAD>
<BODY>

<h1><sup><a name="msdn_backup"></a></sup>Backup and Recovery Guidelines for Microsoft SQL Server</h1>
<p>
Gary Schroeder<br>
SQL Server Group, Microsoft</p>
<p>
Created: February 1994<br>
Volume 4, Number 3</p>
<h2>Abstract</h2>
<p>
Microsoft® SQL Server is a powerful, multiuser, relational database management system designed to support high-volume transaction processing, as well as less demanding decision-support applications. SQL Server provides data processing capabilities (reliability, data integrity, performance, security) that meet or exceed those found in production-oriented minicomputer- and/or mainframe-based database management systems. Because of its power and stability, Microsoft SQL Server is widely used in many diverse production environments. </p>
<p>
This technical note provides guidelines for proper backup and recovery of SQL Server databases running in production environments. Much of the information discussed here is based on a compilation of common questions that Microsoft Product Support has received from SQL Server customers. Too often, customers have suffered hardware or other failures and only then realized that either they did not have backups, or their backups were done improperly, resulting in unnecessary loss of valuable data. Because this technical note is not intended to replace the existing SQL Server documentation regarding backup and recovery, you should still thoroughly read and understand the chapters on backup and recovery in the <i>Microsoft SQL Server System Administrator's Guide </i>and <i>the Microsoft SQL Administrator User's Guide</i>.</p>
<h2>Backup Strategies</h2>
<h3>Why Back Up?</h3>
<p>
Today's hardware and software components are many times more reliable than those of the not-so-distant past; however, they will never be perfect, and there is always the chance of having some type of failure. It is therefore prudent to always have some type of "safety net" in place at all times, so that if a situation arises that results in the loss or damage of online data, production can be restored quickly with minimal or no loss of data. When people think of situations that could result in the loss of data, they typically think of hard disk crashes. There are, however, other situations that can have the same end result, but are not as often considered when planning for recovery. Some of these problems include the following:
<ul type=disc>
<li>
Hard disk subsystem failure<br><br></li>
<li>
Systems software failure<br><br></li>
<li>
Accidental or malicious use of deletion statements (for example, delete from <i>prod_table</i>, drop database <i>customers</i>, format c:, and so on)<br><br></li>
<li>
Accidental or malicious use of updating statements (for example, expecting to update one row in a table but not qualifying the WHERE clause, so all rows are updated)<br><br></li>
<li>
Destructive viruses<br><br></li>
<li>
Natural disasters (fire, flood, earthquake, and so on)<br><br></li>
<li>
Theft</li>
</ul>
<p>
As the remainder of this technical note will show, Microsoft SQL Server provides many fast and reliable ways to back up your data. To ensure that your data is safeguarded, you need to define a solid backup strategy.</p>
<h3>Planning a Backup Strategy</h3>
<p>
You should have a solid backup plan in place before any application is ever moved from a development/test environment into a production environment. In addition, just as you would never put an untested application into production, you should never go into production without testing your backup strategy. It is always a good idea to include backup strategy planning as a key component of any project. In some cases, the backup strategy may even have a large impact on the design of the application. In planning the backup strategy for a given application, answer the following questions before launching the application into production:
<ul type=disc>
<li>
How often should backups be done?<br><br></li>
<li>
What will be backed up at various times (for example, full database dumps versus transaction log dumps)?<br><br></li>
<li>
To what medium will backups go (tape, diskette, disk)?<br><br></li>
<li>
Will backups be done online (while users are working) or after-hours?<br><br></li>
<li>
Will backups be done manually or by an automatic scheduling facility?<br><br></li>
<li>
If backups are automated, how will it be verified that they actually occurred without error?<br><br></li>
<li>
How long will backups be saved before reusing the storage medium?<br><br></li>
<li>
Assuming failure, how long will it take to restore to the last backup? Is that an acceptable amount of down time? If not, what is?<br><br></li>
<li>
Is there a mechanism in place to ensure that backups are good, that they can be reapplied if necessary?<br><br></li>
<li>
Where will backups be stored, and do the necessary people have access to them?<br><br></li>
<li>
Who is responsible for seeing that backups are done and done correctly?<br><br></li>
<li>
If the system administrator is gone, is there someone else who knows the proper passwords and procedures to do backups and, if necessary, restore the backups?</li>
</ul>
<p>
This, of course, is not a complete list of all the questions you should think about in planning your backup strategy. You will need to answer these questions for every SQL Server environment, and there may be many other questions specific to your particular environment.</p>
<h3>Where and When to Back Up</h3>
<p>
It is best to keep live data and backups on separate computers, or better yet, in different buildings. If databases and/or logs are dumped to physical disk devices, it is often optimal from a performance standpoint to dump them to local hard drives on the server, as opposed to drives on some other computer on the network. However, from that point they should be copied to tape or to another server, so that data and backups are not on the same computer. For convenience, most people choose to have their backups close at hand so that they can get to them quickly if needed. If this is done, keep another copy somewhere offsite (such as a bank safety deposit box) so that in the event of damage to the building, backup of data will remain safe.</p>
<p>
The frequency and type of backups you do will generally depend on two factors: the "acceptable" amount of work that can be lost due to media or other failure, and the volume of transactions that occur on the SQL Server. For many sites, databases are dumped weekly and transaction logs are dumped daily. This can vary widely, however. For systems that have little update activity and that are used primarily for decision-support, only weekly database dumps might be needed. For other high-volume online transaction processing (OLTP) environments, databases might be dumped daily and the transaction logs dumped hourly. The strategy chosen should be one that best fits your environment while providing adequate insurance of recovering needed data.</p>
<p>
If dumps are performed online, they should be scheduled for times when the server is not being heavily updated, because the dumps will slow SQL Server somewhat. In addition, the dumps should be issued on a fixed schedule. By using a fixed schedule, users will always know when the dump is occurring and can expect a slight delay in performance, or they can plan to do other non-SQL-Server–related work during that time.</p>
<p>
As an added level of data protection, SQL Server provides facilities for mirroring database devices. With device mirroring enabled, data that is written to one database device is automatically duplicated on another, separate device. Should a media failure occur on either device, the damaged device is unmirrored and the nondamaged device automatically takes over writing all transactions. Depending on the number of hard disks and hard disk controllers present in the computer, it is possible to have a configuration in which any one disk can fail and SQL Server will continue to run uninterrupted. Combining device mirroring along with a regular backup schedule can help ensure a high level of redundancy and minimize the risk of data loss.</p>
<h3>Testing a Backup Strategy</h3>
<p>
Once a backup strategy has been designed, it should be thoroughly tested. In testing the strategy, as many failures as possible should be simulated so you can be sure there are no vulnerable areas that could jeopardize the recovery of data. For example, some customers have implemented highly redundant device and disk mirroring with multiple disks in a computer, so that should any one disk drive fail, they can continue operation uninterrupted. This is a good plan, but it does not eliminate the need to do backups. Asking the question, "Will this be sufficient in the event of theft of the computer?" would show that this is a vulnerable area.</p>
<p>
Testing a backup strategy will also demonstrate how much time is required to restore databases from backups. If that amount of time is unacceptable, you may want to reevaluate the frequency with which databases and transaction logs are being dumped, or you may want to look at the feasibility of maintaining a "warm" backup server (discussed in detail later in this article).</p>
<h2>Database Dumping and Loading</h2>
<h3>Managing Database Dump Files</h3>
<p>
SQL Server databases can be dumped to diskettes, hard disks, or tape. It is a good idea to keep not only the most current dump, but the previous <i>n</i> dumps as well (the value of <i>n</i> depends on what you judge to be necessary). To illustrate the need for this, assume that some failure occurred that resulted in your need to reload the database from a dump. If only the most current dump was kept and something destroyed that dump, you would have to attempt to reconstruct your data from scratch. Or say that a user accidentally issued a misqualified update or delete statement against one or more tables. This could result in the incorrect modification or loss of large amounts of data, which could go undetected for several days, weeks, or more. By keeping a succession of previous dumps, you have multiple "safety nets" in place for recovery.</p>
<p>
If you dump your database or transaction logs to the same disk device each time, the current dump will overwrite the existing dump file, so it is best to either rename the dump file after the dump or copy it to another medium, such as tape, to preserve it. When dumping to tape devices using the OS/2® version of SQL Server, the dump will also overwrite anything existing on the tape, so it is a good idea to keep a "pool" of tapes and dump the current database to the tape containing the oldest dump. SQL Server for Windows NT™ allows you to specify whether the new dump should overwrite or be appended to the existing contents of the tape. The tapes can then be circulated for reuse in least-recently-used order. SQL Server provides a configuration parameter, "media retention," that helps to ensure that a new dump does not overwrite an existing dump too soon. The sp_configure system procedure can be used to set the "media retention" parameter, which specifies the number of days that must pass before an existing dump can be overwritten by a new one.</p>
<p>
When a database is dumped, SQL Server dumps not only the data portion, but also the transaction log for that database as well. The transaction log must be included in the dump so that when the database is later loaded, transactions that were in progress during the dump can be properly rolled back. Because the transaction log is dumped with the database, the resulting dump file may be larger than the data portion of the database. For example, assume that you have a 100-MB database with a 25-MB transaction log. If both the database and log are nearly full, the dump file would be approximately 125 MB when the database is dumped. Be sure to allow enough room on the media you are dumping to for both the data and log portions of the database being dumped.</p>
<p>
Note that with the OS/2 version of SQL Server, dump files do not "shrink" to accommodate a smaller dump. For example, assume that you have a disk dump device with a logical name of DB_DUMP and a physical file name of C:\SQL\DUMPS\DBDUMP.DAT. Suppose you dump a database to that device, and the resulting DBDUMP.DAT file is 100 MB in size. If you then dump a 5-MB database to that same device, the DBDUMP.DAT file will still appear to be 100 MB in size. This is because SQL Server simply overwrites the dump file if it already exists and writes an "end-of-dump" marker in the file to show the end of the dumped data, but the file size does not decrease. If you want to always know the exact size of the actual dump, you can delete or rename the existing dump file before dumping to it again. In this example, you could copy the 100-MB DBDUMP.DAT file to tape after making the first dump, and then delete the DBDUMP.DAT file from the hard disk. Then when you dump the 5-MB database to that device, SQL Server creates a new DBDUMP.DAT file, which will show a size of 5 MB (or less, depending on the amount of data in the database). With SQL Server for Windows NT, the size of the dump file will always be the correct size of the amount of data in the dump. For example, assume again that you dump a database to the DB_DUMP device and the resulting DBDUMP.DAT file is 100 MB. If you then dump a 5-MB database to the same device, the size of the DBDUMP.DAT file will have a size of 5 MB (or less, depending on the amount of data in the database).</p>
<p>
SQL Server for Windows NT includes an enhancement that increases the speed of database and transaction log dumping. On other platforms, the data is written to the dump device in 2K blocks. SQL Server for Windows NT writes the data to the dump device in 60K blocks, which can significantly reduce the time needed for backups.</p>
<h3>Database Consistency Checking</h3>
<p>
Regardless of the frequency of database dumps, it is highly recommended that you always run DBCC CHECKDB, DBCC CHECKALLOC, and DBCC CHECKCATALOG on a database either just before or just after dumping it, to check the logical and physical consistency of the database. Because transactions can occur during or after the DBCC but before the dump, you may want to first dump the database and then run the DBCCs to ensure that the database was consistent at the time it was dumped. If a database or transaction log that contains errors is dumped, the errors will still exist when the database is reloaded. Under some conditions this can even prevent successful reloading. There are two important reasons to run DBCC statements on a database when no users are currently using the database. First, if users are actively updating the database while DBCC is running, it can report spurious errors, which can be misleading. Second, because DBCC performs numerous checks on the data pages in the database, it can have a noticeable effect on performance if users are trying to query the database while DBCC is running.</p>
<p>
For databases with a large amount of data (for example, in excess of 5 GB), DBCC statements may take several hours to run, which can hamper the feasibility of running DBCCs on a production server. SQL Server for Windows NT addresses this by including the <b>no_index</b> option with DBCC statements. When using the <b>no_index </b>option (or when choosing the Fast checkbox in SQL Administrator), SQL Server checks only the consistency of the data pages, not the index pages, for each user table, which can dramatically reduce the time necessary to run DBCC. It is still advisable, however, to periodically run full DBCCs on the database to ensure consistency of the indexes as well. The syntax for this option is to include the <b>no_index </b>keyword after the DBCC statement. For example, DBCC CHECKDB (pubs, no_index) and DBCC CHECKTABLE (inventory, no_index) perform all the consistency checks on the data pages but skip the checks on any indexes.</p>
<p>
Another solution to running DBCCs on large amounts of production data is to have a backup server running. In this configuration, database and transaction logs can be dumped from the production server and loaded onto the backup server. The DBCCs can then be run on the backup server without negatively affecting the performance of the production server. This is also a good safety net in the event that a hardware or other failure causes the production server to become unavailable for a long period of time. In this case, the backup server can quickly become the new production server, resulting in very little disturbance to users. The rapidly declining cost of PC hardware makes this option much more feasible than with the mini/mainframe platforms. (Managing a backup server is discussed in detail later in this article.)</p>
<p>
When dumping a database, you will occasionally receive SQL Server error 3004 stating,</p>
<pre><code><i>xx</i> uninitialized pages encountered while dumping database &lt;db name&gt;. Run DBCC CHECKALLOC on this database for more information, then call technical support if there are any errors.
</code></pre>
<p>
This error occurs if users are actively updating the database while it is being dumped. The dump procedure takes a "snapshot" of the database in order to begin the dump. Because some transactions may have allocated pages but not yet initialized them at the time the "snapshot" was taken, the dump procedure raises this informational message simply as a precaution. </p>
<p>
If you encounter this message, the proper procedure, as the error message states, is to run DBCC CHECKALLOC on that database, ensuring that no users are currently using the database while DBCC is executing. Under most circumstances, CHECKALLOC will not return any errors, and the previous 3004 error can safely be ignored. If CHECKALLOC does report other errors, however, you should contact your primary support provider for further instructions.</p>
<h3>Loading Database Dumps</h3>
<p>
You need to consider several things when loading a database from a dump. First, the target database needs to be at least as large as the database that was dumped. Attempting to load a dump into a database that is smaller than the database that the dump came from will result in a SQL Server 3105 error message, stating,</p>
<pre><code>Data on dump will not fit into current database. Need <i>xx</i> Mbyte database.
</code></pre>
<p>
where <i>xx</i> is the size of the original database. You can, however, load a dump into a target database that is larger than the original database.</p>
<p>
Also consider whether the database will be loaded on the same server from which it was dumped, or if there is a potential for it to be loaded onto a different server. If the database will be dumped and loaded between servers, the code page and sort order between the servers must match. You cannot load a database on a SQL Server that is running with a code page or sort order different from the SQL Server from which the dump originated. With SQL Server for OS/2, the only way to move data between servers running different code pages or sort orders is by using the <b>bcp</b> utility. SQL Server for Windows NT version 4.21 provides an additional tool, the SQL Transfer Manager, which uses a graphical interface to allow you to easily transfer some or all of the data from one server to another. This is useful when the servers are using different sort orders or code pages, or when they are using different hardware architectures (for example, to move data from a SQL Server running on an Intel®-based computer to SQL Server for Windows NT running on a RISC-based computer). If your environment has multiple SQL Servers, it is recommended that they all be configured with the same code page and sort orders.</p>
<p>
If a database needs to be recreated and loaded from a previous dump, it is important that the database be recreated in exactly the same way that it was created. That is, the device usage, log space allocation, and segment definitions in the new database must be identical to the way they were when the database was dumped. This information is kept in the <i>sysusages </i>table in the <i>master</i> database, so you should always dump the <i>master</i> database whenever a change to a database is made that records anything in <i>master</i>. In addition, if you save the scripts you used to create the initial database, recreating it will simply be a matter of rerunning those scripts. If a dump is loaded into a database that has data, log, or segment mappings different from the original database, DBCC CHECKALLOC may report SQL Server error 2558,</p>
<pre><code>Extent not within segment: Object &lt;object id&gt;, indid &lt;index id&gt; includes extents on allocation page &lt;page number&gt; which is not in segment &lt;segment number&gt;.
</code></pre>
<p>
This error can indicate that there are data pages allocated in portions of the database intended for log pages, or log pages allocated in portions intended for data. In the first case (data pages in log space), the effect is that less space is available for the transaction log to grow. The second case (log pages in data space) is less serious, because the data space being used by log pages will eventually be freed when the log is truncated with the DUMP TRANSACTION statement.</p>
<h2>Transaction Log Dumping and Loading</h2>
<h3>Creating and Sizing Transaction Logs</h3>
<p>
SQL Server's ability to perform online incremental backups (transaction log dumps) provides a very effective mechanism for up-to-the-minute recovery of data. If your environment will be implementing both database and transaction log dumps as a means of backup, it is important to have a well-planned procedure in place and tested prior to going into production. This section contains some tips and guidelines for managing transaction logs.</p>
<p>
In order for a transaction log to be dumped to a physical dump device, the log must reside on its own device. This can be accomplished using the LOG ON clause of the CREATE DATABASE statement when the database is initially created. Or, after the database has been created, it can be altered (using the ALTER DATABASE statement) onto a new device, and the <b>sp_logdevice</b> system procedure executed to move the log to the new device. Ideally, every database should have its log on a separate device. Exceptions are small databases that are either dumped often enough to negate the need for transaction log dumps, or "test" databases whose contents are not needed or can be easily rebuilt, such as the <i>pubs </i>database. The other exception is the <i>master</i> database, which must have the data and log portions on the MASTER device.</p>
<p>
The first step in managing transaction logs is to estimate how large to make the log. The size will depend on the volume of transactions in the database and the frequency with which the log is dumped. After a transaction log is dumped to a physical dump device, the inactive portion of the log (the portion from the beginning of the log to the first noncommitted transaction) is automatically truncated to free new space for the log to use. As a general rule, creating a log that is approximately 10–25 percent of the size of the database is a good place to start—depending on your environment, you may require a log that is significantly larger or smaller than that amount. It is best to simulate the volume and types of transactions that will be expected when the system is in production. If the log fills or comes close to filling up before the scheduled transaction log dump occurs, you should either enlarge the size of the log or decrease the time between transaction log dumps.</p>
<h3>Frequency of Transaction Log Dumps</h3>
<p>
When deciding how frequently to dump the transaction log between database dumps, there are a couple of important points to consider. In the following example, assume a production application dumps its database at midnight every Friday, and transaction log dumps are performed every hour from 8 a.m. to 5 p.m. Monday through Friday (10 log dumps per day). If a failure occurs at 4:30 p.m. on Friday, resulting in the need to recover from backups, the previous Friday's database dump would need to be loaded, followed by 49 transaction log loads.</p>
<table border=1 cellpadding=5 cols=7 frame=below rules=rows>
<tr valign=top>
<td class=label width=14%>Fri</td>
<td class=label width=15%>Mon</td>
<td class=label width=14%>Tue</td>
<td class=label width=14%>Wed</td>
<td class=label width=14%>Thu</td>
<td class=label width=14%>Fri</td>
<td class=label width=15%></td>
</tr>
<tr valign=top>
<td width=14%><code>12 a.m.</code></td>
<td width=15%><code>8 a.m.</code></td>
<td width=14%><code>8 a.m.</code></td>
<td width=14%><code>8 a.m.</code></td>
<td width=14%><code>8 a.m.</code></td>
<td width=14%><code>8 a.m.</code></td>
<td width=15%><code>4:30 p.m.</code></td>
</tr>
<tr valign=top>
<td width=14%><code>--|---</code></td>
<td width=15%><code>--|---</code></td>
<td width=14%><code>--|---</code></td>
<td width=14%><code>--|---</code></td>
<td width=14%><code>--|---</code></td>
<td width=14%><code>--|---</code></td>
<td width=15%><code>--|----&gt;</code></td>
</tr>
<tr valign=top>
<td width=14%><code>database dump</code></td>
<td width=15%><code>10 log dumps</code></td>
<td width=14%><code>10 log dumps</code></td>
<td width=14%><code>10 log dumps</code></td>
<td width=14%><code>10 log dumps</code></td>
<td width=14%><code>9 log dumps</code></td>
<td width=15%><code>Failure</code></td>
</tr>
</table><br>
<p>
Although doing online log dumps results in a smaller performance hit to the SQL Server compared to database dumps, loading that many transaction logs can take a significant amount of time, resulting in a longer period of down time. A better approach is to do nightly database dumps and still do hourly transaction log dumps during the day. Assuming the same scenario of a failure at 4:30 p.m. Friday, only Thursday night's database dump would need to be loaded, followed by the 9 hourly transaction log dumps from 8 a.m. through 4 p.m. on Friday.</p>
<p>
Also consider that the larger the number of log dumps, the bigger the risk of something happening to one of them. If any one transaction log dump is misplaced, deleted, or the transaction log truncated, no further transaction logs past that point can be loaded. Think of individual transaction logs as the pairs of teeth in a zipper. If one set of teeth is missing, you can only zip the zipper up to that point—likewise, you can't "skip over" a missing transaction log.</p>
<h3>Transaction Log Sequencing</h3>
<p>
SQL Server keeps track of the order of transaction log dumps via a method that can be thought of as similar to a "sequence number." The sequence number is incremented every time a DUMP TRANSACTION statement is issued, regardless of whether the log is being dumped to a physical dump device, or whether it is being truncated by using either of the WITH TRUNCATE_ONLY or WITH NO_LOG clauses. When you restore transaction logs, they must be loaded in the same order in which they were dumped, and their sequence numbers must be in monotonically increasing numbers. </p>
<p>
In the following example, assume again that a database is dumped at midnight each night, and the transaction log is dumped to a separate disk device every hour from 8 a.m. to 5 p.m. The 8 a.m. dump would have a sequence number of 1, the 9 a.m. dump would have a sequence number of 2, and so on. Suppose that the transaction log completely fills up at 9:30 a.m., such that it must be dumped (truncated) using the WITH NO_LOG option, and the remaining hourly dumps continue as scheduled. If the hard disk fails at 3 p.m. that day, the previous night's database dump could be loaded, followed by the 8 a.m. (sequence number 1) and 9 a.m. (sequence number 2) transaction log dumps. However, attempting to load the 10 a.m. log dump would result in SQL Server error 4305,</p>
<pre><code>Specified file 'dumpfile' is out of sequence. Current time stamp is '&lt;some date and time&gt;' while dump was from '&lt;some other date and time&gt;'.
</code></pre>
<table border=1 cellpadding=5 cols=7 frame=below rules=rows>
<tr valign=top>
<td class=label width=12%><b>12 a.m.</b></td>
<td class=label width=15%><b>8 a.m.</b></td>
<td class=label width=12%><b>8:30 a.m.</b></td>
<td class=label width=16%><b>9 a.m.</b></td>
<td class=label width=16%><b>9:30 a.m.</b></td>
<td class=label width=16%><b>10 a.m.</b></td>
<td class=label width=13%><b>10:30 a.m.</b></td>
</tr>
<tr valign=top>
<td width=12%><code>|---------</code></td>
<td width=15%><code>|-----------</code></td>
<td width=12%><code>-----|----</code></td>
<td width=16%><code>|-------------</code></td>
<td width=16%><code>|------------</code></td>
<td width=16%><code>|-----------</code></td>
<td width=13%><code>|-----&gt;</code></td>
</tr>
<tr valign=top>
<td width=12%><code>database dump</code></td>
<td width=15%><code>log dump (Sequence #1)</code></td>
<td width=12%></td>
<td width=16%><code>log dump<br>
(Sequence #2)</code></td>
<td width=16%><code>LOG FULL!<br>
TRUNCATE LOG<br>
(Sequence #3)</code></td>
<td width=16%><code>log dump (Sequence #4)</code></td>
<td width=13%></td>
</tr>
</table><br>
<p>
Error 4305 would be encountered because the 9:30 a.m. truncation of the log would have been given a sequence number of 3, and the 10 a.m. dump has a sequence number of 4. Since the truncation of the log does not produce a physical dump file, it cannot be loaded. The net effect is an attempt to go from sequence number 2 to sequence number 4, which (as stated earlier) is prohibited because the sequence numbers must increase monotonically; therefore, the transactions that took place after 9 a.m. cannot be recovered. To avoid this situation, it is imperative that a full database dump be issued immediately after the transaction log is truncated, so that the sequence number in the database is reset. </p>
<h3>Effect of "trunc. log on chkpt." on Transaction Logs</h3>
<p>
Two database options have an important effect on transaction logs: <b>trunc. log on chkpt.</b> and <b>select into/bulkcopy</b>. The <b>trunc. log on chkpt.</b> option automatically performs a </p>
<pre><code>DUMP TRANSACTION &lt;database&gt; WITH TRUNCATE_ONLY
</code></pre>
<p>
statement every time the SQL Server checkpoint handler wakes up, about once every minute. Setting this option for a database is useful primarily in a development environment when backups of the transaction log are not wanted. However, if this option is set, you can rely only on recovering from your database dumps, not from your log dumps. When this option is set, SQL Server prohibits a transaction log dump to a physical dump device—attempting to do so will yield SQL Server error 4208,</p>
<pre><code>DUMP TRANsaction is not allowed while the trunc. log on chkpt. option is enabled: use DUMP DATABASE, or disable the option with sp_dboption.
</code></pre>
<p>
It is possible to turn the option off, so subsequent transaction log dumps to physical dump devices will be allowed. However, those dumps will not be recoverable, as the sequence number for the database will have already been incremented for as many times as the checkpoint handler woke up and truncated the log. </p>
<p>
The following example illustrates the effects of the <b>trunc. log on chkpt.</b> option. Assume that a database was dumped at midnight, the transaction log was dumped to a physical device at 8 a.m., and at 8:15 a.m. the <b>trunc. log on chkpt.</b> option was set for the database. At 8:45 a.m., the option was turned off, and at 9 a.m. the log was successfully dumped to another dump device. Between 8:15 and 8:45, there may have been as many as 30 automatic truncations of the transaction log (one truncation for each time the checkpoint handler woke up). If this database later needed to be restored, the database dump could be loaded, followed by the 8 a.m. transaction log dump. However, attempting to load the 9 a.m. log dump would result in SQL Server error 4305, because the log had been truncated several times between 8:15 a.m. and 8:45 a.m. while the <b>trunc. log on chkpt.</b> option was set. Therefore, if you need to rely on your transaction log dumps for recovery, you must dump the database after turning off the <b>trunc. log on chkpt.</b> option, or future log dumps will not be recoverable.</p>
<table border=1 cellpadding=5 cols=5 frame=below rules=rows>
<tr valign=top>
<td class=label width=20%>12 a.m.</td>
<td class=label width=14%>8 a.m.</td>
<td class=label width=34%>8:15 a.m.</td>
<td class=label width=17%>8:45 a.m.</td>
<td class=label width=15%>9:00 a.m. </td>
</tr>
<tr valign=top>
<td width=20%><code>|--------</code></td>
<td width=14%><code>|-----</code></td>
<td width=34%><code>|---------------------</code></td>
<td width=17%><code>|--------</code></td>
<td width=15%><code>|------&gt;</code></td>
</tr>
<tr valign=top>
<td width=20%><code>database dump</code></td>
<td width=14%><code>log dump</code></td>
<td width=34%><code>trunc log&nbsp;&nbsp; &lt;approx. 30<br>
on chkpt.&nbsp;&nbsp; automatic <br>
turned on&nbsp;&nbsp; truncations<br>
 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; of the log&gt;</code></td>
<td width=17%><code>trunc log<br>
on chkpt.<br>
turned off</code></td>
<td width=15%><code>log dump</code></td>
</tr>
</table><br>
<p>
For databases that do not have the log on a separate device, there is no harm in having the <b>trunc. log on chkpt.</b> option set. If the log is located on the same device as the data, SQL Server disallows the dumping of the transaction log to a physical device (only DUMP DATABASE is allowed for backing them up), so sequencing of the transaction log is not an issue.</p>
<h3>Effect of "select into/bulkcopy" on Transaction Logs</h3>
<p>
The <b>select into/bulkcopy </b>option must be set in a database when you want to perform nonlogged operations such as SELECT INTO or fast bulk copy (<b>bcp</b>). The relation between this option and the dumping of transaction logs differs between SQL Server versions 1.<i>x</i> and 4.2. In SQL Server version 1.<i>x</i>, attempting to dump a transaction log to a physical dump device while this option is set will result in SQL Server error 4207,</p>
<pre><code>DUMP TRANsaction is not allowed while the select into/bulk copy option is enabled: use DUMP DATABASE, or disable the option with sp_dboption.
</code></pre>
<p>
In version 1.<i>x</i>, it is possible to set the option, perform nonlogged operations, turn the option off, and do successive transaction log dumps. These transaction logs can later be loaded, but none of the nonlogged operations will be able to be recovered. SQL Server version 4.2 allows transaction logs to be dumped while this option is turned on, provided that no nonlogged activity has taken place since the last database dump.</p>
<p>
To illustrate the effect of nonlogged operations, suppose that you dump a version 1.<i>x</i> database, set the <b>select into/bulkcopy</b> option, and issue a nonlogged statement such as</p>
<pre><code>SELECT * INTO <i>newtable </i>FROM <i>oldtable</i>.
</code></pre>
<p>
You then turn the option off and dump the transaction log to a physical dump device. If the database dump and transaction log dumps are then reloaded, the database will contain the <i>newtable</i> table, but it will have zero rows in it, because the creation of the table is logged, but the insertion of the data rows is not logged.</p>
<p>
SQL Server version 4.2 detects if any nonlogged operations have been performed in a database. If an attempt is made to dump the transaction log to a physical dump device and it detects that nonlogged operations have been performed, it raises a slightly revised version of the 4207 error, stating,</p>
<pre><code>DUMP TRANsaction is not allowed while the select into/bulk copy option is enabled or if a nonlogged operation has occurred: use DUMP DATABASE, or disable the option with sp_dboption.
</code></pre>
<p>
As the error message states, it is necessary to turn off the <b>select into/bulkcopy</b> option and dump the database before transaction log dumps can be issued.</p>
<h3>Dumping a Full Transaction Log</h3>
<p>
It is possible for a database's transaction log to become so full that there is no room left to log new transactions. When this occurs, SQL Server will report error 1105,</p>
<pre><code>Can't allocate space for object 'syslogs' in database &lt;database name&gt; because the 'logsegment' segment is full. If you ran out of space in Syslogs, dump the transaction log. Otherwise, use ALTER DATABASE or sp_extendsegment to increase the size of the segment.
</code></pre>
<p>
In most cases, the log can be dumped to a physical dump device, which will free up new space in the log. When the transaction log is dumped, SQL Server first writes a checkpoint record in the log in order to get a snapshot of what transactions are currently open. It then proceeds to write out the pages of the log and then remove the inactive portion of the log. </p>
<p>
If the log is so full that there is not even enough room left to write the checkpoint record, the only option available is to issue a DUMP TRANSACTION <i>database_name</i> WITH NO_LOG statement. This statement truncates the inactive portion of the log without writing the initial checkpoint record and without logging any of the truncation operation. Because the log is not written to a physical dump device, it is not recoverable. It is therefore imperative that the database be dumped immediately so that future transaction log dumps will be recoverable. If you encounter this situation regularly, you should increase the size of the log or increase the frequency of your log dumps.</p>
<p>
SQL Server for Windows NT includes enhancements that can help prevent a transaction log from ever filling completely. Through its close integration with the Windows NT Performance Monitor, the percentage of log space used for each database can be continuously monitored. Using Performance Monitor alerts, the database administrator can have Performance Monitor automatically run a batch file to dump the transaction log when the percentage of log space used hits a predefined threshold. Using this approach, along with the automated scheduled backup facilities of SQL Administrator, the database administrator's task of managing backups has been greatly simplified and automated.</p>
<h3>Loading Transaction Logs</h3>
<p>
If a situation arises that requires you to recover files from database and transaction log dumps, the load procedure needs to be completed in one continuous step. That is, the database should first be loaded, followed by each of the transaction logs, with no activity taking place in between each load. However, users can be actively performing updates in other databases. If any updates take place in the target database before all the logs have been loaded, when the next log load is attempted, SQL Server will raise error 4306,</p>
<pre><code>There was activity on database since last load, unable to load. Must restart load sequence with the load database to continue.
</code></pre>
<p>
As the error message indicates, you must start back at the beginning by loading the database dump, followed by all the log dumps. For this reason, it is often advisable to start SQL Server in single-user mode when loading a database. In single-user mode, only one user at a time is allowed access to SQL Server, and this should be the person loading the database. Alternatively, SQL Server could be started normally, but <b>sp_dboption</b> could be used to turn on the "single user" option for the database being loaded. This allows users to freely access other databases on the server, but allows only one person to access the database being loaded (this should be the person performing the load).</p>
<p class=indent>
<B><b>Note</b></B>&nbsp;&nbsp;&nbsp;Shutting down and restarting SQL Server during a load sequence can lead to error 4306, because the SHUTDOWN statement puts a checkpoint record in each database, and each database is checkpointed when SQL Server is restarted.</p>
<h2>Managing a Backup Server</h2>
<h3>Purpose of a Backup Server</h3>
<p>
If a hard disk drive or other failure results in the computer being unusable for a time, it can be a relief to know that you have reliable backups of your data. However, making the necessary repairs and reloading the dumps of the database and logs can take considerable time. A very cost-effective alternative to this is to maintain a "warm" backup server at all times.</p>
<p>
SQL Server includes features that make it very easy to have another server "standing by" in case something should happen to the primary production server. Assume that you have two servers: Primary is the primary production SQL Server and Standby is another server on the network, also running SQL Server, with devices, databases, log-ons, and user accounts set up identically to those on Primary. Each time a database is dumped from Primary, you will immediately load it onto Standby. Likewise, you will immediately load each transaction log dump from Primary onto Standby. (It is still recommended as a further precaution that these dumps also be copied off to tape or some other medium and stored.) Should a problem arise on Primary that will require significant downtime, Primary can be taken offline, and Standby's server name changed to Primary. </p>
<p>
Using this approach, the only down time will be the few minutes necessary to take Primary offline and change Standby's name—no user applications will need to change, and the data on the server will be current through the last transaction log dump made from the old Primary server.</p>
<h3>Setting Up a Backup Server</h3>
<p>
To accomplish a backup server setup, there are a couple of changes that need to be made to the databases on Standby. After initially creating the databases on Standby, the two database options <b>read only</b> and <b>no chkpt on recovery</b> need to be set for each database that will be maintained. As stated earlier, when loading a database and transaction logs, no update activity can take place in the database until all logs have been loaded. These two options help to reduce the risk of having any activity between log loads. When SQL Server is started, a checkpoint record is written in each database after it has been recovered. Setting the <b>no chkpt on recovery</b> option prevents SQL Server from writing this checkpoint, so that if the Standby server goes down, after SQL Server is restarted the databases will still show that no activity has taken place, and transaction logs can continue to be loaded from the Primary server. </p>
<p>
If SQL Server is manually stopped by using the SHUTDOWN statement, the shutdown procedure waits for all active processes to complete and checkpoints each database before stopping. The writing of this checkpoint prevents any further transaction log loads, because there would have been an update made to the database. Setting the <b>read only</b> option not only prevents users from performing any write activity in a database, it also causes SQL Server to bypass writing a checkpoint record in the database when a SHUTDOWN statement is issued. When these two options are set for the databases on Standby, database and transaction logs should be able to continue loading regardless of how the Standby server goes down. Note that if the Standby server is needed to replace Primary, you must turn off the <b>no chkpt on recovery </b>and <b>read only</b> options before users can resume updating.</p>
<h2>Summary</h2>
<p>
Microsoft SQL Server offers unparalleled performance and integrity for a PC-based relational database management system, with a robust set of data integrity features, including device mirroring, online database and transaction log backups, and automatic roll forward/rollback recovery. SQL Server is an ideal platform for large production applications, whether they be high-volume transaction-processing systems or decision-support environments. As with any software application, it is expected that all proper precautions be taken to avoid loss of data, whether due to hardware, software, or user error. Before using any SQL Server application for production, you should have a well-tested, solid backup strategy in place so that in the event of any type of failure, production can be restored with minimal downtime and little or no loss of data.</p>
<h2>Additional Information</h2>
<p>
To receive more information about Microsoft SQL Server or to have other technical notes faxed to you, call Microsoft Developer Services FAX Request at (206) 635-2222.</p>
<h3>Available Technical Notes</h3>
<p>
<i>Microsoft Open Data Services: Application sourcebook<br>
</i>part number 098-54944</p>
<p>
<i>Designing Client-Server Applications for Enterprise Database Connectivity<br>
</i>part number 098-54943</p>
<p>
<i>Query Optimization Techniques<br>
</i>part number 098-54942</p>
<p>
<i>Microsoft SQL Server Network Integration Architecture<br>
</i>part number 098-54941</p>
<p>
<i>Using Binary Columns and Bitwise Operations in Microsoft SQL Server for Windows NT<br>
</i>part number 098-54940</p>
<p>
<i>Network Configuration Options with Microsoft SQL Server for Windows NT</i><br>
part number 098-54650</p>
<p>
<i>Discussion of the ANSI SQL Standard and Microsoft SQL Server</i><br>
part number 098-34656</p>
<p>
© 1994 Microsoft Corporation. All rights reserved. </p>
<p>
Information in this document represents the current view of Microsoft Corporation on the issue discussed as of the date of publication. Because Microsoft must respond to changing market conditions, it should not be interpreted to be a commitment on the part of Microsoft and Microsoft cannot guarantee the accuracy of any information presented after the date of publication. This document is for informational purposes only. MICROSOFT MAKES NO WARRANTIES, EXPRESS OR IMPLIED, IN THIS DOCUMENT.</p>
<p>
Companies, names, and data used in examples herein are fictitious unless otherwise noted.</p>
</BODY>
</HTML>
