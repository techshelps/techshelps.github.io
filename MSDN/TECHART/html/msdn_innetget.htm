<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2 Final//EN">
<HTML dir=ltr>
<HEAD>
<META HTTP-EQUIV="Content-Type" Content="text/html; charset=Windows-1252">
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-83731338-2"></script><script>  window.dataLayer = window.dataLayer || [];  function gtag(){dataLayer.push(arguments);}  gtag('js', new Date());  gtag('config', 'UA-83731338-2');</script><title>The Internet API (or How to Get from Here to There)</title>
                <style>@import url(msdn_ie4.css);</style>
	<link disabled rel="stylesheet" href="msdn_ie3.css">
</HEAD>
<BODY>

<h1><sup><a name="msdn_innetget"></a></sup>The Internet API (or How to Get from Here to There)</h1>
<p>
Robert Coleridge<br>
Microsoft Developer Network Technology Group</p>
<p>
July 1996</p>
<p>
<OBJECT id=sample1 type="application/x-oleobject"
	classid="clsid:adb880a6-d8ff-11cf-9377-00aa003b7a11"
	width=0 height=0 hspace=0>
<PARAM name="Command" value="Sample">
<PARAM name="Item1" value="Technical Articles Sample">
<PARAM name="Item2" value="4999">
</OBJECT><a href="javascript:sample1.Click()">Click to open or copy the files in the NetGet sample application for this technical article.</a></p>
<h2>Abstract</h2>
<p>
This article discusses the Internet application programming interface (API) in general and then looks in detail at several of the API functions that would be foundational to anyone who is interested in writing Internet browser or crawler applications. By using these functions—<b>InternetOpen</b>, <b>InternetOpenUrl</b>, <b>InternetReadFile</b>, and <b>InternetCloseHandle</b>—you can easily put together a number of useful Internet-aware utilities and applications. This article also examines how to access the Internet, as well as access and download pages from the Internet.</p>
<p>
The Win32® Internet functions ("WinInet" for short) are exported from the WININET.DLL. The functions are documented in the ActiveX™ Software Development Kit (SDK), under "Microsoft Win32 Internet Programmer's Reference," or the newest release of the Win32 SDK.</p>
<p>
To compile the NetGet sample application, you also need Microsoft® Visual C++® version 4.0 or later.</p>
<h2>Introduction</h2>
<p>
Programming for the Internet used to require knowledge of various protocols such as TCP/IP, an understanding of sockets, and so on. Fortunately for the majority of us, this is no longer the case. Developing an Internet-aware application can now be done easily and painlessly. This is done through a set of API functions known as the WinINet Software Development Kit (SDK). These functions, by hiding most of the "techie" stuff for us, make programming for the Internet painless and productive. What used to take days now takes hours to create. Although there is still a certain level of knowledge required to use the API functions, it is now a significantly easier task to develop an Internet application. </p>
<p>
The WinINet SDK currently comprises four interrelated groups of functions: general Internet URL functions, FTP functions, HTTP functions, and Gopher functions. A number of functions within each group overlap, but with such an impressive array of functions available to you, the world is at your fingertips—literally.</p>
<h2>Overview of the Internet API</h2>
<p>
The Internet API provides general URL functions and protocol-specific functions, such as HTTP, FTP, and Gopher.</p>
<h3>General URL Functions</h3>
<p>
The first group of functions are more general or generic in nature in that they allow you to access FTP, HTTP, or Gopher simply by the associated URL. With this group of functions, access to Internet information is a simple three-step process. You first obtain a handle to a specified URL with one function, then use another function to read information with the handle, and lastly, close the handle. What could be easier? This all happens without your having to know much about what the functions are doing.</p>
<p>
This group also provides functions for doing things such as combining URL components, breaking or cracking a URL into its components, and moving around a URL file (similar to setting a file pointer).</p>
<h3>HTTP, FTP, and Gopher Functions</h3>
<p>
The next three groups of functions are grouped together based on their protocol. At present, there are groups of functions for HTTP, FTP, and Gopher. Each group deals with specifics of that protocol at a deeper level than does the more general, first group of functions.</p>
<p>
Each of these protocol-specific groups of functions has overlapping functionality: for example, each group uses <b>InternetConnect</b> to make the initial connection. Yet each group has functions that are more suited to the usage of that protocol. For example, the FTP functions enable you to manipulate directories, which for HTTP pages would not be of much use. The specifics of each group are beyond the scope of this article. For a comprehensive listing of the API function set, see the "Microsoft Win32 Internet Programmer's Reference," which can be found in the ActiveX™ Software Development Kit (SDK), or the newest release of the Win32 SDK.</p>
<h2>Some Specific Internet API Functions</h2>
<p>
The following sections discuss some Internet API functions that are more general or generic in their usage. I have selected the ones most likely to be used in a general Internet-aware application. To use these functions you need an Internet agent, such as Microsoft Internet Explorer. This agent, or browser, performs the actual Internet accessing, verification, and so forth.</p>
<h3>InternetOpen</h3>
<p>
For a general utility or application, you should start with the <b>InternetOpen</b> function. By specifying the Internet agent you want to do the Internet access (for example, Microsoft Internet Explorer), the type of access you want, and a few optional flags, this function returns you a handle to an Internet session. When you are finished with the connection, you must close it by passing the handle to the <b>InternetCloseHandle</b> function. For example:</p>
<pre><code>HINTERNET hInternetSession;&nbsp;&nbsp; 
hInternetSession = InternetOpen(
 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "Microsoft Internet Explorer",&nbsp; // agent
 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; INTERNET_OPEN_TYPE_PRECONFIG,&nbsp;&nbsp; // access
 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; NULL, NULL, 0);&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // defaults
.
.
.
InternetCloseHandle(hInternetSession);
</code></pre>
<p>
This will make a connection to the Internet, using the Internet agent "Microsoft Internet Explorer," and return a handle to the connection, if successful. By specifying the parameter INTERNET_OPEN_TYPE_PRECONFIG, you have requested the agent to use certain values that are stored in the registry. The rest of the parameters are set to use the default configurations. In one simple call, you have made a connection to the Internet—assuming nothing went wrong with the connection, of course! With the returned hInternetSession handle used as a parameter to the other functions, you can start accessing Internet information.</p>
<h3>InternetOpenUrl</h3>
<p>
With an Internet connection established by the <b>InternetOpen</b> function you can now access Internet information through the <b>InternetOpenUrl</b> function. This function allows you to access information on the Internet by specifying the URL you wish to access. This is done in a protocol-independent way, through HTTP, FTP, or Gopher. The function and agent sort all this out at run time. By passing in the handle obtained from a call to <b>InternetOpen</b>, along with the URL and a few optional parameters, this function (if successful) will return you a handle to the information. When you are finished with the connection to the URL, you must close it by passing the handle you received from the call to <b>InternetOpen</b> to the <b>InternetCloseHandle</b> function. You now can do want you want with that page or file (provided you do have access, of course). For example, to access the hypothel site http://www.acompany.com/welcome.htm you would simply do the following:</p>
<pre><code>HINTERNET hURL;
HINTERNET hInternetSession;
.
.
.
hURL = InternetOpenUrl(
 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; hInternetSession,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // session handle
 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "http://www.acompany.com/welcome.htm",&nbsp;&nbsp; // URL to access
 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; NULL, 0, 0, 0);&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // defaults
.
.
.
InternetCloseHandle(hURL);
</code></pre>
<p>
It is as simple as that. Of course, this sample assumes that you obtained the hInternetSession handle before you made the call to <b>InternetOpenUrl</b>.</p>
<h3>InternetReadFile</h3>
<p>
The <b>InternetReadFile</b> function is the one you would use to actually download Internet information into memory. You do this by simply passing in the handle to a URL (obtained from a previous call to <b>InternetOpenUrl)</b>, a pointer to a buffer to receive the data, and the size of the buffer. Let me show you how to read 1024 bytes of a page into memory:</p>
<pre><code>BOOL bResult;
char cBuffer[1024];&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // I'm only going to access 1K of info.
DWORD dwBytesRead;
HINTERNET hURL;

.
.
.
bResult = InternetReadFile(
 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; hURL,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // handle to URL
 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (LPSTR)cBuffer,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // pointer to buffer
 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (DWORD)1024,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // size of buffer
 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;dwBytesRead);&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // pointer to var to hold return value
</code></pre>
<p>
This example reads the information pointed to by the hURL handle, stores it in the cBuffer character buffer, and sets the variable <i>dwBytesRead</i> to the number of bytes stored in the buffer. That is all it takes to read some information off the Internet. (This example assumes that you will receive all of the information in one call. If not, you simply repeat the call until all information is received.) </p>
<p>
If the return value is TRUE and the number of bytes read is zero, the transfer has been completed and there are no more bytes to read on the handle. This is the same as reaching EOF in a local file. The <b>InternetCloseHandle</b> function should always be called when the work with this handle is done.</p>
<h3>InternetCloseHandle</h3>
<p>
The <b>InternetCloseHandle</b> function is the Internet equivalent of the Win32 <b>CloseHandle</b> API function. It is used to shut down the connections specified, be they from <b>InternetOpenUrl</b> or <b>InternetOpen</b>. It is quite simple to use:</p>
<pre><code>HINTERNET hURL;
.
.
.
InternetCloseHandle(hURL);
</code></pre>
<p>
It is imperative to remember that for Internet work, just as for Win32, you should always close connections when you are finished with them.</p>
<h3>Putting It All Together</h3>
<p>
Let's say you wanted to do something really simple, like read a file from a given URL. A code segment might look like the following:</p>
<pre><code>HINTERNET hInternetSession;&nbsp;&nbsp; 
HINTERNET hURL;
char cBuffer[1024];&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // I'm only going to access 1K of info.
BOOL bResult;
DWORD dwBytesRead;

// Make internet connection.
hInternetSession = InternetOpen(
 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "Microsoft Internet Explorer",&nbsp;&nbsp; // agent
 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; INTERNET_OPEN_TYPE_PRECONFIG,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // access
 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; NULL, NULL, 0);&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // defaults

// Make connection to desired page.
hURL = InternetOpenUrl(
 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; hInternetSession,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // session handle
 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "http://www.acompany.com/welcome.htm",&nbsp;&nbsp; // URL to access
 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; NULL, 0, 0, 0);&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // defaults

// Read page into memory buffer.
bResult = InternetReadFile(
 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; hURL,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // handle to URL
 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (LPSTR)cBuffer,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // pointer to buffer
 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (DWORD)1024,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // size of buffer
 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;dwBytesRead);&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // pointer to var to hold return value

// Close down connections.
InternetCloseHandle(hURL);
InternetCloseHandle(hInternetSession);
</code></pre>
<p>
That is all it takes to connect to, access some information from, and disconnect from a specific URL on the Internet. As I said in the beginning of this article, it is a very simple task to do.</p>
<h2>Where Can You Go from Here?</h2>
<p>
By using just a few Internet APIs I will show you how to write a console application that will "reach out and touch someone." This application, which I have called NetGet, will allow you to download information from the Internet (in page, file, or other format) and store it on your local machine. By using the Internet APIs discussed in this article plus one or two others, you will be able to get information from a single URL or multiple URLs, parse the HTML tags on these pages, and extract the files and links.</p>
<p>
In effect, you will be writing your own Internet information extractor. I give you fair warning, though: downloading a large file, or a page that has intensive graphics or links, can result in filling your hard disk. Don't say I didn't warn you! Let's examine the basic process required to first download information onto your hard drive.</p>
<h3>Downloading a Single Page</h3>
<p>
This procedure is the simplest to do. You first make a connection to the Internet via <b>InternetOpen</b>, and then make a connection to the desired URL with <b>InternetOpenUrl</b>. With that handle, download the information into memory with I<b>nternetReadFile</b>. Once that information is in memory, you simply write the data to your local hard drive.</p>
<p>
The following example assumes that you are only downloading a small file (1024 bytes). For the sake of brevity, some parameter lists are not filled in.</p>
<pre><code>BOOL GetURLPageAndStoreToDisk(LPSTR pURLPage, LPSTR pOutputFile)
 &nbsp; {
HINTERNET hSession;&nbsp;&nbsp; 
HINTERNET hURL;
char cBuffer[1024];&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // Assume small page for sample.
BOOL bResult;
DWORD dwBytesRead;
HANDLE hOutputFile;

// Make internet connection.
hSession = InternetOpen("Microsoft Internet Explorer", . . .

// make connection to desired page
hURL = InternetOpenUrl(hSession, pURLPage, . . .

// read page into memory buffer
bResult = InternetReadFile(hURL, (LPSTR)cBuffer,
 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (DWORD)1024, &amp;dwBytesRead);

// close down connections
InternetCloseHandle(hURL);
InternetCloseHandle(hInternetSession);

// create output file
hOutputFile = CreateFile)pOutputFile, . . .

// write out data
bResult = WriteFile(hOutputFile, cBuffer, . . .

// close down file
CloseHandle(hOutputFile);

// return success
return(TRUE);
}
</code></pre>
<h3>Downloading Multiple Pages</h3>
<p>
Now that you have looked at how to download a single file from the Internet, all you need to do to download multiple files is repetitively call the single-page procedure and pass in different URL references. For example:</p>
<pre><code>GetURLPageAndStoreToDisk(
 &nbsp; "HTTP://WWW.SOMESITE.COM/INTERESTING.HTM",
 &nbsp; "C:\\PAGES\\INTEREST.HTM");
GetURLPageAndStoreToDisk(
 &nbsp; "HTTP://WWW.OTHERSITE.COM/COOL.GIF",
 &nbsp; "C:\\PAGES\\COOL.GIF");
etc.
</code></pre>
<p>
Note that the use of the double backslash ("\\") in the code above has to do with C/C++ language syntax. It may need to be only single (\) in another language, such as Visual Basic. For details, see your language reference.</p>
<h3>Parsing the HTML Tags</h3>
<p>
Parsing the HTML data is the most challenging aspect of this type of application. HTML is a tag-based language. What do I mean by "tag-based language"? Let's examine a line of HTML.</p>
<pre><code>&lt;a href="menu.htm"&gt;&lt;img src="img/blue-icon.gif" border="0"&gt;&lt;/a&gt;
</code></pre>
<p>
There are two "tag" sets on this line. There is the "&lt;a href . . .&gt; . . . &lt;/a&gt;" tag set and the "&lt;img . . .&gt;" tag. Both of these tags contain URLs. The "&lt;a" tag contains "menu.htm", and the "&lt;img" tag contains "img/blue-icon.gif". These URLs are <i>relative</i> to the page they are contained on. Either one could just have easily read something like "http://www.home.com/welcome.htm", which is an <i>absolute</i> URL.</p>
<p>
The difficulty in parsing the tags presents us with two different challenges. The first challenge, as you have seen, is determining if a URL is relative or absolute, and adjusting it accordingly. The second challenge comes from trying to determine which tags you need to examine and which ones are meaningless for your application. For the NetGet application, I have supplied a simple data file containing a list of relevant tags. This file can be modified at any time, thus making the application more usable.</p>
<p>
Please note that <i>not </i>all pages can be downloaded. For example, an .ASP file, which is a Microsoft Internet Server library module, often sits in an "execute-only" virtual directory somewhere. For these reasons I have coded the sample application to read another data file, containing acceptable extensions, and to download URLs with these extensions only. This file can be modified and extended by adding whatever extensions you would like.</p>
<h2>The NetGet Sample Application</h2>
<p>
Using only the above-mentioned APIs (and one or two other simple API functions) I have put together a sample application that demonstrates how to do the things discussed in this article. With this application you can:
<ul type=disc>
<li>
Download a single page.<br><br></li>
<li>
Download multiple pages.<br><br></li>
<li>
Extract all graphics files from a page or multiple pages.<br><br></li>
<li>
Extract all links from a page or multiple pages.<br><br></li>
<li>
Create a listing of all links on a page.<br><br></li>
<li>
Download all pages referenced within an HTML document.<br><br></li>
<li>
Download all pages whose URLs are listed in a data file.</li>
</ul>
<p>
In order to do these things I have provided functions to:
<ul type=disc>
<li>
Parse an HTML document.<br><br></li>
<li>
Split and combine URL components similar to the C run time <b>splitpath</b> and <b>makepath</b> functions.</li>
</ul>
<h3>HTML Parsing</h3>
<p>
The first group of key functions this article examines are those used in conjunction with the HTML data. They are <b>LookForTagAndExtractValue</b> and <b>ExtractTagValue</b>. These functions, respectively, scan a line of data for a specified HTML tag, and extract the URL reference from that tag.</p>
<p>
The first function, <b>LookForTagAndExtractValue</b>, operates by scanning the line of data for any tag found in the tag table. Once it has found what appears to be a match, it calls the <b>ExtractTagValue</b> function, which will extract the value from the tag. If a value is found, <b>LookForTagAndExtractValue</b> will null-terminate the returned string at any end-of-tag delimiter. If a tag and value are found, a result of TRUE is returned to the caller. If not found, FALSE is returned.</p>
<p>
The second function, <b>ExtractTagValue</b>, simply scans through the data passed to it, looking for a specified keyword. Once it has found what appears to be a match, it looks on either side of the found tag to see if it is stand-alone (that is, does not have alphanumeric characters on either side, but only spaces or punctuation). If it has found a valid match, a pointer to the data is returned; if a match is not found, a NULL result is returned.</p>
<h3>URL Retrieval and Parsing</h3>
<h4>Retrieval</h4>
<p>
The sample code examined in the "Putting It All Together" section of this article is the function to actually retrieve Internet information via a URL. The function in the NetGet sample, called <b>GetURLPage</b>, does more than the "Putting It All Together" function. It encompasses the following:
<ul type=disc>
<li>
Making a connection to a specified URL.<br><br></li>
<li>
Parsing the URL to obtain an output filename.<br><br></li>
<li>
Creating a data file, using the filename.<br><br></li>
<li>
Reading the all the information from the URL, looping if necessary.<br><br></li>
<li>
Closing the URL connection.</li>
</ul>
<p>
The class was written to store a handle to the Internet connection, returned from a previous call to <b>StartInternetSession</b>. The function takes three parameters that require some explanation.
<ul type=disc>
<li>
The first parameter is the URL to retrieve, which could use any protocol (HTTP, FTP, or Gopher). As stated before, the Internet agent being used will work out the details of what to do.<br><br></li>
<li>
The second parameter is the root directory where you want all of the information to be written. For example, if you pass in a subdirectory called INETDOWNLOAD and you retrieved two pages, one called http:/www.site.com/reference.htm and the other called http:/www.offsite.uk/manual.htm, the function would create three subdirectories: one named \INETDOWNLOAD plus two subdirectories beneath it, named \www.site.com and \www.offsite.uk. The reason for creating such subdirectories is that it keeps the downloaded files separate: All files downloaded from the www.site.com site would go into the eponymous subdirectory, likewise all files from the www.offsite.uk site would go into \www.offsite.uk.<br><br></li>
<li>
The third parameter passed in is a pointer to a storage buffer that will receive the complete output file specification of the downloaded Internet information. The results at times may not be immediately obvious. If you pass in a URL of ftp:/www.site.com/index.txt, the file specification is obvious: index.txt. But what would the output file specification be for a URL of http:/www.offsite.com/? No, that's not a spelling mistake—there is no file specification! In this case, the site receiving the request has set up a "default" page. You may not know the name of the default, so you simply phrase the request as such and allow the other site to resolve the "conflict." That is all fine and swell, but you still need to have a filename to write to. The <b>GetURLPage</b> function determines if there is a file specification given, and if so, will use it; if not, it will call the <b>GetDefaultFilespec</b> function. This function will generate default names: DEFAULT.HTM, DEFAULT_1.HTM, and so on.</li>
</ul>
<h4>Parsing</h4>
<p>
Once an HTML page has been downloaded, it must be parsed, if requested. This is done by the <b>ParseURLPage</b> function. This function encapsulates four functions, similar to the directory parsing functions in the Win32 API. The <b>ParseOpen</b> and <b>ParseClose</b> function pair simply initializes and cleans up the parsing structures and data. The <b>ParseFirstURL</b> and <b>ParseNextURL</b> pair are written to be used in a looping paradigm—that is, you call <b>ParseFirstURL</b> to retrieve the first URL in the HTML file, then process the URL reference, and continue calling <b>ParseNextURL</b> and processing until there are no more URLs to process.</p>
<h3>Other Sample Functions</h3>
<p>
There are other functions in the sample that are relevant and self-explanatory, for example, <b>Set_Logging</b>, which simply sets a Boolean, and <b>MyInternetSplitPath</b>, which implements a "C run time" splitpath type of routine for URLs. These functions are very well commented and left up to you to enjoy and explore.</p>
<h2>Summary</h2>
<p>
Having read this article, you now have the information necessary to write your own simple Internet Browser utility. With this information, the vast resources available to developers and programmers on the Internet are now at your fingertips. How far you go with this information and what type of utilities you write is limited only by your imagination. So dream big and have fun exploring the Internet.</p>
</BODY>
</HTML>
