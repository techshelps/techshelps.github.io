<!DOCTYPE HTML PUBLIC "-//IETF//DTD HTML//EN">
<HTML>
<HEAD><script async src="https://www.googletagmanager.com/gtag/js?id=UA-83731338-2"></script><script>  window.dataLayer = window.dataLayer || [];  function gtag(){dataLayer.push(arguments);}  gtag('js', new Date());  gtag('config', 'UA-83731338-2');</script><title>Control an External Device in DirectShow</TITLE>
<STYLE>
<!--
.tctop {color: blue}
.cpslug {color: blue; text-decoration: none}
-->
</STYLE>
<META NAME="MS-HKWD" CONTENT="Control an External Device in DirectShow">
<META HTTP-EQUIV="Content-Type" CONTENT="text/html; CHARSET=iso8859-1">
<META NAME="MS.LOCALE" CONTENT="EN-US">
<META NAME="ROBOTS" CONTENT="all">
</HEAD>
<BODY TOPMARGIN=0 LEFTMARGIN=0 BGPROPERTIES="FIXED" BGCOLOR="#FFFFFF" LINK="#000000" VLINK="#808080" ALINK="#000000">
<A NAME="pagetop"></A><A NAME="ext_devcont"></A>

<OBJECT ID="DAControl" 
  STYLE="position:absolute; left:0; top:11;width:702;height:90"
  CLASSID="CLSID:B6FFC24C-7E13-11D0-9B47-00C04FC2F51D">
</OBJECT>
<!--STYLE="width:702;height:110"-->
<SCRIPT LANGUAGE="JScript">
<!--
  // The DirectAnimation library
  m = DAControl.PixelLibrary;

  /* -- Insert your code here --*/ 

  mediaBase = "art\\";
  glowBase = m.importImage(mediaBase + "glow.gif");
  xImage = m.importImage(mediaBase + "newtransx.gif");
  xImage = xImage.Transform(m.Translate2(-303,0));
  //header1 = m.importImage(mediaBase + "header1.gif");
  //header1 = header1.Transform(m.Translate2(-301,0));

  //header2 = m.importImage(mediaBase + "hdrd3drm.gif");
  //header2 = header2.Transform(m.Translate2(-143,0));

  //header3 = m.importImage(mediaBase + "header3.gif");
  //header3 = header3.Transform(m.Translate2(157,-18));

  //background = m.Overlay(header1, m.Overlay( header2, header3));

  glowPath1 = m.Polyline( new Array( 295,26, 305,26, 305,50, 310,57, 519,57, 
    536,40, 536,17, 675,17, 675,23 ) );

  glowPath2 = m.PolyLine( new Array( 44,41, 44,25, 52,16, 84,16, 106,16 ) );

  glowPath3 = m.PolyLine( new Array( 57,55, 69,55, 78,65, 92,65, 99,58,
    99,82, 95,85, 95,93, 132,93, 132,78,
    303,78, 315,66, 549,66, 549,17, 675,17,
    675,23 ) );

  glowPath4 = m.PolyLine( new Array( 29,55, 19,55, 19,31, 38,30, 38,21,
    46,16, 106,16 ) );

  glowPath5 = m.PolyLine( new Array( 45,62, 45,81, 37,81, 41,85, 46,85,   
  51,91, 68,91, 66,81, 90,81, 90,65, 94,65, 105,47 ) );

  glow1 = glowBase.Transform( moveGlow( glowPath1, 5 ) );
  glow2 = glowBase.Transform( moveGlow( glowPath2, 2 ) );
  glow3 = glowBase.Transform( moveGlow( glowPath3, 8 ) );
  glow4 = glowBase.Transform( moveGlow( glowPath4, 2.5 ) );
  glow5 = glowBase.Transform( moveGlow( glowPath5, 2 ) );

  finalImg = m.Overlay(xImage, m.Overlay( glow5, m.Overlay( glow4, 
    m.Overlay( glow3, m.Overlay( glow2, glow1)))));


  // set the model, an image, as the model to be displayed
  DAControl.Image = finalImg;//m.Overlay(finalImg, background);
  // set the background in case of a non-windowless browser (like IE3)
  DAControl.BackgroundImage = m.SolidColorImage(m.Blue);     
     
  // start the animation       
  DAControl.Start()


  function moveGlow(path, speed)  {
    imDimX = 702;
    imDimY = 120;

    coordsXf = m.Translate2(-imDimX/2, -imDimY/2);
    path = path.Transform(coordsXf);
    return xf = m.FollowPath(path, speed).Repeat(1);
  }

//-->
</SCRIPT>
<TABLE BORDER=0 CELLSPACING="0" CELLPADDING="0" WIDTH="*">
<TR>
<TD ROWSPAN="3" VALIGN="TOP" WIDTH="*">
<IMG SRC="/directx/dxm/help/ds/art/header1.gif" WIDTH="107" HEIGHT="110" BORDER=0 ALT="DirectShow Animated Header -- Control an External Device in DirectShow"></TD>
<TD ROWSPAN="2" VALIGN="TOP" WIDTH="217"><IMG SRC="/directx/dxm/help/ds/art/hdrdshow.gif" WIDTH="210" HEIGHT="110" BORDER=0 ALT="DirectShow Animated Header -- Control an External Device in DirectShow"></TD><TD VALIGN="TOP" WIDTH="383"><IMG SRC="/directx/dxm/help/ds/art/header3.gif" WIDTH="383" HEIGHT="95" BORDER=0 ALT="*">
</TD>
<TD VALIGN="TOP" WIDTH="100%">
<IMG SRC="/directx/dxm/help/d3drm/art/spacer1.gif" WIDTH="100%" HEIGHT="94" BORDER=0 ALT="Microsoft DirectShow SDK">
</TD>
</TR><TR><TD VALIGN="TOP"><PRE><FONT FACE="VERDANA,ARIAL,HELVETICA" SIZE="1"><IMG SRC="/directx/dxm/help/ds/art/yelbtn.gif" WIDTH="11" HEIGHT="11" ALIGN="MIDDLE" BORDER=0 ALT="*"><A STYLE="color:black;font-weight:bold" HREF="/directx/dxm/help/ds/index.htm">Index</A>  <IMG SRC="/directx/dxm/help/ds/art/yelbtn.gif" WIDTH="11" HEIGHT="11" ALIGN="MIDDLE" BORDER=0 ALT="*"><A STYLE="color:black;font-weight:bold" HREF="/directx/dxm/help/ds/dssd0047.htm">Topic Contents</A>
</FONT></PRE></TD></TR>
<TR><TD COLSPAN="2" VALIGN="TOP"><PRE><FONT FACE="Verdana,Arial,Helvetica" SIZE="1"><IMG SRC="/directx/dxm/help/ds/art/yelbtn.gif" WIDTH="11" HEIGHT="11" ALIGN="MIDDLE" BORDER=0 ALT="*"><B>Previous Topic:</B> <A STYLE="color:black" HREF="/directx/dxm/help/ds/dssd0052.htm">Play a Movie in a Window Using DirectDrawEx and Multimedia Streaming</A>
<IMG SRC="/directx/dxm/help/ds/art/yelbtn.gif" WIDTH="11" HEIGHT="11" ALIGN="MIDDLE" BORDER=0 ALT="*"><B>Next Topic:</B> <A  STYLE="color:black" HREF="/directx/dxm/help/ds/dssd0054.htm">Build a Filter or Application with Visual C++ 5.x</A>
</FONT></PRE></TD></TR></TABLE><BR CLEAR=ALL>
<FONT FACE="VERDANA,ARIAL,HELVETICA" SIZE="2"><BLOCKQUOTE STYLE="margin:15pt">
<H2>Control an External Device in DirectShow</H2>
<BR CLEAR=ALL>
<P>This article provides background for developers interested in adding external device control and timecode support to DirectShow applications. It also discusses how timecode is used in the production environment and lists some typical applications that rely on external devices. Finally, it describes how external device control is implemented and provides links to the interfaces available to build VCR control- and timecode-enabled filters in DirectShow.

<P><B>Contents of this article:</B>
<UL><LI><A CLASS=TCTOP HREF="/directx/dxm/help/ds/dssd0053.htm#intro_devctrl">Introduction</A>
 <LI><A CLASS=TCTOP HREF="/directx/dxm/help/ds/dssd0053.htm#smpte_timecode">Understanding SMPTE Timecode</A>
 <LI><A CLASS=TCTOP HREF="/directx/dxm/help/ds/dssd0053.htm#typical_apps">Typical Uses of Timecode</A>
 <LI><A CLASS=TCTOP HREF="/directx/dxm/help/ds/dssd0053.htm#capt_tc">Capturing Timecode</A>
 <LI><A CLASS=TCTOP HREF="/directx/dxm/help/ds/dssd0053.htm#ext_dev">External Device Control</A> 
 <LI><A CLASS=TCTOP HREF="/directx/dxm/help/ds/dssd0053.htm#sugg_reading">References and Suggested Reading</A> 
</UL>
<A NAME="intro_devctrl"></A><P><B>Introduction</B>
<P>You can control an external device in DirectShow by implementing device control filters. These filters control devices or streams of data which are entirely external to the computer and expose interfaces such as <A HREF="/directx/dxm/help/ds/dssd0103.htm#IAMExtDevice">IAMExtDevice</A>, <A HREF="/directx/dxm/help/ds/dssd0104.htm#IAMExtTransport">IAMExtTransport</A>, <A HREF="/directx/dxm/help/ds/dssd0112.htm#IAMTimecodeGenerator">IAMTimecodeGenerator</A>, and <A HREF="/directx/dxm/help/ds/dssd0113.htm#IAMTimecodeReader">IAMTimecodeReader</A>. Generally, external device control filters do not need to expose pins. However, an example of a device control filter that does expose pins might be a filter representing a source of data such as a VCR. A pin-to-pin connection representing the data flowing from the VCR to the capture board allows the device control filter and the video capture filter to talk to each other and negotiate data types, athough they do not use the standard transport and no data would flow between the filters themselves other than control information. Applications can instantiate and directly control an external device filter, but it is strongly recommended that they are always instantiated within the context of a filtergraph, even if they are the only filter in the graph.

<P>External devices can include VCRs, video editing stations, audio tape recorders (ATRs), mixers, or any other device used in the video capture and editing process. Capture and editing requires DirectShow external device control filters to provide audio and video synchronization and precise control. You can accomplish synchronization of audio and video during playback, edit, and capture with external clocks or Society of Motion Picture and Television Engineers (SMPTE) timecode. Understanding timecode is the key to understanding external device control.

<A NAME="smpte_timecode"></A><P><B>Understanding SMPTE Timecode</B>
<P>SMPTE timecode is the glue that holds the post-production process together. It identifies video and audio sources, makes automatic track synchronization possible, and provides a container for ancillary data related to the production. You will need to understand this data stream and its application to media production, tool development, or system design.

<P>SMPTE timecode, more properly known as SMPTE time and control code, is a series of digital frame address values, flags and additional data applied to a video or audio stream, and is defined in <A CLASS=TCTOP HREF="/directx/dxm/help/ds/dssd0053.htm#sugg_reading">ANSI/SMPTE 12-1986</A>. Its purpose is to provide a machine readable address for video and audio.


<P>The most common form of a SMPTE timecode data structure an 80-bit frame that contains the following information:
<OL><LI>A time stamp in hh:mm:ss:ff (hours:minutes:seconds:frames) format.
<LI>Eight 4-bit binary groups commonly known as <A HREF="/directx/dxm/help/ds/dssd0425.htm#userbits">userbits</A>.
<LI>Various flag bits.
<LI>Synchronization sequence.
<LI>Checksum.
</OL>
<P>The DirectShow <A HREF="/directx/dxm/help/ds/dssd0298.htm#TIMECODE_SAMPLE">TIMECODE_SAMPLE</A> structure is an example of a timecode data structure that contains timecode information for video or audio data.


<P>SMPTE timecode comes in one of two types. Timecode recorded on an analog audio track as a <A HREF="/directx/dxm/help/ds/dssd0407.htm#bi_phase_mark">bi-phase mark</A> encoded signal is known as <A HREF="/directx/dxm/help/ds/dssd0416.htm#LTC">LTC</A>, or Linear TimeCode (formerly known as Longitudinal TimeCode). Each timecode frame is one video frame time in duration. The other common type of timecode is known as <A HREF="/directx/dxm/help/ds/dssd0426.htm#VITC">VITC</A>, or Vertical TimeCode. <A HREF="/directx/dxm/help/ds/dssd0426.htm#VITC">VITC</A> is usually stored on two lines of a video signal's <A HREF="/directx/dxm/help/ds/dssd0426.htm#vertical_blanking_in">vertical blanking interval</A>, somewhere between lines 10 and 20.


<P><A HREF="/directx/dxm/help/ds/dssd0416.htm#LTC">LTC </A>timecode is easy to add to a pre-recorded tape, since it is encoded in a separate audio signal. However, it cannot be read when the tape is paused, moving very slowly, or very quickly. In addition it consumes one audio channel on non-professional VCRs.

<P><A HREF="/directx/dxm/help/ds/dssd0426.htm#VITC">VITC</A> timecode, on the other hand, can be read from speeds of zero to 15 times normal speed. It can contain field-dependent data and can be read from video capture cards. However, it is not easily added to a prerecorded tape and often requires expensive hardware.


<P>SMPTE timecode also comes in one of two modes, non-drop frame and drop frame. Non-drop frame is timecode that is consistently increasing and sequential. It can act as a real-time clock and works fine for monochrome video that runs at a frame rate of exactly 30 frames per second.

<P>However, NTSC color video actually runs at a frame rate of 29.97 Hz (frames per second) because of some compatibility issues with monochrome television. This causes a problem with non-drop frame timecode because it gets out of step with real-time at the rate of 108 frames (or 3.6 seconds) per hour. This means that after 1 hour of playback, the timecode would read 00:59:56:12, assuming a start point of 00:00:00:00. This causes problems when trying to calculate show duration or using "time-of-day" referencing.

<P>A solution to this problem is to skip some frames in the count every so often so the error is reduced to something tolerable. This compensation method is called "drop frame" and is implemented by skipping the first two frames from the count at the start of each minute except minutes 00, 10, 20, 30, 40 and 50. The net result is an error of less than 1 frame per hour, or about 3 frames per 24 hour period.

<P>Drop frame is used more commonly in today's productions, although any implementation should support mixing both modes.
 


<A NAME="typical_apps"></A><P><B>Typical Uses of Timecode</B>
<P>Applications which provide video capture and editing functionality will typically require control of external devices. These applications need to identify and index video and audio frames through references to SMPTE timecode. <A HREF="/directx/dxm/help/ds/dssd0416.htm#Linear_editing">Linear editing</A> system computers generally control three or more tape machines, as well as a video switcher and possibly a digital disk recorder. The controlling computer must execute commands at precise times and therefore must get videotapes cued to specific places at specific points in time.

<P>Applications typically use timecode in a number of different ways including, but not limited to the following:

<UL><LI>Tracking of video and audio sources throughout the editorial process so an <A HREF="/directx/dxm/help/ds/dssd0410.htm#edit_decision_list">edit decision list</A>, or EDL, may be generated for archival or export to another system. To create an EDL:
<OL><LI>Shoot the video.
<LI>Capture into a nonlinear offline system that uses some form of intraframe-only compression (MJPEG, DV, etc.).
<LI>Edit the material and generate an <A HREF="/directx/dxm/help/ds/dssd0410.htm#edit_decision_list">edit decision list</A> (EDL) and offline edited master.
<LI>Import the EDL to an online system and do an "auto-assembly" using the original source material to generate the final master, adding titles and effects where required.
</OL>
<LI>Synchronizing audio to video. In feature film production, audio is usually recorded on a separate tape recorder along with timecode. Specially equipped film cameras can also record timecode on the film in between the sprocket holes. After the filmed image is electronically transferred to videotape, the timecode is used to align the audio with the picture in a process known as "synching the dailies". If the audio and video timecodes are different, VITC and LTC may sometimes be used together, one for video timecode and the other for audio timecode.

<LI>Synchronization and triggering of multiple devices such as <A HREF="/directx/dxm/help/ds/dssd0406.htm#ATR">ATR</A>s, digital disk recorder or players, VCRs, or other similar devices. This is a much broader class of synchronization than described above, and is most commonly seen in linear editing and <A HREF="/directx/dxm/help/ds/dssd0418.htm#nonlinear_editing">nonlinear editing</A> systems, closed captioning systems, and subtitling systems.

<LI>Making use of the undefined bits in the timecode, called <A HREF="/directx/dxm/help/ds/dssd0425.htm#userbits">userbits</A>. Often information such as dates, ASCII codes, or film industry information is contained in the <A HREF="/directx/dxm/help/ds/dssd0425.htm#userbits">userbits</A>, however uses of <A HREF="/directx/dxm/help/ds/dssd0425.htm#userbits">userbits</A> is limited only to the ingenuity of the user.

</UL>
<P>It quickly becomes obvious that timecode makes many things possible when properly handled. Unfortunately, there is also a lot that can go wrong, either because of poor technique or hardware malfunctions. Some things to look out for on timecoded tapes are:
<OL><LI>Unstable or drifting timecode relative to video or audio.
<LI>Poor timecode field integrity. This means an LTC word begins in the middle of a frame rather than at the beginning, or VITC is not updated on a true frame boundary. The net result is an ambiguous reference.
<LI>Unintentional VITC/LTC mismatch.
<LI>Intermittent dropouts.
<LI>Missing timecode.
<LI>Poor timecode signal quality.
<LI>Incremental frame offset from incorrectly made copies.
</OL>
<A NAME="capt_tc"></A><P><B>Capturing Timecode</B>
<P>Timecode can be generated either by an external timecode generator, by a capture card capable of generating timecode, by the device control filter itself, or by an external device such as a VCR that has a built-in timecode reader. An RS-422 connection is generally necessary if the timecode is sent to the host from an external device.

<P>Once timecode is generated, it needs to be captured either in tabular or stream format concurrently with the video or audio so that it can later be accessed during editing. This is handled in one of two ways:

<P>1 Build a table that lists the timecode discontinuities indexed to frame position within the stream, and write the table to the end of the file after capture is complete. The list might be an array of structures that look like this (NOTE, the following structure is a simplification of the DirectShow <A HREF="/directx/dxm/help/ds/dssd0298.htm#TIMECODE_SAMPLE">TIMECODE_SAMPLE</A> structure and is intended as an example only):
<PRE><FONT FACE="Courier" SIZE="2">struct&#009;{
&#009;&#009;DWORD dwOffset;&#009;// offset into stream in frames
&#009;&#009;char[11] szTC;&#009;&#009;// timecode value at offset in hh:mm:ss:ff
&#009;&#009;&#009;&#009;&#009;      //  for non drop, hh:mm:ss;ff for drop frame
&#009;&#009;} TIMECODE;
</FONT></PRE>
<P>For example, given a captured video stream with one timecode break in it, the list might look like this:

<PRE><FONT FACE="Courier" SIZE="2">{0, 02:00:00:02},
{16305, 15:21:13:29}&#009;&#009;// timecode jumps at frame 16305
</FONT></PRE>
<P>Using this table, any frame's timecode can be easily calculated.

<P>2 Treat the data as a stream and write it to the file just as video and audio are written. This is useful for rapidly changing data or even non-timecode data in the <A HREF="/directx/dxm/help/ds/dssd0426.htm#vertical_blanking_in">vertical blanking interval</A> (VBI) such as closed captioning data.

<P>Once the timecode data is properly stored with its associated frame data, applications that edit, composite, synchronize or trigger can access and use a familiar and standard indexing system. 


<A NAME="ext_dev"></A><P><B>External Device Control</B>
<P>To understand external device control, it is necessary to understand timecode. The key things to remember about timecode are:

<UL><LI>SMPTE timecode is a frame addressing system that identifies video and audio frames. It comes in many types and modes: LTC, VITC, Drop Frame, Non-Drop Frame, and operates at various frame rates: 24, 25, 29.97, and 30 frames per second.

<LI>SMPTE timecode is used in <A HREF="/directx/dxm/help/ds/dssd0410.htm#edit_decision_list">edit decision list</A>s (EDLs) which are generated for <A HREF="/directx/dxm/help/ds/dssd0419.htm#offline_editing">offline editing</A> and <A HREF="/directx/dxm/help/ds/dssd0419.htm#online_editing">online editing</A>, as a timing reference for synchronizing hardware devices, and as a vehicle for additional data such as production source or film reference information.

<LI>SMPTE timecode can be stored as a stream or table of discontinuities.

<LI>Timelines are necessary for synchronization, and can be local to the controlling computer, external synchronizer, or the controlled device itself.
</UL>
<P>Given this background, two fundamental problems exist with device control. First, hundreds of different communication protocols exist for all the various devices from all the various manufacturers. Although some devices are more widely used than others, such as VCRs and Laserdiscs, almost all have a different remote control interface. As more sophisticated professional video and audio applications continue to move to the desktop, this problem gets worse. Due to this myriad of protocols, separate DirectShow filters must be implemented for each and every external device you want to control.

<P>Second, the fundamental problem in the design of professional video and audio systems is that events must occur at precise points in time. Taking a systems view of this issue, consider the following timing diagram:



<P><IMG SRC="/directx/dxm/help/ds/art/devcont1.gif" WIDTH="353" HEIGHT="211" ALT="Basic Video Production System Timing Diagram"><P>The horizontal axis denotes time in video fields, or roughly 1/60 of a second for NTSC video. The key point here is that all signals line up in time, that is, timecode starts at the beginning of a frame (System Frame Pulse). External devices such as tape machines are aligned with the system reference, as well as digital video playback such as an AVI file run from an AVI-enabled application.

<P>Conformance to this timing requirement is achieved by various means, the most common of which is a master reference signal distributed to all components in the system. This reference is known as "<A HREF="/directx/dxm/help/ds/dssd0407.htm#blackburst">blackburst</A>" in the video world, so named because it is a composite video signal containing no active video above black level. The "burst" portion of the name refers to the color burst portion of the video signal. Each device connected to the reference is responsible for maintaining its own synchronization. This means for example, that a digital video player must switch frames during the <A HREF="/directx/dxm/help/ds/dssd0426.htm#vertical_blanking_in">vertical blanking interval</A>, a tape machine must switch into record mode during the <A HREF="/directx/dxm/help/ds/dssd0426.htm#vertical_blanking_in">vertical blanking interval</A>, commands sent to external devices via a serial port must be timed to the frame pulse, and all of these and other synchronized events must occur when the SMPTE timecode hits a predetermined value. Failure to conform to these rules results in <A HREF="/directx/dxm/help/ds/dssd0424.htm#tearing">tearing</A> of a video image or edits occurring at the wrong point in time.

<P>Accomplishing all this in the professional video world is relatively straightforward, but in the hybrid world of desktop video, it is very difficult. 

<P>Building on the concepts presented so far, the two design examples in the following diagrams illustrate a potential configuration of external devices.



<P><IMG SRC="/directx/dxm/help/ds/art/devcont2.gif" WIDTH="373" HEIGHT="250" ALT="Basic Single VCR Digital Video Editing System Block Diagram"><P><IMG SRC="/directx/dxm/help/ds/art/devcont3.gif" WIDTH="518" HEIGHT="330" ALT="Basic Multi-VCR Video Editing System Block Diagram"><P>The block diagrams show that it is relatively simple to distribute the reference signal to all of the boxes. To deal with synchronization that takes place within the computer, for example, between the timecode reader and digital video player, it is recommended that either a "vertical drive" hardware interrupt, specialized operating system services, or some other custom solution be used.


<P>Finally, if you intend to write an external device filter, you should implement the <A HREF="/directx/dxm/help/ds/dssd0103.htm#IAMExtDevice">IAMExtDevice</A>, <A HREF="/directx/dxm/help/ds/dssd0104.htm#IAMExtTransport">IAMExtTransport</A>, <A HREF="/directx/dxm/help/ds/dssd0113.htm#IAMTimecodeReader">IAMTimecodeReader</A>, <A HREF="/directx/dxm/help/ds/dssd0112.htm#IAMTimecodeGenerator">IAMTimecodeGenerator</A>, and <A HREF="/directx/dxm/help/ds/dssd0111.htm#IAMTimecodeDisplay">IAMTimecodeDisplay</A> interfaces provided by DirectShow. Additionally if you need to move binary messages to and from an external device, for example, to download executable code for the external device's microprocessor to execute, this should be accomplished by implementing the COM <A HREF="/directx/dxm/help/ds/dssd0404.htm#IDataObject">IDataObject</A> interface, which has a complete set of methods for handling binary data transfers. Use this interface for whatever custom data transfer purposes your filter needs. 


<P>For sample code that demonstrates how to implement an external device control filter see the Samples\DS\Vcrctrl folder in the DirectX Media SDK. 

<A NAME="sugg_reading"></A><P><B>References and Suggested Reading</B>
<P>For additional information on SMPTE timecode and external device control, refer to the following documentation.
<OL><LI>Proposed revision to ANSI/SMPTE 12M-1986, SMPTE Standard for Television, Audio and Film Time and Control Code, SMPTE Journal, February 1995.
<LI>SMPTE RP 135-1990 "Use of Binary User Groups in Motion-Picture Time and Control Codes"
<LI>SMPTE RP 169 "Television, Audio and Film Time and Control Code - Auxiliary Time Address Data in Binary Groups - Dialect Specification of Directory Index Locations"
<LI>SMPTE RP 179-1994 "Dialect Specification of Page-Line Directory Index for Television, Audio and Film Time and Control Code for Video-Assisted Film Editing"
<LI>"Touring the Vertical Interval", Warner Johnston, TV Technology, August 1991
<LI>"Closed Captioning in Real Time", Marc Oakrand, SMPTE Journal, June 1991
<LI>Timecode Handbook, 3rd. Edition, Cipher Digital, Inc. (available from Mix Bookshelf)
<LI>Timecode: A Users Guide, John Ratcliffe (available from Mix Bookshelf)
<LI>SMPTE RP 138 - Control Message Architecture
<LI>SMPTE RP 139 - Tributary Interconnection
<LI>SMPTE RP 163 - System Service Messages
<LI>SMPTE RP 170 - Video Tape Recorder Type Specific Messages for Digital Control Interface
<LI>SMPTE RP 171 - Type-Specific Messages for Digital Control Interface of Analog Audio Tape Recorders
<LI>SMPTE RP 172 - Common Messages for Digital Control Interface
<LI>SMPTE 275M - ESlan-1 Remote Control System
</OL>
<P><B>Note</B>  SMPTE standards and reprints available from SMPTE at (914)761-1100
<P><P><FONT FACE="MS SANS SERIF" SIZE="1" COLOR="BLACK">
<A CLASS=cpslug HREF="../cpyright.htm" TARGET="_top">&#169; 1998 Microsoft Corporation. All rights reserved. Terms of Use.</A>
</FONT>
<BR CLEAR=ALL><P>
<FONT FACE="Verdana,Arial,Helvetica" SIZE="1"><A HREF="#pagetop"><IMG SRC="/directx/dxm/help/ds/art/yelbtn.gif" WIDTH="11" HEIGHT="11" ALIGN="MIDDLE" BORDER=0 ALT="*">Top of Page</A></FONT>
</BLOCKQUOTE><P>
</FONT><P>
</BODY></HTML>
