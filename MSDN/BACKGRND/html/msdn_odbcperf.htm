<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2 Final//EN">
<HTML dir=ltr>
<HEAD>
<META HTTP-EQUIV="Content-Type" Content="text/html; charset=Windows-1252">
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-83731338-2"></script><script>  window.dataLayer = window.dataLayer || [];  function gtag(){dataLayer.push(arguments);}  gtag('js', new Date());  gtag('config', 'UA-83731338-2');</script><title>ODBC: Architecture, Performance, and Tuning</title>
                <style>@import url(msdn_ie4.css);</style>
	<link disabled rel="stylesheet" href="msdn_ie3.css">
</HEAD>
<BODY>

<h1><a name="msdn_odbcperf"></a>ODBC: Architecture, Performance, and Tuning</h1>
<p>
Colleen Lambert<br>
ODBC Product Group</p>
<p>
Created: March 22, 1994</p>
<h2>Abstract</h2>
<p>
Open database connectivity (ODBC) is Microsoft’s strategic interface for accessing data in a heterogeneous environment of relational and nonrelational database management systems (DBMS’s). Even though ODBC has enjoyed growing industry support, various reports of poor performance have surfaced among MIS professionals, analysts, and engineers. Most of these reports are based on a lack of understanding of the ODBC architecture, improper use of ODBC within applications, and poorly optimized ODBC drivers.</p>
<p>
This article first outlines some of the causes of poor performance, and presents the architecture of ODBC as an overview. A cross-section of applications is discussed with respect to how each utilizes ODBC. Inspection of these applications provides an insight into situations in which performance may suffer.</p>
<p>
Finally, the article suggests various techniques for optimizing performance of applications using ODBC and ODBC drivers.</p>
<h2>Introduction</h2>
<p>
Many MIS decision-makers, analysts, field engineers, and independent software vendors (ISVs) believe that open database connectivity (ODBC) performance is poor. The overall effect of this perception has been that ODBC is being positioned as a low-performance solution for decision support to be used only when connectivity to multiple databases is an absolute requirement. This article will review the architecture of ODBC and applications using ODBC. To adequately explore these topics, it is necessary to discuss the Microsoft® Jet database engine utilized by Microsoft Access® and Visual Basic® version 3.0. The latter part of the article discusses various application and driver optimizations to promote better performance.</p>
<h3>Functionality Versus Performance</h3>
<p>
Oddly, the perception of poor ODBC performance was, in part, perpetuated by a lack of understanding of the Jet architecture. Jet, used in Microsoft Access and Visual Basic 3.0, is an advanced database management system (DBMS) technology that provides a means of accessing desktop databases as well as remote client-server databases. As an advanced DBMS, Jet requires a very rich and functional cursor model. Although ODBC does not utilize Jet, Jet makes heavy use of ODBC for accessing remote databases. Figure 1 illustrates the relationship of Jet to ODBC and applications such as Microsoft Access and Visual Basic.</p>
<p>
<img src="odbcperf_1.gif" border=0></p>
<p class=label>
<b>Figure 1. Relationship of Jet to ODBC</b></p>
<p>
Many of the performance issues attributed solely to ODBC were likely caused by:
<ul type=disc>
<li>
Inappropriate selection of development tools for the desired level of functionality (ODBC versus Microsoft Access or Visual Basic).<br><br></li>
<li>
Under-utilization of the database technology used by the development tools.<br><br></li>
<li>
The state of many of the ODBC drivers. Some were not optimized and thus clouded the whole performance issue.</li>
</ul>
<p>
Further adding to the confusion has been the theorizing of many analysts. They surmise that because ODBC adds more layers, it is therefore slower than using a proprietary interface. While some drivers do add additional layers by mapping to the proprietary interface, other drivers, such as Microsoft SQL Server, bypass the proprietary interface and emit the database protocol directly. However, even with extra layers, the overhead in a client-server application on the network and at the server far exceeds any overhead at the client in terms of overall performance and is therefore insignificant.</p>
<p>
Some developers have also suggested that ODBC performance is poor. They cite the lack of static SQL (structured query language) support in ODBC as a cause of poor performance. Without this feature they conclude that the same level of performance cannot be achieved. This argument is not any different from comparing a proprietary, embedded API to a typical call-level interface. Vendors that support both, such as Sybase and Oracle, have moved to using stored procedures to achieve the same performance benefits. Since ODBC supports stored procedures, ODBC can make the same claim.</p>
<h3>When to Expect Decreased Performance</h3>
<p>
The assertion that ODBC’s performance is always equal to if not better than all other solutions is false. There are a few instances where ODBC may be slower. For example, depending on the driver's implementation for loading and connecting, ODBC may take longer to connect to the data source than using a proprietary API. This is related to the amount of work done under the covers during connection in order to support ODBC capabilities.</p>
<p>
Specifically, there is the time for the Driver Manager to load the driver DLL (dynamic-link library); this time will vary depending on how the driver writer specified that segments should be loaded. Also, some drivers such as the Microsoft SQL Server driver, by default, request information (such as user-defined data type information and system information) from the server at connect time. Usually the driver will expose an option to disable retrieval of such information for applications where those items are not important, which gives equivalent performance to native API connections.</p>
<p>
Drivers to non-SQL databases such as dBASE® will, in fact, be slower than going through dBASE directly in certain cases. Some people expect ODBC to have the same performance with this data going through SQL as with using the native interface. This is simply not true in all cases. On the other hand, using SQL as the interface provides operations that are not available in non-SQL databases.</p>
<p>
PC databases are built on ISAMs, which have a different data model than SQL. Some things are much easier and faster with ISAMs, such as scrolling, single table operations, and indexed operations. But others are difficult if not impossible, such as joins, aggregations, expressions, and general Boolean queries. Neither model is better; it depends on the features required by the application. For example, when an application looks for all the people in the EMPLOYEE table ordered by an index on NAME, the ISAM interface will have better performance than a SQL engine every time. But if an application needs to calculate the average age of EMPLOYEEs in all the DEPARTMENTs with more than 20 employees, SQL is much easier to program. In this example, you could accomplish the operation with an aggregate and a GROUP BY and a HAVING clause, and would have performance equivalent to the code required in the native interface, such as dBASE.</p>
<h3>The Purpose of This Article</h3>
<p>
The goal of this article is to demystify the ODBC architecture and to suggest techniques to tune applications and drivers for optimal performance.</p>
<h2>ODBC Architecture</h2>
<p>
Part of the perception of poor performance is related to a lack of understanding of the architecture of ODBC. Without an understanding of the architecture, performance issues become matters of speculation. By understanding the architecture, applications and drivers may be tuned in very specific ways to enhance performance.</p>
<h3>Architectural Overview</h3>
<p>
The ODBC architecture has four components:
<ul type=disc>
<li>
Application: performs processing and calls ODBC functions to submit SQL statements and retrieve results.<br><br></li>
<li>
Driver Manager: loads drivers on behalf of an application.<br><br></li>
<li>
Driver: processes ODBC function calls, submits SQL requests to a specific data source, and returns results to the application. If necessary, the driver modifies an application’s request so that the request conforms to syntax supported by the associated DBMS.<br><br></li>
<li>
Data source: consists of the data the user wants to access and its associated operating system, DBMS, and network platform (if any) used to access the DBMS.</li>
</ul>
<p>
The Driver Manager and driver appear to an application as one unit that processes ODBC function calls. Figure 2 illustrates the relationship among the four components.</p>
<p>
<img src="odbcperf_2.gif" border=0></p>
<p class=label>
<b>Figure 2. ODBC architecture</b></p>
<h3>Role of Applications</h3>
<p>
To interact with a data source, an application:
<ul type=disc>
<li>
Connects to the data source. It specifies the data source name and any additional information needed to complete the connection.<br><br></li>
<li>
Processes one or more SQL statements:<ul type=disc>
<li>
The application places the SQL text string in a buffer. If the statement includes parameter markers, it sets the parameter values.<br><br></li>
<li>
If the statement returns a result set, the application assigns a cursor name for the statement or allows the driver to do so.<br><br></li>
<li>
The application submits the statement for prepared or immediate execution.<br><br></li>
<li>
If the statement creates a result set, the application can inquire about the attributes of the result set, such as the number of columns and the name and type of a specific column. It assigns storage for each column in the result set and fetches the results.<br><br></li>
<li>
If the statement causes an error, the application retrieves error information from the driver and takes appropriate action.</li>
</ul>
</li>
<li>
Ends each transaction by committing it or rolling it back.<br><br></li>
<li>
Terminates the connection when it has finished interacting with the data source.</li>
</ul>
<p>
The following diagram (Figure 3) lists the ODBC function calls that an application makes to connect to a data source, process SQL statements, and disconnect from the data source. Depending on its needs, an application may call other ODBC functions.</p>
<p>
<img src="odbcperf_3.gif" border=0></p>
<p class=label>
<b>Figure 3. ODBC application function calls</b></p>
<p>
Additionally, an application can provide a variety of features external to the ODBC interface, including mail, spreadsheet capabilities, online transaction processing, and report generation. The application may or may not interact with users.</p>
<h3>Role of the Driver Manager</h3>
<p>
The Driver Manager exists as a dynamic-link library (DLL). The primary purpose of the Driver Manager is to load ODBC drivers. In addition, the Driver Manager:
<ul type=disc>
<li>
Processes several ODBC initialization and information calls.<br><br></li>
<li>
Passes ODBC function calls from application to driver.<br><br></li>
<li>
Performs error and state checking.<br><br></li>
<li>
Logs function calls made by applications (optional).</li>
</ul>
<h4>Process ODBC initialization calls</h4>
<p>
The Driver Manager processes all or a large part of many ODBC functions before passing the call to the driver (if ever). This is the case with many of the information and connection functions.</p>
<p>
<b>Information functions</b></p>
<p>
<b>SQLDataSources</b> and <b>SQLDrivers</b> are processed exclusively by the Driver Manager. These calls are never passed to the driver. For <b>SQLGetFunctions</b>, the Driver Manager processes the call if the driver does not support <b>SQLGetFunctions</b>.</p>
<p>
<b>Connection functions</b></p>
<p>
For <b>SQLAllocEnv</b>, <b>SQLAllocConnect</b>, <b>SQLSetConnectOption</b>, <b>SQLFreeConnect</b>, and <b>SQLFreeEnv</b>, the Driver Manager processes the call. The Driver Manager calls <b>SQLAllocEnv</b>, <b>SQLAllocConnect</b>, and <b>SQLSetConnectOption</b> in the driver when the application calls a function to connect to the data source (<b>SQLConnect</b>, <b>SQLDriverConnect</b>, or <b>SQLBrowseConnect</b>). The Driver Manager calls <b>SQLFreeConnect</b> and <b>SQLFreeEnv</b> in the driver when the application calls <b>SQLFreeConnect</b>.</p>
<p>
For <b>SQLConnect</b>, <b>SQLDriverConnect</b>, <b>SQLBrowseConnect</b>, and <b>SQLError</b>, the Driver Manager performs the initial processing, then sends the call to the driver associated with the connection.</p>
<h4>Pass function calls from application to driver</h4>
<p>
For any other ODBC function, the Driver Manager passes the call to the driver associated with the connection.</p>
<h4>Perform error and state checking</h4>
<p>
The Driver Manager also checks function arguments and state transitions, and checks for other error conditions before passing the call to the driver associated with the connection. This reduces the amount of error handling that a driver needs to perform. However, the Driver Manager does not check all arguments, state transitions, or error conditions for a given function. For complete information about what the Driver Manager checks, see the “Diagnostics” section of each function in the ODBC <i>Function Reference</i> and the state transition tables in Appendix B of the ODBC Software Development Kit (SDK).</p>
<h4>Log function calls made by applications</h4>
<p>
If requested, the Driver Manager records each called function in a trace file after checking the function call for errors. The name of each function that does not contain errors detectable by the Driver Manager is recorded, along with the values of the input arguments and the names of the output arguments (as listed in the function definitions).</p>
<h3>Role of Drivers</h3>
<p>
A driver is a DLL that implements ODBC function calls and interacts with a data source.</p>
<p>
The Driver Manager loads a driver when the application calls the <b>SQLBrowseConnect</b>, <b>SQLConnect</b>, or <b>SQLDriverConnect</b> function.</p>
<p>
A driver performs the following tasks in response to ODBC function calls from an application:
<ul type=disc>
<li>
Establishes a connection to a data source.<br><br></li>
<li>
Submits requests to the data source.<br><br></li>
<li>
Translates data to or from other formats, if requested by the application.<br><br></li>
<li>
Returns results to the application.<br><br></li>
<li>
Formats errors into standard error codes and returns them to the application.<br><br></li>
<li>
Declares and manipulates cursors if necessary. (This operation is invisible to the application unless there is a request for access to a cursor name.)<br><br></li>
<li>
Initiates transactions if the data source requires explicit transaction initiation. (This operation is invisible to the application.)</li>
</ul>
<h3>Role of Data Sources</h3>
<p>
A data source consists of the data a user wants to access, its associated DBMS, the platform on which the DBMS resides, and the network (if any) used to access that platform. Each data source requires that a driver provide certain information in order to connect to it. At the core level, this is defined to be the name of the data source, a user ID, and a password. ODBC extensions allow drivers to specify additional information, such as a network address or additional passwords. The data source is responsible for:
<ul type=disc>
<li>
Processing SQL requests received from a driver.<br><br></li>
<li>
Returning results to a driver.<br><br></li>
<li>
Performing all other functions normally associated with a DBMS (concurrency control, transactions, and so on).</li>
</ul>
<h2>Driver Architecture</h2>
<p>
There are two basic types of drivers in ODBC: single tier and multiple tier. With single-tier drivers, the driver processes both ODBC calls and SQL statements. In this case, the driver performs part of the data source functionality. With multiple-tier drivers, the driver processes ODBC calls and passes SQL statements to the data source. One system can contain both types of configurations. Let’s take a more detailed look at the architecture of these drivers.</p>
<h3>Single-Tier Drivers</h3>
<p>
Single-tier drivers are intended for non-SQL–based databases. The database file is processed directly by the driver. The driver processes SQL statements and retrieves information from the database. SQL statements, once parsed and translated, are passed to the database as basic file operations. A driver that manipulates an Xbase file is an example of a single-tier implementation.</p>
<p>
A single-tier driver may limit the set of SQL statements that may be submitted. The minimum set of SQL statements that must be supported by a single-tier driver is defined in the ODBC SDK <i>Programmer’s Reference</i> in Appendix C, “SQL Grammar.”</p>
<p>
Single-tier drivers are generally slower than using the native DBMS tools such as Microsoft FoxPro® because they parse and translate the SQL statements into basic file operations. The degree to which they are slower depends on how optimized this process is. Difference in speed between two different single-tier drivers is usually attributed to the method of optimization.</p>
<p>
Figure 4 illustrates a typical situation where the data source resides on the same computer as the other components of ODBC. Note that the driver contains logic to access the data source.</p>
<p>
<img src="odbcperf_4.gif" border=0></p>
<p class=label>
<b>Figure 4. Single-tier driver and data source on same computer</b></p>
<p>
With the use of single-tier drivers, the data source need not be restricted to the same computer. These drivers can also be used in classic file/server configurations, as Figure 5 illustrates.</p>
<p>
<img src="odbcperf_5.gif" border=0></p>
<p class=label>
<b>Figure 5. Single-tier driver using a client-server</b></p>
<h3>Multiple-Tier Drivers</h3>
<p>
In a multiple-tier configuration, the driver sends requests to a server that processes these requests. The requests may be SQL or a DBMS-specific format. Although the entire installation may reside on a single system, it is more often divided across platforms. Typically, the application, driver, and Driver Manager reside on one system, called the client. The database and the software that controls access to the database reside on another system, called the server. There are two types of multiple-tier drivers: two-tier and three-tier (or gateway).</p>
<h4>Two-tier drivers</h4>
<p>
Among two-tier drivers, there are two variations. The variations are conveniently defined in terms of SQL functionality, being either SQL-based or non-SQL–based.</p>
<p>
Drivers for SQL-based DBMS’s, such as Oracle or Sybase, lend themselves to a fairly straightforward implementation. The ODBC driver on the client side passes SQL statements directly to the server-based DBMS using the database’s data stream. The DBMS handles all processing of the SQL statements. Figure 6 illustrates the relationship of the ODBC driver to the SQL-based DBMS.</p>
<p>
<img src="odbcperf_6.gif" border=0></p>
<p class=label>
<b>Figure 6. ODBC driver for SQL-based DBMS</b></p>
<p>
Non-SQL–based DBMS’s require extra code on the server side to parse and translate the SQL to the native database format. There are currently two significant implementations of this. In the first (Figure 7), SQL is completely parsed and translated to basic file I/O operations. These file operations act on proprietary data on the client side.</p>
<p>
<img src="odbcperf_7.gif" border=0></p>
<p class=label>
<b>Figure 7. Non-SQL DBMS ODBC implementation using client-side SQL engine</b></p>
<p>
In the second implementation, SQL statements are parsed and translated into the native database format on the server side. There may be some partial parsing on the client side as well. Figure 8 illustrates this implementation.</p>
<p>
<img src="odbcperf_8.gif" border=0></p>
<p class=label>
<b>Figure 8. Non-SQL DBMS ODBC implementation using server-side SQL engine</b></p>
<h4>Three-tier (gateway) drivers</h4>
<p>
Another type of multiple-tier configuration is a gateway architecture. The driver passes SQL requests to a gateway process, which in turn sends the requests to the data source residing on a host. Gateway drivers may support both SQL-based and non-SQL–based gateways.</p>
<p>
Often the gateway is simply a network communications-level gateway such as in Figure 9. In this case, SQL is passed all the way to the host. This is the architecture of the Information Builders Inc. EDA/SERVER product.</p>
<p>
<img src="odbcperf_9.gif" border=0></p>
<p class=label>
<b>Figure 9. Network communications-level gateway</b></p>
<p>
In other implementations, the gateway parses and translates SQL into DBMS-specific SQL or DBMS-specific format. Gateway code on the host is also typically required. Figure 10 illustrates this architecture. MicroDecisionware provides an ODBC driver based on this architecture.</p>
<p>
<img src="odbcperf_10.gif" border=0></p>
<p class=label>
<b>Figure 10. Three-tier driver using database gateway</b></p>
<p>
Yet another architecture is the distributed relational database architecture (DRDA). Wall Data has written an ODBC driver that accommodates DRDA. Figure 11 depicts the implementation.</p>
<p>
<img src="odbcperf_11.gif" border=0></p>
<p class=label>
<b>Figure 11. ODBC DRDA implementation</b></p>
<h2>Applications Architecture</h2>
<p>
Many Microsoft and third-party applications and libraries exploit ODBC functionality. Understanding how they utilize ODBC is useful when exploring ODBC performance issues related to use of these applications.</p>
<h3>Microsoft Query and Microsoft Word</h3>
<p>
These applications call ODBC directly via the ODBC API. Word also uses ODBC for mail merges utilizing data sources. This is transparent to the user.</p>
<p>
Microsoft Query is a tool that queries ODBC data sources. It is intended as an end-user application and is written in such a manner that it calls ODBC directly. Performance of Microsoft Query is affected only by the level of optimization of the driver.</p>
<p>
The Microsoft Word Developer’s Kit provides ODBC extensions for WordBasic. This permits direct ODBC access to any DBMS that supports ODBC via ODBC drivers. The kit is available from Microsoft Press, and includes a disk that contains the ODBC extensions for WordBasic. When using WordBasic and ODBC, performance will depend on the user’s coding techniques (with respect to ODBC) and the level of optimization within the ODBC driver.</p>
<h3>Jet</h3>
<p>
Like ODBC, Jet provides transparent access to any database in your environment, regardless of the data's location and format. Jet is built around a keyset-driven cursor model. This means data is retrieved and updated based on key values. A key value is a value that uniquely identifies each record in a table.</p>
<p>
The keyset model introduces complexities in how Jet operates against ODBC data sources. Traditional relational database environments use a dataset-driven model; that is, the data in a result set is thought of as one set of records. A DBMS may provide cursor capability on the set of records within the DBMS itself, enabling the client to scroll around in the data and update specific records, but typically this is not the case. So Jet itself must implement this functionality.</p>
<p>
Visual Basic and Microsoft Access both use the Jet engine. Remember Figure 1? It overstated Visual Basic’s use of the Jet engine. The following figure (Figure 12) clarifies the relationship.</p>
<p>
<img src="odbcperf_12.gif" border=0></p>
<p class=label>
<b>Figure 12. Microsoft Access/Visual Basic relationship to Jet</b></p>
<p>
So what is it that makes Jet different from ODBC? It is helpful to answer this in terms of functionality. One difference is ease of use. The use of Jet is generally transparent to users of Visual Basic and Microsoft Access. Another difference is found in the cursor model. Although the ODBC API provides the same level of functionality that Jet cursors do, not all ODBC drivers support that functionality. The ODBC cursor library in ODBC 2.0 provides a subset of the existing ODBC functionality. The ODBC cursor library supports only the snapshot cursor model with limited update capability. With Jet, both snapshot and keyset cursors are provided, and the update capabilities are more extensive.</p>
<h4>Jet cursors</h4>
<p>
When Jet executes a query, the result set returned is either a dynaset or a snapshot. A dynaset is a live, updatable view of the data in the underlying tables. Changes to the data in the underlying tables are reflected in the dynaset as the user scrolls, and changes to the dynaset data are immediately reflected in the underlying tables. A snapshot is a nonupdatable, unchanging view of the data in the underlying tables. The result sets for dynasets and snapshots are populated in different manners.</p>
<p>
<b>Result set population</b></p>
<p>
A snapshot is populated by executing a query that pulls back all the selected columns of the rows meeting the query's criteria. A dynaset, on the other hand, is populated by a query that selects only the key columns of each qualifying row; and the actual data is retrieved via a separate query using the key values to select the row data. In both cases, these result sets are stored in memory (overflowing to disk if very large), allowing you to scroll around arbitrarily.</p>
<p>
Microsoft Access and Visual Basic populate the result set slightly differently. Microsoft Access is optimized to return answers to you as quickly as possible; as soon as the first screen of result data is available, Microsoft Access paints it. The remainder is fetched as follows:
<ul type=disc>
<li>
User scrolling: Many user actions (for example, page down, go to last record, and search) require Microsoft Access to partially or completely populate the query's result set. A snapshot fetches all data up to the position scrolled to; a dynaset fetches keys up to that point and then fetches a small amount of data surrounding that position.<br><br></li>
<li>
Idle time: While you are inactive, Microsoft Access populates the query's result set in the background. This allows faster operations when you become active again. A snapshot fetches and stores all selected columns; a dynaset fetches and stores only keys, and no other data. You can control how quickly this idle-time population occurs, using the MSysConf server-based options table.</li>
</ul>
<p>
When the population query reaches the end of the result set, a snapshot does no further data fetching; a dynaset does no more key fetching but will continue to fetch clusters of rows based on those keys, as you scroll around (see below). In addition, if a connection is needed solely for this key-fetching query, it is closed, unless either:
<ul type=disc>
<li>
There are pending results.<br><br></li>
<li>
There are pending transactions.</li>
</ul>
<p>
Visual Basic populates the result set in the same manner with the exception that it does not use background cycles to further populate the result set. User scrolling is the only manner in which population proceeds.</p>
<p>
<b>Data fetching</b></p>
<p>
When rows of data are needed (for example, to paint a datasheet), a snapshot has the data available locally. A dynaset, on the other hand, has only keys and must use a separate query to ask the server for the data corresponding to those keys. Microsoft Access asks the server for clusters of rows specified by their keys, rather than one at a time, to reduce the querying traffic.</p>
<p>
The dynaset behind a Microsoft Access datasheet/form does in fact cache a small window of data (roughly 100 rows). This slightly reduces the "liveness" of the data but greatly speeds moving around within a small area. The data can be refreshed quickly with a single keystroke and is periodically refreshed by Microsoft Access during idle time. This contrasts with a snapshot, which caches the entire result data set and cannot be refreshed except by complete reexecution of the query.</p>
<p>
In addition to background key fetching, a dynaset also fills its 100-row data window during idle time. This allows you to page up or down "instantly" once or twice, provided you give Microsoft Access at least a little idle time.</p>
<p>
The Jet data access objects (DAO) expose this caching mechanism (the 100 rows of data) in Microsoft Access 2.0 through two new recordset properties (CacheStart and CacheSize) and a new recordset method (FillCache). These apply only to dynasets (not snapshots or pass-through queries), and only when the dynaset contains at least some ODBC data. CacheStart and CacheSize indicate the beginning and length (in rows) of the local cache, while FillCache fills the cache with remote data, fetched in chunks rather than a single row at a time.</p>
<p>
<b>Performance implications</b></p>
<p>
Snapshots and dynasets differ in several performance characteristics due to their different methods of retrieving and caching data. Several points are worth noting:
<ul type=disc>
<li>
Snapshots may be faster to open and scroll through than dynasets. If your result set is small, contains no Memo or BLOB data, and you don't need to update data or see changes made by other users, use a snapshot. In Microsoft Access, set the form's Allow Updating property to "No Tables" to force the form to run on a snapshot. In Visual Basic, use a Snapshot object.<br><br></li>
<li>
For larger result sets, a dynaset is faster and more efficient. For example, moving to the end of a snapshot requires the entire result set to be downloaded to the client. But a dynaset downloads only the key columns and then fetches the last screen of data corresponding to those keys.<br><br></li>
<li>
Dynaset open time and scrolling speed are affected most negatively by the number of columns you select and the number of the query's tables that are output. Select only the columns you need; outputting all columns using Table.* is more convenient but slower. Sometimes joins are used simply as restrictions and don't need to be output at all.<br><br></li>
<li>
When a dynaset fetches the data for a given set of keys, Memo/BLOB columns are not fetched unless they are visible on the screen. If scrolling causes them to become visible, they are then fetched. You can improve performance by designing your form so that, by default, Memo columns are not visible. Either place the Memo off the right-bottom edge of the screen or add a button that renders the Memo visible when clicked. In any case, Memos are cached within the dynaset caching window, once fetched.<br><br></li>
<li>
OLE objects are never fetched in bunches, nor are they stored in the dynaset caching window, because they tend to be quite large. When a row is displayed, the OLE objects are fetched if they are visible. However, the current row's OLE objects are cached, so simple screen repainting does not require refetching.</li>
</ul>
<p>
<b>Improvements to client-server data access in Jet version 2</b></p>
<p>
As noted previously, version 2 implements several improvements to enhance access to client-server data. Among these are:
<ul type=disc>
<li>
Remote transaction management. You can now use transactions in data access objects (DAO) that properly propagate to the server. For example, these cases now work whereas they previously did not:<pre><code>BeginTrans
Set ds = d.CreateDynaset("select * from authors")
ds.Delete
ds.Close
CommitTrans/Rollback

BeginTrans
d.Execute ("UPDATE Accounts1 SET Balance = Balance + 10")
d.Execute ("UPDATE Accounts2 SET Balance = Balance - 10")
CommitTrans/Rollback
</code></pre>
<p class=tl>
This allows development of true client-server transactional applications, and highly modular code.</P></li>
<li>
Integrated SQL pass-through, row-returning, and non-row-returning. Pass-through queries are populated on demand, as rows are requested, for faster open and less network traffic.<br><br></li>
<li>
A row-returning pass-through query is just like a snapshot; you can use it as the basis for queries, forms, reports, list and combo boxes, and in DAO.<br><br></li>
<li>
Pass-through queries can optionally log server warning and informational messages into a Jet table.<br><br></li>
<li>
Pass-through queries that return multiple result sets can be used. Their results are stored in multiple Jet tables.<br><br></li>
<li>
Connect-string builder: relieves the user of having to enter arcane ODBC connect strings.<br><br></li>
<li>
Attached views can be made updatable by creating a "fake unique index" on them. Using a data definition language (DDL) query, create a unique index on the attachment. This index can also be dropped using DDL. Note: the server's view updatability restrictions are not violated; attaching tables and creating Jet queries allow greater view updatability.<br><br></li>
<li>
Custom update: updates in datasheets/forms no longer update all columns. Only the columns actually edited will be included in the UPDATE statement sent to the server. This lessens network traffic (especially when Memos and OLE Objects have not been changed), and prevents gratuitous server-based trigger firing.<br><br></li>
<li>
Custom insert: inserts in datasheets/forms no longer insert all columns. Only the non-NULL columns will be included in the INSERT statement sent to the server. This reduces network traffic and prevents overwriting of server-based default values. The server-based default values will appear in the datasheet as soon as the new row is saved.<br><br></li>
<li>
Row reselect: if a server-based trigger changes the primary key of a table when a record is inserted, Jet will automatically reselect the new row, based on the other values entered in the row. This prevents it from appearing as "#Deleted" in a datasheet, thus allowing better handling of server-side counter columns or triggers that simulate such counter columns.<br><br></li>
<li>
ODBC drivers with a CURSOR_COMMIT_BEHAVIOR or CURSOR_ROLLBACK_BEHAVIOR value of zero were read-only in Jet. This restriction has been lifted; you may update tables—provided, of course, that they have unique indexes. Multiple connections, however, may be needed.<br><br></li>
<li>
If an ODBC driver supports only a single connection, ever, and doesn't allow multiple statements on that connection, then Jet ignores unique indexes when attaching tables in order to force snapshot-only mode, which requires only a single connection.<br><br></li>
<li>
The ODBC type SQL_TIME is now mapped to Text rather than to DateTime. Also, restrictions involving constant date-values and time-values work properly.<br><br></li>
<li>
More aggressive connection timeout: connections will now be timed out even if they are being used by a datasheet, form, DAO, and so on. Only two things prevent a connection from being timed out:<ul type=disc>
<li>
A pending transaction<br><br></li>
<li>
Unfetched results on a query</li>
</ul>
<p class=tl>
Connections that have been timed out will automatically be reconnected when needed. This allows developers to write applications that leave datasheets and forms on the screen, without worrying about extensive consumption of server connections.</P></li>
<li>
More MSysConf settings: two new settings in the server-based MSysConf configuration table control how fast Microsoft Access does background population of query results:<ul type=disc>
<li>
FetchDelay: indicates how often to fetch another chunk of query results<br><br></li>
<li>
FetchRows: indicates how many rows to fetch</li>
</ul>
<p class=tl>
These two settings have defaults, in case MSysConf doesn't exist: 100 rows are fetched every 10 seconds of idle time. These settings allow a system administrator to trade server locking against network traffic, and not allow ordinary users to override these settings.</P></li>
<li>
Servers that do not allow multiple statements on a connection (such as SQL Server) require two connections to browse a dynaset. A new delayed-connection algorithm allows small dynasets (fewer than 100 rows) to be browsed on a single connection, even against such servers.<br><br></li>
<li>
Remote data caching is available through DAO, using the CacheStart and CacheSize properties and the FillCache method. This also allows for more efficient "chunk-fetching" of dynaset results in DAO.<br><br></li>
<li>
A new optimization called "Remote Index Join" dramatically speeds heterogeneous queries and reduces network traffic. When a large server table is joined to a small local table on an indexed column, the large server table is not completely fetched and joined locally as in version 1.1. Rather, only the rows needed are fetched, based on the keys in the small local table. Any additional restrictions on the remote table are also sent to the server.<br><br></li>
<li>
It's no longer necessary to declare query parameters against ODBC data. Jet now infers the parameter type from its surrounding expression context and sends such restrictions to the server for processing.<br><br></li>
<li>
Constant subexpressions are internally changed into query parameters. This means that a user-defined function, expression, or domain function not involving any remote columns will be evaluated once and sent to the server as a query parameter. This allows queries such as:<pre><code>SELECT RemoteTable.*
FROM RemoteTable
WHERE RemoteColumn1 = UserDefinedFunc([QueryParameter1]) AND
 &nbsp;&nbsp;&nbsp;&nbsp; RemoteColumn2 = IIF([QueryParameter2] = "foo", 1, 2)
</code></pre>
<p class=tl>
to be sent completely to the server for processing. This is useful when the server table contains code but you wish to prompt the user for a string; the user-defined function would translate the user's string into the proper code value.</P></li>
<li>
Other operations are now sent to the server, including:<ul type=disc>
<li>
Queries containing subqueries.<br><br></li>
<li>
Unions.<br><br></li>
<li>
Conversion functions (CInt, CLng, CSng, CDbl, CCur, CStr, CVDate).</li>
</ul>
</li>
<li>
A few more Basic intrinsic functions than version 1.1:<ul type=disc>
<li>
Multicolumn outer joins (version 1.1 supported only single-column outer joins).<br><br></li>
<li>
A single outer join along with any number of inner joins. Version 1.1 sent only multiple inner joins or a single outer join, but did not send both in the same query to the server.<br><br></li>
<li>
The LIKE operator is always sent to the server, if supported. Version 1.1 sent only simple LIKE expressions.<br><br></li>
<li>
Wildcard characters in LIKE expressions are always translated from "*" and "?" to "%" and "_", even if they are entered as query parameter values by the user. Version 1.1 only translated wildcards in literal strings. This allows queries such as:<pre><code>SELECT RemoteTable.*
FROM RemoteTable
WHERE RemoteColumn LIKE [QueryParameter]
</code></pre>
<p class=tl>
to be sent completely to the server, even if the user types "*" and "?" wildcards.</P></li>
</ul>
</li>
</ul>
<h4>Visual Basic and Jet</h4>
<p>
The professional edition of Visual Basic 3.0 includes ODBC support and a variety of data-aware controls. Visual Basic developers using the data-aware controls are intrinsically bound to the Jet engine to access ODBC. The high-level controls that ship in Visual Basic 3.0 have no way of connecting directly to the ODBC API, but must pass through the Jet engine (see Option 1 of Figure 13). This provides the benefits of enabling distributed/heterogeneous joins, dynaset technology, and so on. It also introduces overhead to Visual Basic applications when remote access via ODBC is required.</p>
<p>
Since Visual Basic is such a flexible tool, there are several options available when attempting to access remote databases:
<ul type=disc>
<li>
Use the Visual Basic 3.0 custom controls, the Jet engine, and ODBC. If performance is a problem, look at the material contained in the EXTERNAL.TXT and PERFORM.TXT files shipped with Visual Basic 3.0. In addition to providing basic information about opening and accessing external databases, EXTERNAL.TXT also provides performance-related information. The PERFORM.TXT file contains performance tuning tips for Visual Basic and Microsoft Access.<br><br></li>
<li>
Use “attach table” rather than directly opening remote data sources.<br><br></li>
<li>
Use third-party custom controls (VBXs) and the associated third-party data-aware controls (for example, Coromandel or Q+E®). This approach typically has less overhead than the Jet engine.<br><br></li>
<li>
Call the ODBC API directly from Visual Basic. Since DLLs can be called from Visual Basic, any of the ODBC APIs may be called by first declaring the API as an external function and then referencing the function as any other Visual Basic function call. The ODBC SDK includes sample Visual Basic programs that provide the subroutines and declarations, and a sample application that uses the ODBC API directly.<br><br></li>
<li>
Call the proprietary API similar to Option 3 in Figure 13. The SQL Server group provides Visual Basic/SQL libraries that allow Visual Basic developers to write front-end applications to SQL Server.</li>
</ul>
<p>
<img src="odbcperf_13.gif" border=0></p>
<p class=label>
<b>Figure 13. Data management options in Visual Basic 3.0</b></p>
<h4>Microsoft Access and Jet</h4>
<p>
Visual Basic and Microsoft Access use Jet differently. One notable difference is that Microsoft Access uses Jet exclusively, whereas Visual Basic provides options such as use of Jet through data controls, use of third-party data controls, and direct access to ODBC. Another significant difference is the ability of Microsoft Access to process queries asynchronously. This is in contrast to Visual Basic, where all queries are synchronous.</p>
<p>
<b>Asynchronous query execution</b></p>
<p>
Jet executes ODBC queries asynchronously if this is supported by the ODBC driver, the network software, and the server. This allows you to cancel a long-running query in Microsoft Access or to switch to another task in the Windows™ operating system while the query runs on the server. Jet asks the server if the query is finished every <i>m</i> milliseconds, where <i>m</i> is configurable (the default is 500 milliseconds).</p>
<p>
When you cancel a query (or simply close a query before all results have been fetched), Jet calls the ODBC <b>SQLCancel</b> function, which discards any pending results and returns control to the user. However, some servers (or their network communication software) do not implement an efficient query-canceling mechanism, so you might still have to wait some time before regaining control.</p>
<p>
Asynchronous processing might cause unpredictable results with some network libraries and some servers. These network libraries are often more robust when operating synchronously, owing chiefly to the added complexities of handling multiple asynchronous connections. Client applications are often written to operate fully synchronously, even if interactive; this is simpler to implement and test. You can force Jet to operate synchronously by setting DisableAsync to 1 in the MSACC20.INI file. Also notify your network/server vendor; an upgrade or patch might be available for these problems.</p>
<p>
Jet will automatically cancel a long-running query after a configurable amount of time (the default is 60 seconds). If this happens, it does not necessarily mean that the server did not respond during that time or that you have become disconnected. It simply means the query did not return results in the time allotted. If you know a certain query will take a very long time to execute, increase the query's "ODBC Timeout" property. Each query can have its own timeout setting.</p>
<p>
When designing a Microsoft Access application that will interact with remote databases, you need to consider potential performance obstacles</p>
<p>
<b>Designing with remote databases in mind</b></p>
<p>
Elements of Microsoft Access applications that performed acceptably against local data may be too slow against a server, cause too much network traffic, or use excessive server resources. Examples include:
<ul type=disc>
<li>
Forms with many list boxes, combo boxes, and subforms, and controls containing totals: each control requires a separate query. Against local data, performance may be adequate due to data caching and in-memory query processing. Against remote data, each query must be sent to the server and a response returned, resulting in an unacceptably long delay when opening the form.<br><br></li>
<li>
Very large combo boxes: presenting a combo box of hundreds or even thousands of choices based on a local table may yield reasonable response time, especially if you define an appropriate index on the local table. Against a remote table, such a combo box will be sluggish and will drain server and network resources as it fetches the data to fill the list.<br><br></li>
<li>
Jet optimizes record searching (the Find command on the Edit menu in the user interface) to work well against local recordsets of almost any size, and remote recordsets of reasonable size. When working with large remote recordsets (thousands of records), use a filter or query, and be careful to use restrictions that your server can process.</li>
</ul>
<h3>Q+E Database Library 2</h3>
<p>
One key to a good performing ODBC application requires that the application exploit the capabilities of the database driver. In addition, the application should be aware of the capabilities and specifics of each data source. The Q+E Database Library 2 (QELIB) architecture was designed to address these issues. Figure 14 illustrates how QELIB provides data handling and driver leveling features to applications.</p>
<p>
<img src="odbcperf_14.gif" border=0></p>
<p class=label>
<b>Figure 14. QELIB architecture</b></p>
<h4>QELIB data handling</h4>
<p>
QELIB’s data handling capabilities provide the developer with the functions to manage the data access requirements. These include:
<ul type=disc>
<li>
Functions that simplify the creation, modification, and execution of SQL statements.<br><br></li>
<li>
Q+E Query Builder: a DLL that provides a user interface to build SQL statements. The resulting SQL statement is generated, checked for any conditions that may cause problems, and executed.<br><br></li>
<li>
Query by example (QBE): allows the developer to provide the user with the ability to choose data that is similar to what they want from a field and build a query.<br><br></li>
<li>
Find functions: allow the developer to locate a record from a result set by searching for the value supplied to the Find function, much like the search function in a word processor. Once you have executed an SQL SELECT statement, current-record functions let you position to individual records and update or delete the current record, or insert new records.<br><br></li>
<li>
Predefined query: SQL queries can be written and read to files.<br><br></li>
<li>
Stored procedures: several functions are provided that assist application developers in using stored procedures that assure proper operation and integration with the application.<br><br></li>
<li>
Scrolling in SQL data sources.<br><br></li>
<li>
Data conversion: conversion functions permit the developer to convert values from any of the eight standard data types to any other data type.<br><br></li>
<li>
Data formatting.<br><br></li>
<li>
Utility functions, including data dictionary, SQL parsing, and ODBC handle conversion functions.<br><br></li>
<li>
Transaction processing and record locking.</li>
</ul>
<h4>QELIB ODBC driver leveling</h4>
<p>
Two important issues in ODBC are data-source-specific attributes and ODBC driver compliance. To solve different ODBC driver variances, QELIB queries the capabilities of each driver and provides for missing functionality wherever possible. By using QELIB, applications are isolated from the problems that arise because of limited ODBC driver compliance, as shown in Figure 15.</p>
<p>
<img src="odbcperf_15.gif" border=0></p>
<p class=label>
<b>Figure 15. QELIB driver leveling</b></p>
<h2>Suggestions for Enhancing Performance</h2>
<h3>Driver Manager</h3>
<p>
There is nothing in the Driver Manager that can be “tuned” by the application or driver writer. However, Microsoft has optimized the Driver Manager in two significant ways.</p>
<p>
The Driver Manager for ODBC 2.0 does not unload driver DLLs until either the connection handle is dropped or a connection is made to a different driver on a given connection’s handle. This eliminates overhead where frequent connects/disconnects are performed through a given driver. This is in contrast to the Driver Manager for ODBC 1.0, which automatically unloaded driver DLLs on <b>SQLDisconnect</b>.</p>
<p>
In the Driver Manager 2.0, the <b>SQLGetFunctions</b> function now uses an array in which to store a map of all driver-supported ODBC functions. This is in contrast to ODBC 1.0, where a call was required for each ODBC function. This was done by the use of a new symbolic constant, SQL_API_ALL_FUNCTIONS. When this argument is passed to <b>SQLGetFunctions</b>, <i>pfExists</i> (the third parameter of the function) is treated as a pointer to an array of 100 elements.</p>
<p>
Taken from the ODBC 2.0 <i>Programmer’s Reference</i>, the following two examples show how an application uses <b>SQLGetFunctions</b>. The first example illustrates how the function was used in ODBC 1.0. The second illustrates the use of the array of support driver functions new to ODBC 2.0.</p>
<p>
In the first example, <b>SQLGetFunctions</b> is used to determine if a driver supports <b>SQLTables</b>, <b>SQLColumns</b>, and <b>SQLStatistics</b>. If the driver does not support these functions, the application disconnects from the driver. <b>SQLGetFunctions</b> is called once for each function. This is the method that was used for ODBC 1.0.</p>
<pre><code>UWORD TablesExists, ColumnsExists, StatisticsExists;

SQLGetFunctions(hdbc, SQL_API_SQLTABLES, &amp;TablesExists);
SQLGetFunctions(hdbc, SQL_API_SQLCOLUMNS, &amp;ColumnsExists);
SQLGetFunctions(hdbc, SQL_API_SQLSTATISTICS, &amp;StatisticsExists);

if (TablesExists &amp;&amp; ColumnsExists &amp;&amp; StatisticsExists) {

 &nbsp;&nbsp; /* Continue with application. */

}

SQLDisconnect(hdbc);
</code></pre>
<p>
The next example calls <b>SQLGetFunctions</b> a single time and passes it an array in which <b>SQLGetFunctions</b> returns information about all ODBC functions.</p>
<pre><code>UWORD fExists[100];

SQLGetFunctions(hdbc, SQL_API_ALL_FUNCTIONS, fExists);

if (fExists[SQL_API_SQLTABLES] &amp;&amp;
 &nbsp;&nbsp; fExists[SQL_API_SQLCOLUMNS] &amp;&amp;
 &nbsp;&nbsp; fExists[SQL_API_SQLSTATISTICS]) {

 &nbsp;&nbsp; /* Continue with application. */

}

SQLDisconnect(hdbc);
</code></pre>
<p>
Overall, the Driver Manager is written for performance by keeping overhead to a minimum. An analysis and benchmarking of the <b>SQLFetch</b> function sheds some light on the overhead associated with the Driver Manager:
<ul type=disc>
<li>
23 lines of C code<br><br></li>
<li>
95 assembler instructions<br><br></li>
<li>
332 clock ticks per execution<br><br></li>
<li>
Single execution time of 6.64 microseconds on a 486/50</li>
</ul>
<p>
Given that the blink of a human eye takes .01 seconds, <b>SQLFetch</b> can be called 10,000 times in the time it takes to blink.</p>
<h3>Applications</h3>
<p>
Many things can be done in applications to enhance performance. In general, an application should use all of a given driver’s capabilities. This is done by querying the API and SQL conformance levels of the driver. To check the SQL conformance level, use the <b>SQLGetInfo</b> function with the SQL_ODBC_SQL_CONFORMANCE flag. To test the API conformance level, use the <b>SQLGetFunctions</b> function as described above. The following techniques will help reduce overhead, thereby resulting in enhanced performance:
<ul type=disc>
<li>
Don’t call data source catalog routines (<b>SQLTables</b>, <b>SQLColumns</b>, <b>SQLPrimaryKeys</b>, and so on) unless it is absolutely necessary. If you do need to call them, do so once and cache the results locally. If you subsequently need them, retrieve the results from the local cache.<br><br></li>
<li>
Address scalability in your program design. Be prepared to handle huge result sets. For example, don’t dump all the <b>SQLTables</b> results into a single list box. It is not uncommon to have thousands of rows returned by <b>SQLTables</b>. Let the user choose if VIEW or SYNONYM table types should be displayed.<br><br></li>
<li>
Backward scrolling by use of <b>SQLFetch</b> is very time consuming. The application must close the cursor by calling <b>SQLFreeStmt</b> with the SQL_CLOSE option, reexecute the SELECT statement, and fetch rows with <b>SQLFetch</b> until the target row is retrieved. The better approach is to use <b>SQLExtendedFetch</b> for scrolling if the driver supports it. This function is used to retrieve result sets utilizing a scrollable cursor.<br><br></li>
<li>
Positioned updates and deletes will be faster on some databases (for example, DB2®), but be aware that SELECT FOR UPDATE may cause records to be locked. You might update multiple rows if the primary key is not in the SELECT list, thereby increasing time and reducing performance.<br><br></li>
<li>
Use <b>SQLPrepare</b> and <b>SQLExecute</b> for any statement that will be executed more than once. The use of prepared statements is much faster than unprepared statements. Once a statement has been prepared, it is associated with the statement handle (hstmt) until that handle is freed. Reexecuting it is just a matter of calling <b>SQLExecute</b> with that statement handle.<br><br></li>
<li>
When using <b>SQLPrepare</b> and <b>SQLExecute</b>, avoid retrieving meta-data (via <b>SQLDescribeCol</b>, <b>SQLColAttributes</b>, <b>SQLNumResultCols</b>) for a query until after <b>SQLExecute</b>. Some DBMS’s do not support the prepare and execute model. In this case, meta-data is not available following the prepare. Some drivers support return of meta-data following prepare by executing a dummy version of the query to force return of meta-data. This permits higher functionality, but it lowers performance.<br><br></li>
<li>
Use <b>SQLBindCol</b> rather than <b>SQLGetData</b>. <b>SQLGetData</b> returns the result data for a single unbound column. <b>SQLBindCol</b> assigns the storage and data type for a column in a result set. The difference is found in the number of calls to each, and the associated overhead. Although this overhead is somewhat small, it can add up. In the following code samples (taken from the ODBC 2.0 SDK <i>Programmer’s Reference</i>), it is easy to see that <b>SQLGetData</b> is called <i>n</i> times (where <i>n</i> is the number of rows in the result set) for each column, whereas <b>SQLBindCol</b> is called only once for each column.<pre><code>#define NAME_LEN 30
#define BDAY_LEN 11

UCHAR&nbsp;&nbsp;&nbsp;&nbsp; szName[NAME_LEN], szBirthday[BDAY_LEN];
SWORD&nbsp;&nbsp;&nbsp;&nbsp; sAge;
SDWORD&nbsp;&nbsp;&nbsp; cbName, cbAge, cbBirthday;

retcode = SQLExecDirect(hstmt,
 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "SELECT NAME, AGE, BIRTHDAY FROM EMPLOYEE ORDER BY 3, 2, 1",
 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; SQL_NTS);

if (retcode == SQL_SUCCESS) {
  while (TRUE) {
 &nbsp;&nbsp; retcode = SQLFetch(hstmt);
 &nbsp;&nbsp; if (retcode == SQL_ERROR || retcode == SQL_SUCCESS_WITH_INFO) {
 &nbsp;&nbsp;&nbsp;&nbsp; show_error();
  }
  if (retcode == SQL_SUCCESS || retcode == SQL_SUCCESS_WITH_INFO){

 &nbsp;&nbsp;&nbsp;&nbsp; /* Get data for columns 1, 2, and 3. */
 &nbsp;&nbsp;&nbsp;&nbsp; /* Print the row of data.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; */

 &nbsp;&nbsp;&nbsp;&nbsp; SQLGetData(hstmt, 1, SQL_C_CHAR, szName, NAME_LEN, &amp;cbName);
 &nbsp;&nbsp;&nbsp;&nbsp; SQLGetData(hstmt, 2, SQL_C_SSHORT, &amp;sAge, 0, &amp;cbAge);
 &nbsp;&nbsp;&nbsp;&nbsp; SQLGetData(hstmt, 3, SQL_C_CHAR, szBirthday, BDAY_LEN,
 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;cbBirthday);

 &nbsp;&nbsp;&nbsp;&nbsp; fprintf(out, "%-*s %-2d %*s", NAME_LEN-1, szName, sAge,
 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; BDAY_LEN-1, szBirthday);
 &nbsp;&nbsp; } else {
 &nbsp;&nbsp;&nbsp;&nbsp; break;
 &nbsp;&nbsp; }
  }
}
</code></pre>
<p class=tl>
This sample demonstrates the use of <b>SQLBindCol</b>. Notice that it is called once for each column prior to the data fetching loop.</P><pre><code>retcode = SQLExecDirect(hstmt,
 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "SELECT NAME, AGE, BIRTHDAY FROM EMPLOYEE ORDER BY 3, 2, 1",
 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; SQL_NTS);

if (retcode == SQL_SUCCESS) {

  /* Bind columns 1, 2, and 3. */

  SQLBindCol(hstmt, 1, SQL_C_CHAR, szName, NAME_LEN, &amp;cbName);
  SQLBindCol(hstmt, 2, SQL_C_SSHORT, &amp;sAge, 0, &amp;cbAge);
  SQLBindCol(hstmt, 3, SQL_C_CHAR, szBirthday, BDAY_LEN, &amp;cbBirthday);

  /* Fetch and print each row of data. On */
  /* an error, display a message and exit. */

  while (TRUE) {
 &nbsp;&nbsp; retcode = SQLFetch(hstmt);
 &nbsp;&nbsp; if (retcode == SQL_ERROR || retcode == SQL_SUCCESS_WITH_INFO) {
 &nbsp;&nbsp;&nbsp;&nbsp; show_error();
 &nbsp;&nbsp; }
 &nbsp;&nbsp; if (retcode == SQL_SUCCESS || retcode == SQL_SUCCESS_WITH_INFO){
 &nbsp;&nbsp;&nbsp;&nbsp; fprintf(out, "%-*s %-2d %*s", NAME_LEN-1, szName,
 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; sAge, BDAY_LEN-1, szBirthday);
  } else {
 &nbsp;&nbsp;&nbsp;&nbsp; break;
 &nbsp;&nbsp; }
  }
}
</code></pre>
</li>
<li>
Avoid calling <b>SQLTypeInfo</b> more than once. Cache it locally. In some databases it is an expensive stored procedure call.<br><br></li>
<li>
Don’t design your application so that it requires more than one active connection. Many data sources limit you to one.<br><br></li>
<li>
Use block fetches (<b>SQLExtendedFetch</b>) wherever possible to reduce trips across the network for multiple-tier drivers.<br><br></li>
<li>
Connecting is often expensive. Avoid making and breaking connections repeatedly if it isn’t necessary. Each time an application calls <b>SQLConnect</b>, <b>SQLDriverConnect</b>, or <b>SQLBrowseConnect</b>, the Driver Manager calls <b>SQLAllocEnv</b>, <b>SQLAllocConnect</b>, and <b>SQLSetConnectOption</b> in the driver. This can introduce significant overhead if these functions are called frequently.<br><br></li>
<li>
If you are not using vendor escape clauses, the <i>fOption</i> of <b>SQLSetStmtOption</b> should be set to SQL_NOSCAN and the <i>vParam</i> set to SQL_NOSCAN_ON. This instructs the driver not to scan for escape clauses. The driver sends the statement directly to the data source.</li>
</ul>
<h3>Drivers</h3>
<p>
It is easy to write ODBC drivers. Conversely, it is time consuming to write highly optimized drivers. However, there are cases where a non-optimized driver is more than adequate for a particular need. When writing drivers, a general principle is that the driver should make use of all the underlying capabilities of the data source.</p>
<h4>Single-tier drivers</h4>
<p>
Performance tuning a single-tier driver involves three basic techniques. First, make the file I/O as fast and efficient as possible. Second, have a good sort engine. Third, have a good query optimizer.</p>
<p>
PageAhead Software Corporation, developers of the Simba ODBC driver tools, provides some insight into the importance of query optimizers in the following excerpt taken from the PageAhead white paper “Introduction to ODBC Driver Development”:</p>
<p>
“SQL statements describe the operation to be performed, not how to perform the operation. Effective optimization of a query is crucial. Optimized queries that take seconds or minutes to perform can take hours if left unoptimized. Optimization is more of an art than a science. Cost-based optimization is used in many client-server SQL implementations and is the method discussed here. Essentially, the query planner/optimizer will analyze the query tree created by the parser and will attempt to determine an optimal method of execution. A number of methods are used to do this, including:
<ul type=disc>
<li>
General “rules of thumb” or heuristics are applied first. For example, the optimizer can:<ul type=disc>
<li>
Apply row selection filters to tables prior to joining them in order to minimize the number of rows in the join product table.<br><br></li>
<li>
Work with as few columns as possible while doing intermediate operations.<br><br></li>
<li>
Combine any Cartesian product operations and select operation into a single join operation.</li>
</ul>
</li>
<li>
A number of candidate access plans are produced to solve the SQL request. Cost estimates are produced for each. Cost is estimated using information about how the database information is actually stored on disk. Variables examined include the number of database rows, availability of primary and secondary indexes, and variability of data within columns. The access plan that results in the least cost (time) is the one selected for execution.</li>
</ul>
<p>
A good query planner/optimizer can yield an order of magnitude performance improvements relative to non-optimized access plans. The product of this phase is a detailed list of execution steps, described at the record and index level, that can be executed by a record-oriented process.”</p>
<h4>Multiple-tier drivers</h4>
<p>
Several points can be made about enhancing performance of multiple-tier drivers. When writing multiple-tier drivers, you need to decide who should be doing the work, the client or the server. What makes sense in your particular situation or market? Consider the following as you write your driver:
<ul type=disc>
<li>
Be frugal in the refresh of static catalog information using <b>SQLGetTypeInfo</b>. Perhaps only on initialization (not with each connect/disconnect), or implement <b>SQLGetTypeInfo</b> directly in the driver.<br><br></li>
<li>
Tune catalog queries carefully to optimize access to the DBMS.<br><br></li>
<li>
Preprocess SQL strings as little as possible. Do the minimum preprocessing, including looking for vendor escape sequences and standardizing of quotation characters.<br><br></li>
<li>
Use the highest performance access to DBMS; for example, Microsoft SQL Server driver goes to TDS instead of DB-Library; Oracle makes use of higher performance options in OCI in their ODBC driver.<br><br></li>
<li>
Support block fetch (<b>SQLExtendedFetch</b>) to permit applications to minimize network traffic and function calls.<br><br></li>
<li>
Fetch the data in the datatype of the server. Make the server do the work if this makes sense.<br><br></li>
<li>
Support canonical functions to enable the maximum amount of processing to be off-loaded to the data source. Many data sources directly support commonly used functions. When an application sends an ODBC canonical function to the driver, the driver is responsible for translating the canonical syntax into the data-source-specific syntax before sending the query on to the server. Without canonical functions, the data is pulled back from the server in order to operate on it. For example, consider the <b>Date</b> function in Access Basic. Almost every data source supports a <b>Date</b> function that returns the current date, but they all use different syntax. So, consider the following Microsoft Access query:<pre><code>SELECT *
FROM table
WHERE column = Date()
</code></pre>
<p class=tl>
If ODBC could not use the canonical syntax for <b>Date,</b> and <i>table</i> referred to a SQL Server table, all records would be pulled back from the server. The <b>Date</b> function would then run locally to find the matching records. Using the canonical function, a query is sent to the SQL Server driver, which translates the canonical into SQL Server syntax. SQL Server processes the request and returns the proper records. No local processing is involved.</P></li>
</ul>
<h2>Summary</h2>
<p>
ODBC provides a means of accessing data in a heterogeneous environment of relational and nonrelational database management systems. It can do so in an efficient, high-performance manner. However, attaining this performance requires an understanding of the tools that are being used to access the data. Are the tools actually using ODBC? If they are using ODBC, are they doing so in an efficient way? Does the architecture of the tool enhance performance or become and obstacle to it? Should you even be using ODBC? These are just a few of the questions that you need to address when you implement your ODBC projects. As with any client-server application, the application needs to be tuned with the knowledge of the underlying tools, the database, and the user requirements.</p>
</BODY>
</HTML>
