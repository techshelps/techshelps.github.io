<HTML><HEAD><META HTTP-EQUIV="Content-Type" Content="text/html; charset=Windows-1252"><script async src="https://www.googletagmanager.com/gtag/js?id=UA-83731338-2"></script><script>  window.dataLayer = window.dataLayer || [];  function gtag(){dataLayer.push(arguments);}  gtag('js', new Date());  gtag('config', 'UA-83731338-2');</script><title>3.2    Compressed File Data Organization</TITLE><style>@import url(msdn_ie4.css);</style>
<link disabled rel="stylesheet" href="msdn_ie3.css"></HEAD><BODY BGCOLOR="#FFFFFF">

</OBJECT></FORM>
<H2>3.2    Compressed File Data Organization </H2>A file in FFS is not compressed as one monolithic entity.  It is compressed as fixed length blocks.  The file system specifies the optimal size for a compression block to a system-specific compression service.  The compression service must support this block size.  The compression server may elect to support a compression block in excess of the size specified by the file system.  However, the file system will not use this excess buffer space to compress very large blocks because doing so is likely to have an extremely adverse effect on the file system's write performance.  For FFS the optimal compression block size is 4 Kbytes. </P>
As has been pointed out in the previous FileInfo discussion, there is no guarantee that there is sufficient free space in any erase block to fit the newly compressed block of data.  The same is true for any arbitrary write.  What the file system does in this case is to split the write across erase blocks.  During reads, the file system will need to read an entire compressed block into the decompression server buffer before the server can decompress it.  The compression block sequence bits in the FileInfo (CompressionBlockStart, CompressionBlockEnd) are used by the file system to locate and decompress an entire compressed block.</P></BODY></HTML>
