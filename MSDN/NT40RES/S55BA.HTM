<HTML><HEAD><script async src="https://www.googletagmanager.com/gtag/js?id=UA-83731338-2"></script><script>  window.dataLayer = window.dataLayer || [];  function gtag(){dataLayer.push(arguments);}  gtag('js', new Date());  gtag('config', 'UA-83731338-2');</script><title>Reading from Stripe Sets</TITLE><META HTTP-EQUIV="CONTENT-TYPE" CONTENT="text/html; charset= iso-8859-1"><style>@import url(msdn_ie4.css);</style>
<link disabled rel="stylesheet" href="msdn_ie3.css"></HEAD>
<BODY BGCOLOR="#FFFFFF"><FONT FACE="Verdana, Arial, Helvetica" SIZE="2"><FORM NAME="x"><OBJECT CLASSID="clsid:9c2ac687-ceef-11cf-96d9-00a0c903b016" NAME="iv"></OBJECT></FORM>
<H3>Reading from Stripe Sets</H3><P CLASS="t">Windows NT Workstation supports most hardware RAID configurations and stripe sets without parity. Testing the performance of these volume sets is much like testing single disk. The Response Probe tests used to measure single disks can also be run on any disk in a stripe set and on the virtual volume that hardware RAID exposes to Windows NT.</P>
<P CLASS="t">To test your volume sets, use the following counters:</P>
<UL><LI>System: % Total Processor Time</LI></UL><UL><LI>System: % Total Interrupts or Total Interrupts/sec</LI></UL><UL><LI>Physical Disk: Avg. Disk Read Queue Length</LI></UL><UL><LI>Physical Disk: Avg. Disk Bytes/Read</LI></UL><UL><LI>Physical Disk: Avg. Disk sec/Read</LI></UL><UL><LI>Physical Disk: Disk Read Bytes/sec</LI></UL><UL><LI>Physical Disk: Disk Reads/sec</LI></UL><P></P>
<P></P>
<P CLASS="t"><B>Note</B></P>
<P>The equivalent counters for measuring writing (for example, Avg. Disk Write Bytes/sec) are used to test the performance of volume sets while writing to disk. The values for reading and writing in our tests were so similar that showing the writing test added little value. However, you can use the same methods to test writing to disk on your volume sets.</P>
<P></P>
<P CLASS="t">These reading tests were run on a stripe set of four physical disks. Disks 0, 1, and 2 are on a single disk adapter, and Disk 3 is on a separate adapter. Performance Monitor is logging to Disk 3. In each test, the test tool is doing unbuffered, sequential reads of 64K records from a 60 MB file on a FAT partition. The test begins with reading only from Disk 0. Another physical disk was added with each iteration of the test to end with 4 stripes. During the test, Performance Monitor was logging data to Stripe_read.log, which is included on the Windows NT Resource Kit 4.0 CD.</P>
<P></P>
<P CLASS="t"><B>Tip</B></P>
<P>The logs recorded during these tests are included on the Windows NT Resource Kit 4.0 CD in the Performance Tools group. The logs are Stripe_read.log (sequential reading), Stripe_rand.log (random reading), and Stripe_write.log (sequential writing). Use Performance Monitor to chart the logs and follow along with the discussion that follows. The logs include data for the Processor, Logical Disk, and Physical Disk objects, so you can add more counters than those shown here.</P>
<P></P>
<P CLASS="t">The following graph shows an overview of the test, and the disk time contributed by each disk to the total effort. In the first segment of the graph there is one disk, the second, two disks; the third, three disks; and the fourth, four disks.</P>
<P><img src="xwr_n26.gif"></P>
<P CLASS="t">The graph consists of Physical Disk: % Disk Read Time for all disks in the stripe set. The thin gray line represents Disk 0, the white line is Disk 1, the heavy black line is Disk 2, and the heavy gray line is Disk 3. The striping strategy apportions the workload rather equally in this case, so the lines are superimposed upon each other. This graph is designed to show that, as new disks were added to the test, each disk needed to contribute a smaller portion of its time to the task.</P>
<P CLASS="t">The following table shows the average values of Avg. Disk Read Queue Length, a measure of disk time in decimals, for each disk during each segment of the test. </P>

<TABLE COLS="6" BORDER="0" CELLPADDING="7"><COLGROUP><COL WIDTH="99pt" VALIGN="TOP"><COL WIDTH="69pt" VALIGN="TOP"><COL WIDTH="59pt" VALIGN="TOP"><COL WIDTH="59pt" VALIGN="TOP"><COL WIDTH="59pt" VALIGN="TOP"><COL WIDTH="0pt" VALIGN="TOP"></COLGROUP><TBODY><TR><TD VALIGN="TOP"><P CLASS="th"><B># Stripes</B></P></TD><TD COLSPAN="4" VALIGN="TOP"><P CLASS="th"><B>Avg. Disk Read Queue Length</B></P></TD></TR><TR><TD COLSPAN="5" VALIGN="TOP"><P></P></TD></TR><TR><TD VALIGN="TOP"><P></P></TD><TD VALIGN="TOP"><P>Disk 0</P></TD><TD VALIGN="TOP"><P>Disk 1</P></TD><TD VALIGN="TOP"><P>Disk 2</P></TD><TD VALIGN="TOP"><P>Disk 3</P></TD></TR><TR><TD COLSPAN="6" VALIGN="TOP"><P></P></TD></TR><TR><TD VALIGN="TOP"><P>1</P></TD><TD VALIGN="TOP"><P>58.673</P></TD><TD VALIGN="TOP"><P>0</P></TD><TD VALIGN="TOP"><P>0</P></TD><TD VALIGN="TOP"><P>0</P></TD></TR><TR><TD VALIGN="TOP"><P>2</P></TD><TD VALIGN="TOP"><P>1.047</P></TD><TD VALIGN="TOP"><P>1.054</P></TD><TD VALIGN="TOP"><P>0</P></TD><TD VALIGN="TOP"><P>0</P></TD></TR><TR><TD VALIGN="TOP"><P>3</P></TD><TD VALIGN="TOP"><P>0.603</P></TD><TD VALIGN="TOP"><P>0.639</P></TD><TD VALIGN="TOP"><P>0.645</P></TD><TD VALIGN="TOP"><P>0.000</P></TD></TR><TR><TD VALIGN="TOP"><P>4</P></TD><TD VALIGN="TOP"><P>1.562</P></TD><TD VALIGN="TOP"><P>0.366</P></TD><TD VALIGN="TOP"><P>0.377</P></TD><TD VALIGN="TOP"><P>0.355</P></TD></TR></TBODY></TABLE>
<P CLASS="spacing"><BR></P><P></P>
<P CLASS="t">This table shows how the FTDISK, the Windows NT fault tolerant disk driver, distributes the workload among the stripes in the set, so each disk requires less time. The exception is Disk 0 which has a disproportionate share of the activity during the last stage of the test.</P>
<P CLASS="t">The following graph shows the effect of their combined efforts in the total work achieved by the stripe set.</P>
<P><img src="xwr_n27.gif"></P>
<P CLASS="t">In this graph, the gray line is Disk Reads/sec: Total, the heavy black line is Avg. Disk Bytes/Read: Total, the white line is Disk Read Bytes/sec:&nbsp;Total and the thin, black line at the bottom is Avg. Disk sec/Read: Total. The vertical maximum on the graph is increased to 200 to include all values.</P>
<P CLASS="t">The following figure shows the average values for each segment of the test.</P>
<P><img src="xwr_n28.gif"></P>
<P></P>
<P CLASS="t"><B>Tip</B></P>
<P>To produce a figure like this, open four copies of Performance Monitor and chart the counters you want to see for all available instances. The first copy is used just to show the counter names. Use the time window to set each copy of Performance Monitor to a different time segment of the test. Then, you can scroll each copy to the instance you want to examine in that time segment. In this example, the Total instance is shown for all time segments. </P>
<P></P>
<P></P>
<P CLASS="t">The graph and reports show that the transfer rate (Disk Reads/Sec: Total) is most affected by adding stripes to the set. It increases from an average of 69 reads/sec on a single disk to an average of 179 reads per second with four stripes. Throughput (Disk Read Bytes/Sec: Total) increases from an average of 4.5 MB per second to 11.75 MB/sec with four stripes.</P>
<P CLASS="t">Note that there is almost no change in the values upon adding the third stripe, Disk 2, to the set. The total transfer rate increases significantly with the addition of the second disk, but not at all with the third disk. Throughput, which is 4.5 MB with one disk, inches up to an average of 4.8 MB, then stays there until the fourth disk is added. </P>
<P CLASS="t">We cannot measure it directly, but it appears that this plateau is caused by a bottleneck on the disk adapter shared by Disks 0, 1, and 2. Although each of the physical disks has a separate head stack assembly, they still share the adapter. Shared resource contention is one of the limits of scalability. Multiple computers share the network, multiple processors share memory, multiple threads share processors, and multiple disks share adapters and buses. Fortunately, we can measure it and plan for future equipment needs.</P>
<P CLASS="t">The following graph shows how each disk is affected when stripes are added to the set. While the totals go up, each disk does less work. Potentially, it has time available for other work.</P>
<P><img src="xwr_n29.gif"></P>
<P CLASS="t">This is a graph of the transfer rate, as measured by Disk Reads/sec. The gray line is Disk Reads/Sec: Total, the black line is Disk Reads/sec: Disk 0, and the white line is Disk Reads/sec: Disk 1. The lines for Disks 2 and 3 run along the bottom of the graph until they are added, and then they are superimposed on the line for Disk 1.</P>
<P CLASS="t">The average values are:</P>

<TABLE COLS="7" BORDER="0" CELLPADDING="7"><COLGROUP><COL WIDTH="83pt" VALIGN="TOP"><COL WIDTH="57pt" VALIGN="TOP"><COL WIDTH="49pt" VALIGN="TOP"><COL WIDTH="49pt" VALIGN="TOP"><COL WIDTH="49pt" VALIGN="TOP"><COL WIDTH="56pt" VALIGN="TOP"><COL WIDTH="0pt" VALIGN="TOP"></COLGROUP><TBODY><TR><TD VALIGN="TOP"><P CLASS="th"><B>#Stripes</B></P></TD><TD COLSPAN="5" VALIGN="TOP"><P CLASS="th"><B>Disk Reads/sec</B></P></TD></TR><TR><TD COLSPAN="6" VALIGN="TOP"><P></P></TD></TR><TR><TD VALIGN="TOP"><P></P></TD><TD VALIGN="TOP"><P>Disk 0</P></TD><TD VALIGN="TOP"><P>Disk 1</P></TD><TD VALIGN="TOP"><P>Disk 2</P></TD><TD VALIGN="TOP"><P>Disk 3</P></TD><TD VALIGN="TOP"><P>Total</P></TD></TR><TR><TD COLSPAN="7" VALIGN="TOP"><P></P></TD></TR><TR><TD VALIGN="TOP"><P>1</P></TD><TD VALIGN="TOP"><P>68.948</P></TD><TD VALIGN="TOP"><P>0.000</P></TD><TD VALIGN="TOP"><P>0.000</P></TD><TD VALIGN="TOP"><P>0.000</P></TD><TD VALIGN="TOP"><P>68.948</P></TD></TR><TR><TD VALIGN="TOP"><P>2</P></TD><TD VALIGN="TOP"><P>74.107</P></TD><TD VALIGN="TOP"><P>74.107</P></TD><TD VALIGN="TOP"><P>0.000</P></TD><TD VALIGN="TOP"><P>0.010</P></TD><TD VALIGN="TOP"><P>148.223</P></TD></TR><TR><TD VALIGN="TOP"><P>3</P></TD><TD VALIGN="TOP"><P>49.020</P></TD><TD VALIGN="TOP"><P>49.020</P></TD><TD VALIGN="TOP"><P>49.020</P></TD><TD VALIGN="TOP"><P>0.000</P></TD><TD VALIGN="TOP"><P>147.069</P></TD></TR><TR><TD VALIGN="TOP"><P>4</P></TD><TD VALIGN="TOP"><P>102.487</P></TD><TD VALIGN="TOP"><P>25.619</P></TD><TD VALIGN="TOP"><P>25.619</P></TD><TD VALIGN="TOP"><P>25.619</P></TD><TD VALIGN="TOP"><P>179.343</P></TD></TR></TBODY></TABLE>
<P CLASS="spacing"><BR></P><P></P>
<P CLASS="t">These averages are a fairly good representation of the strategy of the stripe set controller as it distributes the workload equally among the stripes in the set. Each disk does less work, and the total achieved increases two and half times. Note that the transfer rate did not increase fourfold; the difference is likely to be related to sharing of resources.</P>
<P CLASS="t">The cause of the exceptional values of Disk 0, which appear in every test, are not entirely clear. They probably result from updates to the File Allocation Table. The tests were run on a FAT partition which was striped across all participating drives. In each case, the File Allocation Table is likely to be written to the first disk, Disk 0. Because the File Allocation Table is contiguous and sequential, Disk 0 can perform at maximum capacity. It appears that distributing the load to the other disks let Disk 0 double its productivity in the last sample interval. More research will be required to determine what happened.</P>
<P CLASS="t">The next graph shows that the same pattern holds for throughput. As more stripes are added, the total throughput increases, and the work is distributed across all four disks. This data also shows the disproportionate workload on Disk 0.    </P>
<P><img src="xwr_n30.gif"></P>
<P CLASS="t">This is a graph of disk throughput, as measured by Disk Read Bytes/sec. The gray line is Disk Read Bytes/sec: Total, the black line is Disk Read Bytes/sec: Disk 0, the white line is Disk Read Bytes/Sec: Disk 1. Again, the lines for Disks 2 and 3 run along the bottom of the graph until they are added, and then they are superimposed on the line for Disk 1.</P>
<P CLASS="t">This table shows the average throughput, in megabytes, for each disk as the test progresses.</P>

<TABLE COLS="7" BORDER="0" CELLPADDING="7"><COLGROUP><COL WIDTH="67pt" VALIGN="TOP"><COL WIDTH="57pt" VALIGN="TOP"><COL WIDTH="60pt" VALIGN="TOP"><COL WIDTH="56pt" VALIGN="TOP"><COL WIDTH="63pt" VALIGN="TOP"><COL WIDTH="42pt" VALIGN="TOP"><COL WIDTH="0pt" VALIGN="TOP"></COLGROUP><TBODY><TR><TD VALIGN="TOP"><P CLASS="th"><B>#Stripes</B></P></TD><TD COLSPAN="5" VALIGN="TOP"><P CLASS="th"><B>Disk Read Bytes/sec</B></P></TD></TR><TR><TD VALIGN="TOP"><P></P></TD><TD COLSPAN="5" VALIGN="TOP"><P></P></TD></TR><TR><TD VALIGN="TOP"><P></P></TD><TD VALIGN="TOP"><P>Disk 0</P></TD><TD VALIGN="TOP"><P>Disk 1</P></TD><TD VALIGN="TOP"><P>Disk 2</P></TD><TD VALIGN="TOP"><P>Disk 3</P></TD><TD VALIGN="TOP"><P>Total</P></TD></TR><TR><TD COLSPAN="7" VALIGN="TOP"><P></P></TD></TR><TR><TD VALIGN="TOP"><P>1</P></TD><TD VALIGN="TOP"><P>4.52</P></TD><TD VALIGN="TOP"><P>0.00</P></TD><TD VALIGN="TOP"><P>0.00</P></TD><TD VALIGN="TOP"><P>0.00</P></TD><TD VALIGN="TOP"><P>4.52</P></TD></TR><TR><TD VALIGN="TOP"><P>2</P></TD><TD VALIGN="TOP"><P>2.43</P></TD><TD VALIGN="TOP"><P>2.43</P></TD><TD VALIGN="TOP"><P>0.00</P></TD><TD VALIGN="TOP"><P>0.000039</P></TD><TD VALIGN="TOP"><P>4.86</P></TD></TR><TR><TD VALIGN="TOP"><P>3</P></TD><TD VALIGN="TOP"><P>1.61</P></TD><TD VALIGN="TOP"><P>1.61</P></TD><TD VALIGN="TOP"><P>1.61</P></TD><TD VALIGN="TOP"><P>0.00</P></TD><TD VALIGN="TOP"><P>4.82</P></TD></TR><TR><TD VALIGN="TOP"><P>4</P></TD><TD VALIGN="TOP"><P>6.72</P></TD><TD VALIGN="TOP"><P>1.68</P></TD><TD VALIGN="TOP"><P>1.68</P></TD><TD VALIGN="TOP"><P>1.68</P></TD><TD VALIGN="TOP"><P>11.75</P></TD></TR></TBODY></TABLE>
<P CLASS="spacing"><BR></P><P></P>
<P CLASS="t">The pattern, quite reasonably, is very similar to that for the transfer rate. The workload is distributed evenly and the total throughput rate achieved increases by 2.6%. Disk 0 is still doing a disproportionate share of the work (57%), which probably consists of its share of the read operations plus updating the FAT table.</P></BODY></HTML>
