<HTML><HEAD><script async src="https://www.googletagmanager.com/gtag/js?id=UA-83731338-2"></script><script>  window.dataLayer = window.dataLayer || [];  function gtag(){dataLayer.push(arguments);}  gtag('js', new Date());  gtag('config', 'UA-83731338-2');</script><title>Understanding the Normal Distribution</TITLE><META HTTP-EQUIV="CONTENT-TYPE" CONTENT="text/html; charset= iso-8859-1"><style>@import url(msdn_ie4.css);</style>
<link disabled rel="stylesheet" href="msdn_ie3.css"></HEAD>
<BODY BGCOLOR="#FFFFFF"><FONT FACE="Verdana, Arial, Helvetica" SIZE="2"><FORM NAME="x"><OBJECT CLASSID="clsid:9c2ac687-ceef-11cf-96d9-00a0c903b016" NAME="iv"></OBJECT></FORM>
<H3>Understanding the Normal Distribution</H3><P CLASS="t">You describe workloads to Response Probe, in part, by choosing a mean and standard deviation for several parameters that characterize threads in the workload. You choose a mean and standard deviation for</P>
<UL><LI>Length of think times</LI></UL><UL><LI>Number of processing cycles (file access and computing) between think times</LI></UL><UL><LI>Length of computing times</LI></UL><UL><LI>Position of records accessed from the workload file</LI></UL><UL><LI>Position of pages accessed from the data page file</LI></UL><UL><LI>Position of functions read from the code page file</LI></UL><P></P>
<P CLASS="t">The theory behind Response Probe is that real workloads are normally distributed; that is, the time spent thinking, accessing memory, reading and writing to files, finding a record in a file, computing, and other such tasks, are distributed on a standard bell-shaped curve. Alternatively, if the workloads were invariant or random, they wouldn't simulate real use of real computers.</P>
<P></P>
<P CLASS="t"><B>Note</B></P>
<P>The actions of threads and processes are normally distributed, so they are not, technically, fixed. However, in repeated trials the same inputs will produce the same results.</P>
<P></P>
<P CLASS="t">In a bell-shaped curve, most activity is concentrated around the mean, decreasing with distance from the mean, and with the least frequency at both extremes. The standard deviation determines how much activity is concentrated near the mean and how often more distant values occur. By definition, 2/3 of all activity occurs within one standard deviation on either side of the mean, 95% is within two standard deviations, and 99% is within three standard deviations of the mean.</P>
<P CLASS="t">Usually, the midpoint is chosen as the mean, and one-third of the total (1/6 on either side of the mean) is chosen as the standard deviation.</P>
<P><img src="xwr_j18.gif"></P>
<P CLASS="t">For example, in a 1024-page file, if page 512 is the mean and 170 pages is the standard deviation, then:</P>
<UL><LI>2/3 of  reads will be from pages 342–682,  (512-170 to 512+170).</LI></UL><UL><LI>95% will be from pages 172–852  (512 - (170 *2) to 512 + (170 * 2).</LI></UL><UL><LI>99% will be from pages 2–1022  (512 - (170 *3) to 512 + (170 * 3).</LI></UL><P></P>
<P CLASS="t">If, instead, the mean was 512 and the standard deviation was 256 (1/2 of 512), then 2/3 of  reads will be from pages 256–768, and the remaining third would be equally distributed throughout the rest of the file. At the extremes, if the standard deviation is 0, page 512 is the only one read, and if the mean is equal to the standard deviation, reading is random, so all pages are equally likely to be read.</P></BODY></HTML>
