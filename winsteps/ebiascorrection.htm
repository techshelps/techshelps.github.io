<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
   <title>Estimation bias correction - warnings
   </title>
   <meta name="generator" content="Help &amp; Manual">
   <meta name="keywords" content="Winsteps, Rasch Measurement, Rasch Analysis, 1-PL, 1PL">
   <meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
   <meta http-equiv="Content-Style-Type" content="text/css">
   <link type="text/css" href="default.css" rel="stylesheet">
<script type= "text/javascript">   
var min=10;
var max=300;
function increaseFontSize() {
   var p = document.getElementsByTagName('p');
   for(i=0;i<p.length;i++) {
      if(p[i].style.fontSize) {
         var s = parseInt(p[i].style.fontSize.replace("%",""));
      } else {
         var s = 100;
      }
      if(s<max) {
         s += 10;
      }
      p[i].style.fontSize = s+"%"
   }
}
function decreaseFontSize() {
   var p = document.getElementsByTagName('p');
   for(i=0;i<p.length;i++) {
      if(p[i].style.fontSize) {
         var s = parseInt(p[i].style.fontSize.replace("%",""));
      } else {
         var s = 100;
      }
      if(s>min) {
         s -= 10;
      }
      p[i].style.fontSize = s+"%"
   }   
}


</script>
<style type="text/css" media="print">
.printbutton {
  visibility: hidden;
  display: none;
}
</style>   

<script type="text/javascript" src="helpman_topicinit.js"></script>
</head>

<body style="margin: 0px 0px 0px 0px; background: #FFFFFF;">




<table width="100%" border="0" cellspacing="0" cellpadding="5" bgcolor="#FFFFFF">
  <tr valign="middle">
    <td align="left">
      <p class="p_Heading1"><span class="f_Heading1">Estimation bias correction - warnings</span></p>

    </td>
    <td align="right">
     <a href="controlindex.htm">Top</a>&nbsp;<a href="equating.htm">Up</a>&nbsp;<a href="estimation.htm">Down</a>&nbsp;
     <a href="javascript:decreaseFontSize();"><font size="-2"><b>A</b></font></a>&nbsp;<a href="javascript:increaseFontSize();"><font size="+2"><b>A</b></font></a><br>
<script  type= "text/javascript">
document.write("<input type='button' " +
"onClick='window.print()' " +
"class='printbutton' " +
"STYLE='background-color:00BFFF' " +
"value='Print This Page'>");
</script>
     

    </td>
  </tr>
</table>




<!-- Placeholder for topic body. -->
<div style="padding: 10px 10px 10px 5px; width: 100%;"> 
<p>Winsteps uses JMLE (= Joint Maximum-Likelihood Estimation), implemented with iterative-curve fitting, rather than Newton-Raphson estimation, because iterative curve-fitting is more robust against awkward data patterns.<br>
&nbsp;<br>
Every estimation method has strengths and weaknesses. The primary weakness of JMLE is that estimates have statistical bias. This is most obvious in a test of two dichotomous items (Andersen, 1973). In such a test, the difference between the item difficulties of the two items will be estimated to be twice its true value. In practical situations, the statistical bias is usually less than the standard errors of the estimates . However, the advantages of JMLE far outweigh its disadvantages. JMLE is estimable under almost all conditions including arbitrary and accidental patterns of missing data, arbitrary anchoring (fixing) of parameter estimates, unobserved intermediate categories in rating scales, and multiple different Rasch models in the same analysis.</p>
<p>Andersen, E. B. Conditional inference for multiple-choice questionnaires. British Journal of Mathematical and Statistical Psychology, 1973, 26, 31-44.</p>
<hr noshade size=1 style="color : #000000"><p>&nbsp;</p>
<p>In the psychometric literature, the terms &quot;bias&quot; and &quot;inconsistency&quot; are usually used in the context of the estimation of the difficulties of dichotomous items in a fixed length test administered to a sample of persons. Each measure-parameter is imagined to have a true value, and it is the purpose of the estimation procedure is to estimate that value from the available data. We can never be sure that we have exactly estimated the true value.</p>
<p>&nbsp;</p>
<p>If the sample size is infinite, and the resulting item estimates are their true values, then the estimates are consistent.</p>
<p>If the sample size is finite, and the expectations of the possible item estimates are their true values, then the estimates are unbiased.</p>
<p>&nbsp;</p>
<p>Winsteps implements JMLE. JMLE estimates are inconsistent and biased. They are less central than the true values. For instance, if the test consists of two dichotomous items, then, with an infinite sample, the JMLE estimate of the difference between the two item difficulties will be twice the true value. In this situation, an immediate solution is <a href="paired.htm">PAIRED=</a>Yes.</p>
<p>&nbsp;</p>
<p>Ben Wright and Graham Douglas discovered that the multiplier (L-1)/L is an approximate correction for JMLE item bias, where L is the test length. For a two-item test this correction would be (2-1)/2 = 0.5 . It is implemented in Winsteps with <a href="stbias.htm">STBIAS=</a>YES. </p>
<p>&nbsp;</p>
<p>Winsteps uses the raw scores as sufficient statistics for its estimates. The parameter estimates reported by Winsteps are the values for which &quot;the observed raw score = the model-expected raw score&quot;.</p>
<p>&nbsp;</p>
<p>&quot;Statistical Consistency&quot; (as usually conceptualized in the psychometric literature) relates to an infinite sample size with a finite test length. Under these conditions, Winsteps estimates are statistically inconsistent (i.e., are not the &quot;true&quot; parameter values even with an infinite amount of data) because inestimable extreme scores are included in the estimation space. &quot;Conditional&quot; estimation methods, CMLE, remove (&quot;condition out&quot;) extreme scores from the estimation space, </p>
<p>&nbsp;</p>
<p>The practical concern is &quot;estimation bias&quot;, &quot;departure of estimates from their true values with a finite amount of data&quot;. Winsteps estimates do have estimation bias. The Winsteps estimates are less central than they should be. But, as the likelihood of observing extreme scores reduces, the bias in the Winsteps estimates also reduces. Published studies indicate that when the test length is longer than 20 dichotomous items and the sample size is greater than 20 cases, then the Winsteps estimation bias is inconsequentially small. Estimation bias is usually only of concern if exact probabilistic inferences are to be made from logit measures obtained from small samples or short tests. But such inferences are imprecise irrespective of the size of the estimation bias.</p>
<p>&nbsp;</p>
<p>There are techniques which correct for estimation bias under specific conditions. One such condition is when the data correspond to pairwise observations (such as a basketball league or chess competition). Winsteps has the <a href="paired.htm">PAIRED=</a>YES option for this situation.</p>
<p>&nbsp;</p>
<p>At least two sources of estimation error are reported in the literature.</p>
<p>&nbsp;</p>
<p>An &quot;estimation bias&quot; error. This is usually negligibly small after the administration of 10 dichotomous items (and fewer rating scale items). Its size depends on the probability of observing extreme score vectors. For a two item test, the item measure differences are twice their theoretical values, reducing as test length increases. This can be corrected. <a href="stbias.htm">STBIAS=</a> does this approximately, but is only required if exact probability inferences are to be made from logit measure differences.</p>
<p>&nbsp;</p>
<p>A &quot;statistical inflation&quot; error. Since error variance always adds to observed variance, individual measures are always reported to be further apart (on average) than they really are. This cannot be corrected, in general, at an individual- measure level, because, for any particular measurement it cannot be known to what extent that measurement is biased by measurement error. However, if it is hypothesized that the persons, for instance, follow a normal distribution of known mean and standard deviation, this can be imposed on the estimates (as in <a href="estimation.htm">MMLE</a>) and the global effects of the estimate dispersion inflation removed. This is done in some other Rasch estimation software.</p>
<p>&nbsp;</p>
<p><span style="font-weight: bold;">Estimation Bias</span></p>
<p>&nbsp;</p>
<p>All Rasch estimation methods have some amount of estimation bias (which has no relationship with demographic bias). The estimation algorithm used by Winsteps, <a href="estimation.htm">JMLE</a>, has a slight bias in measures estimated from most datasets. The effect of the bias is to spread out the measures more widely than the data indicate. In practice, a test of more than 20 dichotomous items administered to a reasonably large sample will produce measures with inconsequential estimation bias. Estimation bias is only of concern when exact probabilistic inferences are to be made from short tests or small samples. Ben Wright opted for JMLE in the late 1960's because users were rarely concerned about such exact inferences, but they were concerned to obtain speedy, robust, verifiable results from messy data sets with unknown latent parameter distributions. Both of the identifiable sources of error are reduced by giving longer tests to bigger samples. With short tests, or small samples, other threats to validity tend to be of greater concern than the inflationary ones.</p>
<p>&nbsp;</p>
<p>If estimation bias would be observed even with an infinitely large sample (which it would be with JMLE), then the estimation method is labeled &quot;statistically inconsistent&quot; (even though the estimates are predictable and logical). This sounds alarming but the inconsistency is usually inconsequential, or can be easily corrected in the unlikely event that it does have substantive consequences.</p>
<p>&nbsp;</p>
<p>The JMLE joint likelihood estimation algorithm produces estimates that have a usually small statistical bias. This bias increases the spread of measures and calibrations, but usually less than the <a href="standarderrors.htm">standard error</a> of measurement. The bias quickly becomes insignificantly small as the number of persons and items increases. The reason that JMLE is statistically inconsistent under some conditions, and noticeably biased for short tests or small samples, is that it includes the possibility of extreme scores in the estimation space, but cannot actually estimate them. Inconsistency doesn't really matter, because it asks &quot;if we have infinite data, will the estimation method produce the correct answer?&quot; Estimation bias, also called statistical bias, is more important because it asks &quot;How near to correct are the estimates with finite data?&quot; In practice, JMLE bias is smaller than the other sources of noise in the data. See Ben Wright's comments at <a href="http://www.rasch.org/memo45.htm" class="weblink">www.rasch.org/memo45.htm</a></p>
<p>&nbsp;</p>
<p>For paired comparisons and very short tests, estimation can double the apparent spread of the measures, artificially inflating test reliability. This can be eliminated by specifying <a href="paired.htm">PAIRED=YES</a>.</p>
<p>&nbsp;</p>
<p>Correcting for bias may be helpful when it is desired to draw exact probabilistic inferences for small, complete datasets without anchoring.</p>
<p>&nbsp;</p>
<p>Correcting for bias may be misleading, or may be suppressed by Winsteps, in the presence of missing data or anchored persons or items. </p>
<p>&nbsp;</p>
<p>Bias correction can produce apparently inconsistent measures if bias-corrected measures, estimated from an unanchored analysis, are then used to anchor that same dataset.</p>
<p>&nbsp;</p>
<p><span style="font-weight: bold;">Estimation correction methods:</span></p>
<p>&nbsp;</p>
<p><a href="stbias.htm">STBIAS=YES</a> implements a variant of the simple bias correction proposed in Wright, B.D. and Douglas, G.A. (1977). Best procedures for sample-free item analysis. Applied Psychological Measurement, 1, 281-294. With large samples, a useful correction for bias is to multiply the estimated measures by (L-1)/L, where L is the smaller of the average person or item response count, so, for paired comparisons, multiply by 0.5. This is done automatically when <a href="paired.htm">PAIRED=</a>YES.</p>
<p>&nbsp;</p>
<p>Other Rasch programs may or may not attempt to correct for estimation bias. When comparing results from other programs, try both STBIAS=Y and STBIAS=N to find the closest match.</p>
<p>&nbsp;</p>
<p>Estimation methods with less bias under sum circumstances include CMLE and MMLE, but these have other limitations or restrictions which are deemed to outweigh their benefits for most uses.</p>
<p>&nbsp;</p>
<p><span style="font-weight: bold;">Technical information:</span></p>
<p>&nbsp;</p>
<p>Statistical estimation bias correction with JMLE is relevant when you wish to make exact probabilistic statements about differences between measures for short tests or small samples. The (L-1)/L correction applies to items on short dichotomous tests with large samples, where L is the number of non-extreme items on a test. For long dichotomous tests with small samples, the correction to person measures would be (N-1)/N. Consequently Winsteps uses a bias correction on dichotomous tests for items of (L-1)/L and for persons of (N-1)/N</p>
<p>&nbsp;</p>
<p>The reason for this correction is because the sample space does not match the estimation space. The difference is extreme score vectors. Estimation bias manifests itself as estimated measures which are more dispersed than the unbiased measures. The less likely an extreme score vector, the smaller the correction to eliminate bias. Extreme score vectors are less likely with polytomies than with dichotomies so the bias correction is smaller. For example, if an instrument uses a rating scale with m categories, then Winsteps corrects the item measures by (m-1)(L-1)/((m-1)(L-1)+1) and person measures by (m-1)(N-1)/((m-1)(N-1)+1) - but these are rough approximations.</p>
<p>&nbsp;</p>
<p>With most Rasch software using CMLE, PMLE or MMLE bias correction of item measures is not done because the estimation bias in the item difficulties is generally very small. Bias correction of person abilities is not done though estimation bias exists.</p>
<p>&nbsp;</p>
<p>Interaction terms are computed in an artificial situation in which the abilities and difficulties estimates are treated as known. Estimation bias is a minor effect in the interaction estimates. It would tend to increase very slightly the probability that differences between interaction estimates are reported as significant. So this is another reason to interpret DIF tests conservatively. If the number of relevant observations for an interaction term is big enough for the DIF effect to be regarded as real, and not a sampling accident, then the estimation bias will be very small. In the worst case, the multiplier would be of the order of (C-1)/C where C is the number of relevant observations.</p>
<p>&nbsp;</p>
<p><span style="font-weight: bold;">Comparing Estimates</span></p>
<p>&nbsp;</p>
<p>Bigsteps and Winsteps should produce the same estimates when</p>
<p>&nbsp;</p>
<p>(a) they are run with very tight convergence criteria, e.g.,</p>
<p>RCONV=.00001</p>
<p>LCONV=.00001</p>
<p>MJMLE=0</p>
<p>&nbsp;</p>
<p>(b) they have the same statistical bias adjustment</p>
<p>STBIAS=YES ; estimates will be wider spread</p>
<p>or</p>
<p>STBIAS=NO ; estimates will be narrower</p>
<p>&nbsp;</p>
<p>(c) they have the same extreme score adjustment</p>
<p>EXTRSC=0.5</p>
<p>&nbsp;</p>
<p>The item estimates in BTD were produced with statistical bias adjustment, but with convergence criteria that would be considered loose today. Tighter convergence produces a wider logit spread. So the BTD item estimates are slightly more central than Winsteps or Bigsteps.</p>
<p>&nbsp;</p>
<p>Winsteps and Bigsteps are designed to be symmetric. Transpose persons and items, and the only change is the sign of the estimates and an adjustment for local origin. The output reported in BTD (and by most modern Rasch programs) is not symmetric. So the person measure estimates in BTD are somewhat different.</p>
<p>&nbsp;</p>
<p><span style="font-weight: bold;">Do-it-yourself estimation-bias correction</span></p>
<p>&nbsp;</p>
<p>Correcting for estimation-bias in Winsteps estimates has both advantages and disadvantages. Corrected estimates are usually slightly more central than uncorrected estimates. The only conspicuous advantage of bias correction is for making inferences based on the exact logit distance between the Rasch estimates. Since, with small data sets, the bias correction is usually less than the standard error of the Rasch estimates, bias correction may be of doubtful statistical utility.</p>
<p>&nbsp;</p>
<p>STBIAS= will not correct for bias accurately with missing data, IWEIGHT= or PWEIGHT=. It may over- or under- correct for estimation bias.</p>
<p>&nbsp;</p>
<p>If you do need estimation-bias correction that is as accurate as possible with your data set, you will need to discover the amount of bias in the estimates, and then use USCALE= to perform your own estimation-bias correction.</p>
<p>&nbsp;</p>
<p>Here is a procedure using simulated data sets:</p>
<p>&nbsp;</p>
<p>1. In your control file, STBIAS=No and USCALE=1 </p>
<p>2. Obtain the Winsteps estimates for your data</p>
<p>3. Simulate many datasets using those estimates. (SIFILE= on the Winsteps Output Files menu).</p>
<p>4. Obtain the Winsteps estimates from the simulated data sets </p>
<p>5. Regress the simulated estimates on your initial estimates. These will give a slope near 1.0. </p>
<p>6. Obtain the Winsteps estimates for your data with USCALE = 1/slope</p>
<p>&nbsp;</p>
<p>The set of estimates in 6 is effectively unbiased.</p>
 
</div>



</body>
</html>
